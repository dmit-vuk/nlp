{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 3 \n",
    "\n",
    "# Классификация с использованием BERT  и Transfer learning\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: <впишите>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "\n",
    "В этом задании вы будете определять категории товара по данным из чеков, предоставленным в соревновании [Data Fusion Context](https://boosters.pro/championship/data_fusion/data).\n",
    "\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    " - [Pytorch](https://pytorch.org/).\n",
    " - [Transformers](https://github.com/huggingface/transformers).\n",
    " - [Tokenizers](https://github.com/huggingface/tokenizers).\n",
    "\n",
    "Данные лежат в архиве data.zip, в котором лежит файл `data.csv`, содержащий тексты и соответствующие им категории товаров. Все объекты поделены между train, test, val и unsupervised. Для unsupervised объектов категории товаров недоступны. \n",
    "\n",
    "Скачать данные можно здесь: [ссылка на google диск](https://drive.google.com/file/d/1AHs7qJYg2tc8zblGlT0Dpe50e6RW-gAW/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tests as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка данных (2 балла)\n",
    "\n",
    "Классические методы NLP (например, как мы делали в первом и втором домашнем задании) преобразуют тексты в списки индексов следующим образом:\n",
    "1. \"Очистка текста\" от плохих символов, приводим (или не приводим) текст к нижнему регистру.\n",
    "2. Текст делится по пробелам на слова.\n",
    "3. По полученной коллекции текстов строится словарь вида \"слово -> индекс\", редкие слова выбрасываются, стопслова иногда тоже\n",
    "4. Побитый на слова текст превращается в список индексов с помощью этого словаря.\n",
    "\n",
    "Для трансформеров схема выглядит немного по-другому — используются более продвинутые методы токенизации типа `wordpiece, bpe, sentencepiece`. Основное концептуальное отличие — текст делится не только на слова по пробелам, но и сами слова делятся на \"подслова\" (читай subwords). Это верно для BPE и wordpiece, а sentencepiece вообще не учитывает пробелы. Более подробно ознакомиться с этими методами токенизации можно в наших лекциях.\n",
    "\n",
    "В данном задании предлагается использовать wordpiece токенизатор, который использовали в оригинальной статье про BERT. Построить его можно с помощью библиотеки `tokenizers`:\n",
    "1. Считайте данные с помощью `pandas`\n",
    "2. Используя метод `tokenizers.BertWordPieceTokenizer.train` и список сырых текстов постройте токенизатор. Используйте нижний регистр (lowercase), чистый текст (clean_text), без акцентов (strip_accents), размера словаря 30000 (vocab_size).\n",
    "3. Сохраните построенный токенизатор (метод `tokenizer.save_model`) и создайте объект класса `transformers.BertTokenizerFast`, который работает быстрее стандартной реализации, но не позволяет её обучать.\n",
    "\n",
    "**Важно:** нужно при обучении c помощью параметра `special_tokens` завести индексы для токенов `[PAD], [UNK], [CLS], [SEP], [MASK]`, которые понадобятся нам дальше для обучения и использования модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('task3_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Флуконазол капс 150мг №1 Вертекс</td>\n",
       "      <td>-1</td>\n",
       "      <td>unsupervised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Борщ Станичный с тушенкой, 103 ккал (завод, з/у)</td>\n",
       "      <td>-1</td>\n",
       "      <td>unsupervised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Филе Горизонт (филе птицы), 218 ккал</td>\n",
       "      <td>-1</td>\n",
       "      <td>unsupervised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3  Бумага Упак 1/Прочи/В асс</td>\n",
       "      <td>-1</td>\n",
       "      <td>unsupervised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4  Бумага Упак 1/Прочи/В асс</td>\n",
       "      <td>-1</td>\n",
       "      <td>unsupervised</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text  label         split\n",
       "0                  Флуконазол капс 150мг №1 Вертекс     -1  unsupervised\n",
       "1  Борщ Станичный с тушенкой, 103 ккал (завод, з/у)     -1  unsupervised\n",
       "2              Филе Горизонт (филе птицы), 218 ккал     -1  unsupervised\n",
       "3                      3  Бумага Упак 1/Прочи/В асс     -1  unsupervised\n",
       "4                      4  Бумага Упак 1/Прочи/В асс     -1  unsupervised"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, 48,  0, 38, 41, 88, 22, 61, 62, 78, 55, 51, 60, 53, 95, 57, 27,\n",
       "       25, 47, 54, 24, 75,  4, 59, 94, 58, 40, 46, 87, 20, 83, 33, 39, 56,\n",
       "       86, 45, 52, 80, 85, 34, 50, 70,  2,  1,  7, 30, 35,  9, 43, 28,  3,\n",
       "       74, 31, 32, 23, 49, 79, 42, 37, 16, 11, 81, 71, 44, 91, 76, 17, 93,\n",
       "       26,  8,  6, 73, 92, 21, 84, 12, 77, 63, 72, 65, 13,  5, 68, 14, 15,\n",
       "       36, 64, 82, 18, 10, 69, 19, 89, 90, 66, 67, 29])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unsupervised', 'train', 'val', 'test'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.split.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "class WordpieceTokenizer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_path, \n",
    "            strip_accents=True, \n",
    "            clean_text=True, \n",
    "            lowercase=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "            vocab_path: путь к словарю\n",
    "            strip_accents: очистка текста от акцентов\n",
    "            clean_text: просто чистка текста от непонятных символов\n",
    "            lowercase: приведение текста к нижнему регистру\n",
    "            \n",
    "            Подгружает токенизатор с помощью BertTokenizerFast.\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self._tokenizer = BertTokenizerFast(\n",
    "            vocab_path,\n",
    "            strip_accents=strip_accents,\n",
    "            clean_text=clean_text,\n",
    "            do_lower_case=lowercase,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(\n",
    "            cls,\n",
    "            corpus,\n",
    "            corpus_save_path,\n",
    "            tokenizer_save_path,\n",
    "            tokenizer_name,\n",
    "            vocab_size=30000,\n",
    "            min_frequency=2,\n",
    "            strip_accents=True,\n",
    "            clean_text=True,\n",
    "            lowercase=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "            corpus: список текстов\n",
    "            corpus_save_path: временный путь для сохранения текстов в текстовом файле\n",
    "            tokenizer_save_path: путь для сохранения файлов токенизатора\n",
    "            tokenizer_name: название токенизатора, влияет на названия файлов токенизатора\n",
    "            vocab_size: размер словаря\n",
    "            min_frequency: минимальная частота элемента в словаре\n",
    "            strip_accents: очистка текста от акцентов\n",
    "            clean_text: просто чистка текста от непонятных символов\n",
    "            lowercase: приведение текста к нижнему регистру\n",
    "            \n",
    "            С помощью списка сырых текстов формирует токенизатор\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        with open(corpus_save_path, 'w', encoding='utf-8') as f:\n",
    "            for text in corpus:\n",
    "                f.write(f\"{text}\\n\")\n",
    "\n",
    "        tokenizer = BertWordPieceTokenizer(\n",
    "            clean_text=clean_text,\n",
    "            strip_accents=strip_accents,\n",
    "            lowercase=lowercase\n",
    "        )\n",
    "\n",
    "        tokenizer.train(\n",
    "            files=[corpus_save_path],\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=min_frequency,\n",
    "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "        )\n",
    "\n",
    "        tokenizer.save_model(tokenizer_save_path, tokenizer_name)\n",
    "        vocab_path = tokenizer_save_path + tokenizer_name + \"-vocab.txt\"\n",
    "        return cls(\n",
    "            vocab_path,\n",
    "            strip_accents,\n",
    "            clean_text,\n",
    "            lowercase\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: str. Сырой текст\n",
    "            \n",
    "            returns: list of ints. Список индексов\n",
    "            \n",
    "            C помощью метода .encode преобразует текст в индексы.\n",
    "        \"\"\"\n",
    "        return self._tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    @property\n",
    "    def cls_token_id(self):\n",
    "        \"\"\"\n",
    "            returns: индекс CLS токена\n",
    "        \"\"\"\n",
    "        return self._tokenizer.cls_token_id\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self._tokenizer.pad_token_id\n",
    "\n",
    "    @property\n",
    "    def mask_token_id(self):\n",
    "        return self._tokenizer.mask_token_id\n",
    "\n",
    "    @property\n",
    "    def sep_token_id(self):\n",
    "        return self._tokenizer.sep_token_id\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self):\n",
    "        return self._tokenizer.unk_token_id\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: размер словаря\n",
    "        \"\"\"\n",
    "        return self._tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте токенизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordpieceTokenizer.from_corpus(\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "    corpus=data[\"text\"].tolist(),\n",
    "    corpus_save_path='text_data.txt',\n",
    "    tokenizer_save_path='',\n",
    "    tokenizer_name=\"BertWordPieceTokenizer\",\n",
    ")\n",
    "\n",
    "tests.test_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам доступно довольно большое количество неразмеченных данных, которое можно использовать для предобучения модели. Мы рассмотрим две задачи предобучения:\n",
    "1. **Masked Language Modeling** — задача из BERT. Выбираем 15% слов, из них 80% заменяем на токен [MASK], 10% меняем на другие случайные слова, 10% оставляем как есть. Эти 15% слов предсказываем моделью. Вспомним пример из оригинальной статьи:\n",
    "    * Исходный текст: `my dog is hairy`\n",
    "    * Выбираем случайным образом 15% токенов для задачи. Допустим, выбрали четвертый токен - `hairy`\n",
    "    * В 80% случаев заменяем токен на `[MASK]`: `my dog is [MASK]`\n",
    "    * В 10% случаев на другой случайный токен: `my dog is apple`\n",
    "    * В 10% случаев оставляем неизменным: `my dog is hairy`\n",
    "    \n",
    "    \n",
    "2. **Sentence Order Prediction** — задача из ALBERT. Делим текст на два сегмента, с вероятностью 50% меняет сегменты местами. Предсказываем, в правильном ли порядке находятся сегменты.\n",
    "    * Текст: `the man went to the store. he bought a gallon of milk`\n",
    "    * Токенизируем и делим его на два сегмента: `the man went to the store` и `he bought a gallon of milk`\n",
    "    * C вероятностью 50% меняем их местами: `[CLS] he bought a gallon of milk [SEP] the man went to the store`\n",
    "    * С вероятностью 50% оставляем на месте: `[CLS] the man went to the store [SEP] he bought a gallon of milk`\n",
    "    * \"Левому\" сегменту соответствует нулевой индекс сегмента, \"правому\" - индекс 1\n",
    "\n",
    "Большая часть логики предобучения реализуется при подготовке данных.\n",
    "\n",
    "Реализуйте **PretrainDataset**, который токенизирует поданные сырые тексты и умеет возвращать для текста с конкретным индексом случайный сегмент длины, не большей чем `maxlen`. Логика для задачи **SOP** должна быть реализована в `__getitem__`: выбранный сегмент надо поделить на два равных сегмента, подбросить монетку, и с 50% вероятностью поменять сегменты местами. Нужно также добавить `[CLS]` и `[SEP]` токены.\n",
    "\n",
    "**hint:** чтобы существенно ускорить обучение (не потеряв при этом в качестве), после токенизации отсортируйте датасет по длине текстов.\n",
    "**hint:** токенизация датасета для предобучения занимает существенное время (5 минут), поэтому во время отладки стоит сделать её один раз и сохранить результат на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            corpus, \n",
    "            tokenizer, \n",
    "            minlen,\n",
    "            maxlen,\n",
    "            permute_prob=0.5, \n",
    "            verbose=False, \n",
    "            presort=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "            corpus: list of strings. Список сырых текстов\n",
    "            tokenizer: токенизатор\n",
    "            minlen: минимально допустимая длина текста\n",
    "            permute_prob: вероятность, с которой два сегмента меняются местами (происходит swap)\n",
    "            maxlen: максимальная длина текста\n",
    "            verbose: вывод прогресса токенизации текстов с помощью tqdm\n",
    "            presort: отсортировать датасет по длинам токенизированных текстов (т.е. ds[0] выдает самый короткий текст)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self._tokenizer = tokenizer\n",
    "        self._minlen = minlen\n",
    "        self._maxlen = maxlen\n",
    "        self.permute_prob = permute_prob\n",
    "        self.verbose = verbose\n",
    "        self.presort = presort\n",
    "\n",
    "        self._corpus = []\n",
    "        for text in tqdm(corpus, disable = not verbose):\n",
    "            tokens = tokenizer(text)\n",
    "            if minlen <= len(tokens):\n",
    "                self._corpus.append(tokens)\n",
    "        if presort:\n",
    "            self._corpus.sort(key=len)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._corpus)\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"\n",
    "            returns: tokenizer. Нужно для тестов\n",
    "        \"\"\"\n",
    "        return self._tokenizer\n",
    "    \n",
    "    def set_maxlen(self, maxlen):\n",
    "        \"\"\"\n",
    "            maxlen: максимальная длина текста\n",
    "            \n",
    "            поставить новое максимальное значение длины\n",
    "        \"\"\"\n",
    "        self._maxlen = maxlen\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: \n",
    "                input_ids - тензор с индексами, \n",
    "                token_type_ids - тензор с сегментными айдишниками (0 у левого сегмента, 1 у правого),\n",
    "                permuted - был ли swap сегментов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        tokens = self._corpus[idx][:self._maxlen]\n",
    "        mid = len(tokens) // 2\n",
    "        left, right = tokens[:mid], tokens[mid:]\n",
    "\n",
    "        permuted = np.random.uniform() > self.permute_prob\n",
    "        if permuted:\n",
    "            input_ids = [self._tokenizer.cls_token_id] + right + [self._tokenizer.sep_token_id] + left\n",
    "            token_type_ids = [1]*(len(right)+2) + [0]*len(left)\n",
    "        else:\n",
    "            input_ids = [self._tokenizer.cls_token_id] + left + [self._tokenizer.sep_token_id] + right\n",
    "            token_type_ids = [0]*(len(left)+2) + [1]*len(right)\n",
    "        \n",
    "        return torch.tensor(input_ids), torch.tensor(token_type_ids), permuted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длину текстов нужно ограничить снизу. Нет смысла рассматривать слишком короткие тексты (например, единичную длину), для которых задачи предобучения вообще не работают. Длинные тексты более эффективны для задач типа MLM, так как у модели больше контекста для предсказания и больше таргетов на один объект.\n",
    "\n",
    "Разумный способ определить минимальную длину текстов для MLM  — подобрать такую минимальную длину, чтобы вероятность замаскировать хотя бы одно слово в тексте была больше заданного порога.\n",
    "\n",
    "Т.е. если мы каждое слово маскируем с вероятностью 15%, какой длины должен быть текст, чтобы с вероятностью $\\geqslant$ 50% было замаскировано хотя бы одно слово?\n",
    "\n",
    "Используйте ответ на данный вопрос как минимальную допустимую длину текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.265024281798726"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "\n",
    "#1 - 0.85^n > 0.5\n",
    "n = np.log(0.5) / np.log(0.85)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте датасет (с произвольным разумным значением maxlen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3155764/3155764 [01:57<00:00, 26862.39it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = PretrainDataset(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    corpus=data[\"text\"].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    minlen=5,\n",
    "    maxlen=10**5,\n",
    "    verbose=True, \n",
    "    presort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2731378"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длину текстов нужно как-то ограничить сверху. Иначе, если встретится какой-то очень-очень длинный текст, он не поместится в видеопамять. Самый простой способ определить ограничение по длине  — после токенизации построить гистограмму длин (например, используя **sns.distplot**) и методом пристального взгляда определить разумное ограничение длины. Другой вариант  — взять большое значение квантили.\n",
    "\n",
    "**Вопрос:** какая максимальная длина текста подходит для этого датасета?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "\n",
    "lens = [len(text_tokens) for text_tokens in ds._corpus]\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(lens, bins=100)\n",
    "plt.title(\"Длины токенизированных текстов\")\n",
    "plt.xlabel(\"Длина\")\n",
    "plt.ylabel(\"Количество\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lens).quantile(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присвойте максимально допустимое значение длины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3155764/3155764 [01:57<00:00, 26849.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#ds.set_maxlen(30)\n",
    "\n",
    "ds = PretrainDataset(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    corpus=data[\"text\"].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    minlen=5,\n",
    "    maxlen=MAXLEN,\n",
    "    verbose=True, \n",
    "    presort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если для задачи **SOP** мы готовим данные при индексации датасета `PretrainDataset`, то маскирование для задачи **MLM** удобней делать в Collator'е в тензорном виде.\n",
    "\n",
    "Как с вероятностью `15%` заменить в тензоре `input_ids` значения на `0`: \n",
    "\n",
    "1. `mask = torch.rand(input_ids.shape) < 0.15`\n",
    "2. `input_ids = torch.where(mask, 0, input_ids)`\n",
    "\n",
    "Как сгенерировать случайные элементы словаря на каждый элемент батча: `torch.randint_like(input_ids, low=num_special_tokens, high=self._tokenizer.vocab_size)`.\n",
    "\n",
    "В `Collator` нужно также:\n",
    "1. сделать паддинг.\n",
    "2. из (примерно) 15% выбранных токенов 10% поменять на случайные и 10% оставить в исходном виде, остальные замаскировать\n",
    "3. сформировать таргеты. Нам нужно понимать, какие именно 15% токенов мы выбрали для предсказания + какие были исходные метки для них.\n",
    "\n",
    "Важно: `[CLS]` и другие специальные токены токены маскировать не надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_sequence_with_last_value(sequences):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        num_pad = max_len - len(seq)\n",
    "        last_value = seq[-1] if len(seq) > 0 else 0\n",
    "        padding = seq.new_full((num_pad,), last_value)\n",
    "        padded_seq = torch.cat((seq, padding), 0)\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "class Collator:\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer,\n",
    "            non_target_idx=-100,\n",
    "            mask_prob=0.15,\n",
    "            random_prob=0.1,\n",
    "            keep_unchanged_prob=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            tokenizer: токенизатор\n",
    "            non_target_idx: значение для индексов, не использующихся как таргеты. \n",
    "                Используйте его, чтобы пометить \"не таргет\" токены\n",
    "            mask_prob: вероятность выбрать индекс как таргет\n",
    "            random_prob: вероятность для уже выбранного индекса поменять его на случайное значение вместо маскирования\n",
    "            keen_unchanged_prob: вероятность оставить индекс в исходном виде вместо маскирования\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self._tokenizer = tokenizer\n",
    "        self.non_target_idx = non_target_idx\n",
    "        self.mask_prob = mask_prob\n",
    "        self.random_prob = random_prob\n",
    "        self.keep_unchanged_prob = keep_unchanged_prob\n",
    "        \n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "            batch: список вида [ds[i] for i in [12, 3, 2, 5]]\n",
    "            \n",
    "            returns: \n",
    "                input_ids: испорченные входные индексы токенов с замаскированными значениями\n",
    "                token_type_ids: сегментные эмбеддинги\n",
    "                labels: истинные значения входных индексов, как таргеты\n",
    "                permuted: был ли свап сегментов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        labels = []\n",
    "        permuted = []\n",
    "        special_tokens = [\n",
    "            self._tokenizer.cls_token_id,\n",
    "            self._tokenizer.pad_token_id, \n",
    "            self._tokenizer.unk_token_id,\n",
    "            self._tokenizer.mask_token_id, \n",
    "            self._tokenizer.sep_token_id\n",
    "        ]\n",
    "\n",
    "        for sample in batch:\n",
    "            inputs, token_type_id, permute = sample\n",
    "            input_ids.append(inputs)\n",
    "            token_type_ids.append(token_type_id)\n",
    "            permuted.append(permute)\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self._tokenizer.pad_token_id)\n",
    "        token_type_ids = pad_sequence_with_last_value(token_type_ids)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        mask = (torch.rand(input_ids.shape, device=input_ids.device) < self.mask_prob) * (input_ids != self._tokenizer.cls_token_id) *\\\n",
    "                (input_ids != self._tokenizer.pad_token_id) * (input_ids != self._tokenizer.unk_token_id) * \\\n",
    "                (input_ids != self._tokenizer.mask_token_id) * (input_ids != self._tokenizer.sep_token_id)\n",
    "        \n",
    "        labels[~mask] = self.non_target_idx\n",
    "        input_ids[mask] = self._tokenizer.mask_token_id\n",
    "        \n",
    "        indices_random = mask * (torch.rand(input_ids.shape, device=input_ids.device) < self.random_prob)\n",
    "        random_tokens = torch.randint(low=6, high=self._tokenizer.vocab_size, size=input_ids[indices_random].shape, device=input_ids.device)\n",
    "        input_ids[indices_random] = random_tokens\n",
    "\n",
    "        indices_old = mask * (torch.rand(input_ids.shape, device=input_ids.device) < self.keep_unchanged_prob)\n",
    "        input_ids[indices_old] = labels[indices_old]\n",
    "        \n",
    "        return input_ids, token_type_ids, labels, torch.tensor(permuted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте `collator` и `dataloader`. Для предобучения предлагается использовать большой `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer, non_target_idx=-100)\n",
    "\n",
    "dl = DataLoader(\n",
    "    ds, \n",
    "    collate_fn=collator, \n",
    "    batch_size=1024, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_collator(ds, collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Создание модели (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве модели мы будем использовать энкодер трансформера точь-в-точь в таком же виде, как его использовали в оригинальной статье про BERT. \n",
    "\n",
    "Прежде чем начать писать составляющие энкодера, обсудим инициализацию весов. Для трансформера нам понадобится большое количество линейных слов (`nn.Linear`), у которых для инициализации по дефолту используется равномерное распределение и отсутствует зануление bias'ов: $$\\text{Uniform}\\left(-\\frac{1}{\\sqrt{N_{\\text{in_features}}}}, \\frac{1}{\\sqrt{N_{\\text{in_features}}}}\\right).$$\n",
    "\n",
    "В оригинальной статье про BERT для весов используется **TruncatedNormal** со стандартным отклонением 0.02, bias'ы инициализируются нулями и модель обучается значительно лучше (это можно в ходе домашнего задания проверить).\n",
    "\n",
    "Поэтому, после создания линейных слоев и матрицы эмбеддингов, необходимо в явном виде вызывать для них TruncatedNormal инициализацию:\n",
    "\n",
    "1. `layer = ...`\n",
    "2. `nn.init.trunc_normal_(layer.weight.data, std=0.02, a=-2 * 0.02, b=2 * 0.02)`.\n",
    "\n",
    "Для линейных слоев нужно также вызывать `layer.bias.data.zero_()`.\n",
    "\n",
    "\n",
    "**TruncatedNormal** распределение отличается от нормального тем, что если величины выходят за границы отрезка [a, b], для этих величин повторно происходит сэмплирование до тех пор, пока они не попадут в нужный отрезок. Для BERT stddev = 0.02:\n",
    "\n",
    "$$[a; b] = [- 2  \\cdot \\text{stddev}; 2 \\cdot \\text{stddev}].$$\n",
    "\n",
    "Напишите функцию для инициализации линейных слоев и матрицы эмбеддингов **TruncatedNormal** распределением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def init_layer(layer, initializer_range=0.02, zero_out_bias=True):\n",
    "    \"\"\"\n",
    "        layer: наследник nn.Module, т.е. слой в pytorch\n",
    "        initializer_range: stddev для truncated normal\n",
    "        zero_out_bias: True для линейных слоев, False для матрицы эмбеддингов\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    if isinstance(layer, nn.Linear) or isinstance(layer, nn.Embedding):\n",
    "        nn.init.trunc_normal_(layer.weight.data, mean=0.0, std=initializer_range, a=-2*initializer_range, b=2*initializer_range)\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к созданию энкодера трансформера.\n",
    "\n",
    "<img src=\"images/transformer.png\" width=500 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем слой, создающий входные векторы токенов. Для этого нам нужны:\n",
    "1. Эмбеддинги токенов (`nn.Embedding`)\n",
    "2. Позиционные эмбеддинги (можно либо использовать `nn.Embedding`, либо явно создать матрицу эмбеддингов с помощью `nn.Parameter`)\n",
    "3. Сегментные эмбеддинги\n",
    "\n",
    "Эти три сущности складываются, затем идет layernorm и dropout.\n",
    "\n",
    "<img src=\"images/bert_input.png\" width=800 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size, \n",
    "            hidden_size, \n",
    "            max_seqlen,\n",
    "            dropout_prob=0., \n",
    "            type_vocab_size=2,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "            vocab_size: размер словаря\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            max_seqlen: количество позиционных эмбеддингов\n",
    "            dropout_prob: вероятность дропаута в конце слоя\n",
    "            type_vocab_size: количество сегментных эмбеддингов\n",
    "            eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.position_embeddings = nn.Embedding(max_seqlen, hidden_size)\n",
    "        self.sentence_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def get_token_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: возвращает слой с матрицей эмбеддингов для токенов. Нужен для MLM головы\n",
    "        \"\"\"\n",
    "        return self._token_embeddings\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        \"\"\"\n",
    "            input_ids: тензор с индексами токенов\n",
    "            token_type_ids: сегментные индексы\n",
    "            \n",
    "            returns: эмбеддинги токенов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        token_embed = self._token_embeddings(input_ids)\n",
    "        pos_embed = self.position_embeddings(torch.arange(input_ids.shape[1], dtype=input_ids.dtype).unsqueeze(0).expand(input_ids.shape[0], -1).to(input_ids.device))\n",
    "        if token_type_ids is not None:\n",
    "            sent_embed = self.sentence_embeddings(token_type_ids)\n",
    "        embed = token_embed + pos_embed + sent_embed if token_type_ids is not None else token_embed + pos_embed\n",
    "        return self.dropout(self.layer_norm(embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 7746560.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert_embeddings(BertEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит одноголовый **SelfAttention**:\n",
    "    \n",
    "<img src=\"images/attention.png\" width=600 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит многоголовый (multihead) **SelfAttention:**\n",
    "\n",
    "<img src=\"images/multihead.png\" width=200 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация **MultiHeadSelfAttention** — самая сложная часть энкодера. Дальше будет проще :)\n",
    "\n",
    "1. Принимаем на вход посл-ть векторов для каждого объекта в батче, т.е. тензор размера `batch_size x seqlen x dim`\n",
    "2. Получаем из исходных векторов векторы `query, key, value` с помощью линейного слоя. $W_q X, W_k X, W_v X$.\n",
    "    * **Важно:** не нужно делать три отдельных линейных слоя. Сделайте один линейный слой в три раза шире, затем после его применения разделите результат на три части с помощью метода `.chunk`. $W_{qkv} X$.\n",
    "3. Полученные query, key, value векторы делятся между \"головами\" аттеншна c помощью `.view`. Далее операции происходят для каждой головы отдельно.\n",
    "4. Нужно посчитать скалярные произведения всех запросов (queries) со всеми ключами (keys): $QV^T$.\n",
    "5. Заменить значения для паддинг токенов на очень маленькие (большие отрицательные), чтобы они не влияли на софтмакс:         \n",
    "    `attention_scores = attention_mask * attention_scores + (1 - attention_mask) * -100000`\n",
    "\n",
    "6. Применить Dropout аттеншн скоров, который  выкидывает из аттеншна токены целиком.\n",
    "7. Поделить \"аттеншны скоры\" на корень из размерности векторов и взять софтмакс по ключам. Т.е. $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}}, \\text{axis=1})$\n",
    "8. Посчитать контекстные векторы запросов $\\text{softmax}(\\frac{QK^T}{\\sqrt{d}}, \\text{axis=1})V$.\n",
    "9. Сконкатенировать контекстные векторы всех голов и применить линейный слой той же размерности и dropout.\n",
    "10. Сложить со входом **MultiHeadSelfAttention** слоя, применить layernorm: $\\text{layernorm}(x + \\text{dropout}(f(x)))$.\n",
    "\n",
    "**Про аттеншн маску:**\n",
    "* В полном виде аттеншн маска имеет размерность `batch_size x seqlen x seqlen`\n",
    "* У нас же если токен не паддинг, то его видят остальные токены, поэтому по сути вся информация содержится в матрице размера `batch_size x seqlen` с предикатом является ли токен паддингом\n",
    "* Эту матрицу размера `batch_size x seqlen` можно привести к виду `batch_size x seqlen x seqlen` операцией вида `attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]`\n",
    "\n",
    "**Вопросы:**\n",
    "1. Зачем нужно делить на корень из $d$ результаты скалярных произведений?\n",
    "2. Почему одно большое умножение на матрицу лучше, чем три маленьких?\n",
    "3. Что будет, если мы не будем заменять значения аттеншн скоров паддинг токенов на большие отрицательные значения?\n",
    "4. Какая вычислительная сложность (количество умножений) у операции **MultiheadSelfAttention**?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. Чтобы сохранить дисперсию примерно равную 1 для более стабильной работы сети и процесса её обучения\n",
    "2. Так более эффективно. Можно распараллелить матричное вычисление для более быстрого умножения\n",
    "3. Паддинг нужен только для того, чтобы одновременно обрабатывать батч из нескольких предложений разной длины. Мы не должны обращать внимание на токены, которых по факту нет.\n",
    "4. Для начала нужно вычислить k, q и v, для этого перемножаем исходную матрицу на проекционную: $(n \\times D) \\cdot (D \\times d) = (n \\times d)$. Это $O(ndD)$ операций. Затем считаем внимание: перемножаем матрицы размера $(n \\times d) \\cdot (d \\times n) = (n \\times n)$. И Затем $(n \\times n) \\cdot (n \\times d)$. В обоих случаях это $O(n^2d)$ операций. И таких $K$ голов. Таким образом, квадратично зависит от длины последовательности и линейно для других параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size,\n",
    "            num_attention_heads,\n",
    "            attention_probs_dropout_prob=0.,\n",
    "            dropout_prob=0.,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            num_attention_heads: количество голов аттеншна. Обычно выбирается как hidden_size / num_attention_heads = 64,\n",
    "                т.е. размерность векторов у одной головы 64\n",
    "            attention_probs_dropout_prob: вероятность дропаута для аттеншн скоров\n",
    "            dropout_prob: вероятность дропаута в конце слоя (перед суммой со входами)\n",
    "            eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = hidden_size // num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.qkv = nn.Linear(hidden_size, self.all_head_size*3)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "        self.o_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_dropout = nn.Dropout(dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps)\n",
    "\n",
    "    @property\n",
    "    def size_per_head(self):\n",
    "        \"\"\"\n",
    "            returns: размерность векторов для одной головы\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.attention_head_size\n",
    "    \n",
    "    def forward(self, embeddings, attention_mask):\n",
    "        \"\"\"\n",
    "            embeddings: входные эмбеддинги\n",
    "            attention_mask: тензор из 0, 1 размерности batch_size x seqlen x seqlen\n",
    "            \n",
    "            returns: контекстные векторы\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        query, key, value = self.qkv(embeddings).chunk(chunks=3, dim=-1)\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        query = query.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        \n",
    "        attention_scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(self.attention_head_size)\n",
    "        attention_mask = attention_mask.unsqueeze(1)\n",
    "        attention_scores = attention_mask*attention_scores + (1 - attention_mask)*(-100000.)\n",
    "        attention_scores = self.attn_dropout(attention_scores)\n",
    "        \n",
    "        attn_weights = F.softmax(attention_scores, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        output = self.layer_norm(self.out_dropout(attn_output) + embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters: 263680.\n"
     ]
    }
   ],
   "source": [
    "tests.test_attention(MultiHeadSelfAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать **полносвязный слой** гораздо проще - $\\text{layernorm}(\\text{dropout}(W_2 f(W_1 x + b_1) + b_2) + x)$:\n",
    "1. Линейный слой, расширяющий входные векторы до *intermediate_size*, который традиционно равен 4 * hidden_size, т.е. происходит расширение в четыре раза\n",
    "2. Функция активации (больше вы их нигде в модели не увидите)\n",
    "3. Линейный слой, сужающий векторы обратно до *hidden_size*\n",
    "4. Dropout, сложение со входом полносвязного слоя, layernorm\n",
    "\n",
    "**Вопросы:**\n",
    "1. Что дает \"расширение\" первым линейным слоем? Нельзя ли делать линейный слой поменьше?\n",
    "2. Какая вычислительная сложность (количество умножений) у операции?\n",
    "3. Используются ли где-то еще в трансформере функции активации (если не считать softmax функцией активации)?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. Больше параметров - больше инфы можем запомнить, больше обобщающей способности. Можно и меньше, но тогда резы будут чуть похуже.\n",
    "2. Сначала $(n \\times hid\\_size) \\cdot (hid\\_size \\times inter\\_size) = (n \\times inter\\_size)$. Далее наоборот $(n \\times inter\\_size) \\cdot (inter\\_size \\times hid\\_size) = (n \\times hid\\_size)$. Ну LayerNorm на $hid\\_size$ операций. Таким образом, всего $O(n \\cdot hid\\_size \\cdot inter\\_size)$ операций\n",
    "3. В самом конце на головке, которой предсказываем токен для задач предобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            intermediate_size, \n",
    "            dropout_prob=0., \n",
    "            act_func='relu', \n",
    "            eps=1e-3\n",
    "    ):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            intermediate_size: размерность промежуточно слоя. Обычно 4 * hidden_size\n",
    "            dropout_prob: вероятность дропаута перед суммой со входными представлениями\n",
    "            act_func: функция активации. Должны быть доступны gelu, relu\n",
    "            eps: eps для layernorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            nn.ReLU() if act_func.lower()=='relu' else nn.GELU(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "            embeddings: входные эмбеддинги размерности batch_size x seqlen x hidden_size\n",
    "            \n",
    "            returns: преобразованные эмбеддинги той же размерности\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.layer_norm(self.mlp(embeddings) + embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 526080.\n"
     ]
    }
   ],
   "source": [
    "tests.test_feedforward(FeedForward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим **MultiHeadSelfAttention** и **Feedforward** в один блок энкодера. Они применяются последовательно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            intermediate_size, \n",
    "            num_attention_heads, \n",
    "            dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            act_func='relu',\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._multihead_attention = MultiHeadSelfAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            dropout_prob=dropout_prob,\n",
    "            eps=eps\n",
    "        )\n",
    "        \n",
    "        self._feedforward = FeedForward(\n",
    "            hidden_size=hidden_size,\n",
    "            intermediate_size=intermediate_size,\n",
    "            act_func=act_func,\n",
    "            eps=eps,\n",
    "            dropout_prob=dropout_prob\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = self._multihead_attention(x, attention_mask)\n",
    "        x = self._feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 789760.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert_layer(BertLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объедините **BertEmbeddings** и произвольное заданное число **BertLayer** слоёв в один слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size,\n",
    "            max_seqlen,\n",
    "            hidden_size,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size, \n",
    "            num_attention_heads, \n",
    "            input_dropout_prob=0.,\n",
    "            dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            act_func='relu',\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self._embeddings = BertEmbeddings(\n",
    "            vocab_size, \n",
    "            hidden_size, \n",
    "            max_seqlen, \n",
    "            input_dropout_prob, \n",
    "            eps=eps\n",
    "        )\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            BertLayer(\n",
    "                hidden_size,\n",
    "                intermediate_size,\n",
    "                num_attention_heads,\n",
    "                dropout_prob,\n",
    "                attention_probs_dropout_prob,\n",
    "                act_func,\n",
    "                eps\n",
    "            ) for _ in range(num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "    def get_token_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: эмбеддинги токенов (матрицу эмбеддингов)\n",
    "        \"\"\"\n",
    "        return self._embeddings.get_token_embeddings()\n",
    "    \n",
    "    @staticmethod\n",
    "    def expand_mask(attention_mask):\n",
    "        \"\"\"\n",
    "            attention_mask: маска паддинга размерности batch_size x seqlen\n",
    "            \n",
    "            returns: маска паддинга размерности batch_size x seqlen x seqlen\n",
    "        \"\"\"\n",
    "        return attention_mask[:, None] * torch.ones_like(attention_mask)[..., None]\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, token_type_ids=None):\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        embeddings = self._embeddings(x, token_type_ids)\n",
    "        attention_mask = self.expand_mask(attention_mask)\n",
    "        for layer in self.encoder_layers:\n",
    "            embeddings = layer(embeddings, attention_mask)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 10905600.\n"
     ]
    }
   ],
   "source": [
    "tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для предобучения (и для целевых задач) необходимо задать \"головы\" модели:\n",
    "\n",
    "Голова для **MLM** задачи выглядит как $W_2 \\text{layernorm} (f(W_1 x + b_1)) + b_2$:\n",
    "1. Линейный слой $d \\times d$\n",
    "2. Функция активации\n",
    "3. LayerNorm\n",
    "4. Линейный слой $d \\times |V|$, где $|V|$ --- размер словаря. **Важно:** в качестве матрицы, на которую происходит умножение при аффинном преобразовании, берется матрица эмбеддингов токенов.\n",
    "5. Функционал ошибки тоже будем считать сразу в голове, для него используется **nn.CrossEntropyLoss**: \n",
    "    * `self._criterion(preds.view(-1, self._vocab_size), labels.view(-1))`\n",
    "\n",
    "Чтобы использовать матрицу входных эмбеддингов вместо последнего линейного слоя в голове, можно использовать присваивание вида`self._decoder.weight = input_embeddings.weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlmHead(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            hidden_act, \n",
    "            eps=1e-3, \n",
    "            ignore_index=-100, \n",
    "            input_embeddings=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            vocab_size: размер словаря\n",
    "            hidden_act: функция активации\n",
    "            eps: eps для layernorm\n",
    "            ignore_index: индекс таргета, который необходимо игнорировать при подсчете лосса\n",
    "            input_embeddings: слой с эмбеддингами токенов, для использования матрицы эмбеддингов вместо линейного слоя\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.ignore_index = ignore_index\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU() if hidden_act.lower()=='relu'else nn.GELU(),\n",
    "            nn.LayerNorm(hidden_size, eps=eps)\n",
    "        )\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)        \n",
    "        if input_embeddings is not None:\n",
    "            self.decoder.weight = input_embeddings.weight\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        \"\"\"\n",
    "            hidden_states: эмбеддинги токенов\n",
    "            labels: истинные метки, т.е. изначальные индексы токенов\n",
    "            \n",
    "            returns: посчитанный лосс\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        # self.criterion(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
    "        labels = labels.view(-1)\n",
    "        preds = self.decoder(self.mlp(hidden_states))\n",
    "        preds = preds.view(-1, preds.size(-1))\n",
    "        loss = self.criterion(preds, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 7776304.\n"
     ]
    }
   ],
   "source": [
    "tests.test_mlm_head(MlmHead, BertEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Голова для **SOP**-задачи выглядит аналогично и в оригинальной статье называется \"pooler-слоем\":\n",
    "1. Берем скрытое представление CLS токена\n",
    "2. Линейный слой $d \\times d$\n",
    "3. Функция активации, причем в качестве функции активации используется гиперболический тангенс **nn.Tanh**\n",
    "4. Dropout\n",
    "5. Линейный слой\n",
    "6. Функционал ошибки (бинарная кросс-энтропия с логитами, **nn.BCEWithLogitsLoss**)\n",
    "\n",
    "Эту голову (кроме последнего линейного слоя) мы будем использовать также и для целевой задачи (классификации чеков)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    CLS_POSITION = 0\n",
    "    CRITERION = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes=1, hidden_dropout_prob=0.):\n",
    "        \"\"\"\n",
    "            hidden_size: размерность эмбеддингов\n",
    "            hidden_dropout_prob: вероятность дропаута\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(hidden_dropout_prob),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, permuted=None):\n",
    "        \"\"\"\n",
    "            hidden_states: эмбеддинги\n",
    "            permuted: таргеты (были ли свапы сегментов). Если их нет, то необходимо выдать предсказания\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        cls_token = hidden_states[:, self.CLS_POSITION, :]\n",
    "        pred = self.mlp(cls_token)\n",
    "        if permuted is not None:\n",
    "            loss = self.CRITERION(pred.view(-1, 1), permuted.view(-1, 1).float())\n",
    "            return loss\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct. Amount of parameters is: 66049.\n"
     ]
    }
   ],
   "source": [
    "tests.test_classifier_head(ClassifierHead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим **Bert**, **MlmHead** и **ClassifierHead** в единую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size,\n",
    "            max_seqlen,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size,\n",
    "            num_attention_heads,\n",
    "            act_func='relu',\n",
    "            input_dropout_prob=0.,\n",
    "            hidden_dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            eps=1e-3, \n",
    "            ignore_index=-100\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._backbone = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            max_seqlen=max_seqlen,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size, \n",
    "            num_attention_heads=num_attention_heads, \n",
    "            input_dropout_prob=input_dropout_prob,\n",
    "            dropout_prob=hidden_dropout_prob, \n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            act_func=act_func,\n",
    "            eps=eps\n",
    "        )\n",
    "        self._mlm_head = MlmHead(\n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            act_func, \n",
    "            eps, \n",
    "            ignore_index, \n",
    "            input_embeddings=self._backbone.get_token_embeddings()\n",
    "        )\n",
    "        self._classifier_head = ClassifierHead(\n",
    "            hidden_size, \n",
    "            hidden_dropout_prob=hidden_dropout_prob, \n",
    "            num_classes=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask, labels, permuted, token_type_ids=None):\n",
    "        hidden_states = self._backbone(x, attention_mask, token_type_ids)\n",
    "        mlm_loss = self._mlm_head(hidden_states, labels)\n",
    "        sop_loss = self._classifier_head(hidden_states, permuted)\n",
    "        # в оригинальном BERT лоссы MLP и NSP используются с равными весами\n",
    "        return 0.5 * mlm_loss + 0.5 * sop_loss, {'MLM': mlm_loss, 'SOP': sop_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения гиперпараметров:\n",
    "* для успешного выполнения задания достаточно архитектуры bert-mini: `hidden_size=256`, `num_hidden_layers=4`, в качестве функции активации можно использовать `gelu`\n",
    "* стандартные практики: `intermediate_size = 4 * hidden_size`, `num_attention_heads = hidden_size // 64`\n",
    "* в оригинальной статье везде dropout равен 0.1, но для bert-mini модели можно попробовать значения поменьше. Вопрос - почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BertModel(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    hidden_size=256, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=30,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    "    act_func='gelu',\n",
    "    input_dropout_prob=0.05,\n",
    "    hidden_dropout_prob=0.05, \n",
    "    attention_probs_dropout_prob=0.05,\n",
    "    eps=1e-3, \n",
    "    ignore_index=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BertModel                                          --\n",
       "├─Bert: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                         --\n",
       "│    │    └─Embedding: 3-1                         7,680,000\n",
       "│    │    └─Embedding: 3-2                         7,680\n",
       "│    │    └─Embedding: 3-3                         512\n",
       "│    │    └─LayerNorm: 3-4                         512\n",
       "│    │    └─Dropout: 3-5                           --\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─BertLayer: 3-6                         789,760\n",
       "│    │    └─BertLayer: 3-7                         789,760\n",
       "│    │    └─BertLayer: 3-8                         789,760\n",
       "│    │    └─BertLayer: 3-9                         789,760\n",
       "├─MlmHead: 1-2                                     --\n",
       "│    └─Sequential: 2-3                             --\n",
       "│    │    └─Linear: 3-10                           65,792\n",
       "│    │    └─GELU: 3-11                             --\n",
       "│    │    └─LayerNorm: 3-12                        512\n",
       "│    └─Linear: 2-4                                 7,710,000\n",
       "│    └─CrossEntropyLoss: 2-5                       --\n",
       "├─ClassifierHead: 1-3                              --\n",
       "│    └─Sequential: 2-6                             --\n",
       "│    │    └─Linear: 3-13                           65,792\n",
       "│    │    └─Tanh: 3-14                             --\n",
       "│    │    └─Dropout: 3-15                          --\n",
       "│    │    └─Linear: 3-16                           257\n",
       "===========================================================================\n",
       "Total params: 18,690,097\n",
       "Trainable params: 18,690,097\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "1. Какая часть модели содержит наибольшее количество параметров? Эмбеддинги, аттеншн, полносвязные слои, голова?\n",
    "2. Зачем объединять параметры в голове и параметры матрицы эмбеддингов?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. Судя по саммари сверху больше всего весов в голове (хотя в эмбеддингах не шибко меньше). Это потому, что у нас и там и там есть огромная матрица размера $d \\times |V|$, где $|V|$ --- размер словаря, которая сжирает дофига параметров.\n",
    "2. Как было замечено раннее у нас эта матрица занимает очень много весов. Тем более голова на выходе нужна только для предобучения, потом мы её выкинем, поэтому нет смысла тратить на её обучение ресурсы. Плюс таким образом мы можем довольно хорошо обучить начальную матрицу эмбеддингов. Процесс обучения будет намного стабильнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Оптимизация (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации будем использовать **AdamW**, отличия которого от ванильного **Adam** можно почитать, например, [вот здесь](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)\n",
    "\n",
    "Параметры модели, передаваемые в оптимизатор, следует поделить на две группы с помощью `model.named_parameters()`:\n",
    "1. Все `bias` и `layernorm` слои, присутствующие в модели (их можно выцепить по названию). Для них $l_2$ регуляризацию стоит выключить, т.е. поставить `weight_decay=0`\n",
    "2. Оставшиеся слои, для которых регуляризация не нужна.\n",
    "\n",
    "\n",
    "**Вопрос:** почему $l_2$ регуляризацию не используют для bias'ов? Для layernorm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, weight_decay=0.01):\n",
    "    \"\"\"\n",
    "    model: инициализированная модель\n",
    "    weight_decay: коэффициент l2 регуляризации\n",
    "\n",
    "    returns: оптимизатор\n",
    "    \"\"\"\n",
    "    decayed_parameters, not_decayed_parameters = [], []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' in name or 'LayerNorm.weight' in name:\n",
    "            not_decayed_parameters.append(param)\n",
    "        else:\n",
    "            decayed_parameters.append(param)\n",
    "    \n",
    "    grouped_parameters = [\n",
    "        {'params': decayed_parameters, 'weight_decay': weight_decay},\n",
    "        {'params': not_decayed_parameters, 'weight_decay': 0.}\n",
    "    ]\n",
    "\n",
    "    return torch.optim.AdamW(grouped_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_optimizer(get_optimizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выглядит типичное расписание lr для трансформеров:\n",
    "\n",
    "<img src=\"images/lr.png\" width=300 height=300 />\n",
    "\n",
    "Почему мы сразу не стартуем с большого значения lr? Для больших архитектур трансформера модель разойдется, произойдет взрыв градиентов. Постепенно же увеличить lr до большого значения — можно. Процедуру линейного увеличения lr до какого-то пикового значения называют `linear warmup`.\n",
    "\n",
    "Реализуйте такое \"треугольное\" расписание для learning rate в предложенном шаблоне.\n",
    "\n",
    "**Вопрос:** а зачем нужно убывание learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            optimizer, \n",
    "            init_lr, \n",
    "            peak_lr, \n",
    "            final_lr, \n",
    "            num_warmup_steps, \n",
    "            num_training_steps\n",
    "    ):\n",
    "        \"\"\"\n",
    "            optimizer: оптимизатор\n",
    "            init_lr: начальное значение learning rate\n",
    "            peak_lr: пиковое значение learning rate\n",
    "            final_lr: финальное значение lr\n",
    "            num_warmup_steps: количество шагов разогрева (сколько шагов идем от начального до пикового значения)\n",
    "            num_training_steps: количество шагов обучения (количество батчей x количество эпох)\n",
    "            \n",
    "        \"\"\"\n",
    "        self._optimizer = optimizer\n",
    "        self._step = 0\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self._init_lr = init_lr\n",
    "        self.lr = init_lr\n",
    "        self._peak_lr = peak_lr\n",
    "        self._final_lr = final_lr\n",
    "        self._num_warmup_steps = num_warmup_steps\n",
    "        self._num_training_steps = num_training_steps\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = init_lr\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "            Меняет learning rate для оптимизатора\n",
    "            \n",
    "            Поменять learning rate для группы параметров в оптимизаторе можно присваиванием вида param_group['lr'] = lr\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        \n",
    "        if self._step < self._num_warmup_steps:\n",
    "            self.lr += (self._peak_lr - self._init_lr) / self._num_warmup_steps\n",
    "        else:\n",
    "            self.lr -= (self._peak_lr - self._final_lr) / (self._num_training_steps - self._num_warmup_steps)\n",
    "        \n",
    "        # Обновляем learning rate для каждой группы параметров\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr\n",
    "        \n",
    "        self._step += 1\n",
    "        \n",
    "    def get_last_lr(self):\n",
    "        \"\"\"\n",
    "            returns: текущий learning rate оптимизатора. Нужно для логгирования\n",
    "        \"\"\"\n",
    "        return [param_group['lr'] for param_group in self._optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct.\n"
     ]
    }
   ],
   "source": [
    "tests.test_scheduler(Scheduler, get_optimizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От запуска обучения нас отделяет только создание `Trainer`. От объектов класса `Trainer` требуется, чтобы:\n",
    "* логгировался лосс на каждом батче (`torch.utils.tensorboard.SummaryWriter`, `writer.add_scalar`)\n",
    "* клипались и логгировались нормы градиентов при каждом шаге спуска (`orch.nn.utils.clip_grad_norm_` возвращает нормы градиентов)\n",
    "* логгировались значения learning rate\n",
    "* была поддержана аккумуляция градиентов, нужная для эмуляции больших батчей\n",
    "\n",
    "При предобучении не нужно использовать какую-либо форму валидации, достаточно смотреть на батч лосс.\n",
    "\n",
    "Предлагается также для ускорения обучения использовать mixed precision из библиотеки `apex`:\n",
    "* перед обучением необходимо вызвать строчку вида `model, optimizer = amp.initialize(model, optimizer, opt_level='O1')`\n",
    "* при обучении `.backward()` надо делать в контекстном менеджере:     \n",
    "   `with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward()`\n",
    "        \n",
    "Что такое аккумуляция градиентов:\n",
    "* При использовании Adam в видеопамяти необходимо хранить градиенты и квадраты частных производных\n",
    "* При подсчете градиента по очередному батчу необязательно сразу делать шаг спуска, можно запомнить градиент, а затем посчитать градиент по другому батчу c теми же параметрами модели\n",
    "* Теперь эти два градиента можно сложить и получить градиент, который был посчитан как будто по одному большому батчу (составленному из этих двух). Сэмулировали большой батч. В данном случае количество шагов аккумуляции равно двум.\n",
    "* В данном случае количество шагов аккумуляции равно двум.\n",
    "\n",
    "Зачем нужны большие батчи? Обучение быстрее, оценки градиента точнее, позволяет увеличивать learning rate. Например, при предобучении авторы RoBERTA значительно увеличили размер батча по сравнению с ванильным BERT и получили прирост к качеству решения целевых задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            scheduler,\n",
    "            pad_token_id,\n",
    "            device,\n",
    "            num_accum_steps=1,\n",
    "            logdir=None,\n",
    "            max_grad_norm=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            scheduler: расписание learning rate. Нужно вызывать scheduler.step() ПОСЛЕ optimizer.step()\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            num_accum_steps: количество шагов аккумуляции\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.device = device\n",
    "        self.num_accum_steps = num_accum_steps\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self._n_epoch = 0\n",
    "        self.step = 0\n",
    "        self.global_step = 0\n",
    "        \n",
    "        self.writer = SummaryWriter(logdir) if logdir else None\n",
    "        self.model.to(device)\n",
    "        \n",
    "    def train(self, dataloader, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "            self._train_step(dataloader)\n",
    "            self._n_epoch += 1\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: объект класса DataLoader для обучения\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model.train()\n",
    "        \n",
    "        loss_accum = 0.0\n",
    "        self.optimizer.zero_grad()\n",
    "        for step, batch in tqdm(enumerate(dataloader), ascii=True):\n",
    "            input_ids, token_type_ids, labels, permuted = batch\n",
    "            input_ids = input_ids.int().to(self.device)\n",
    "            token_type_ids = token_type_ids.int().to(self.device)\n",
    "            labels = labels.long().to(self.device)\n",
    "            permuted = permuted.to(self.device)\n",
    "            attention_mask = (input_ids != self.pad_token_id).float()\n",
    "\n",
    "            loss, dict_loss = self.model(input_ids, attention_mask, labels, permuted, token_type_ids)\n",
    "            loss_accum += loss / self.num_accum_steps\n",
    "\n",
    "            if (step + 1) % self.num_accum_steps == 0:\n",
    "                # with amp.scale_loss(loss_accum, self.optimizer) as scaled_loss:\n",
    "                #     scaled_loss.backward()\n",
    "                loss_accum.backward()\n",
    "                \n",
    "                if self.max_grad_norm is not None:\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                    self.writer.add_scalar(\"Gradient Norm\", grad_norm.detach().cpu(), self.global_step)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss_accum = 0\n",
    "                self.global_step += 1\n",
    "                \n",
    "            self.writer.add_scalar(\"Loss train\", loss.item(), self.step)\n",
    "            self.writer.add_scalar(\"MLM Loss train\", dict_loss[\"MLM\"].item(), self.step)\n",
    "            self.writer.add_scalar(\"SOP Loss train\", dict_loss[\"SOP\"].item(), self.step)\n",
    "            self.writer.add_scalar(\"Learning Rate\", self.optimizer.param_groups[0]['lr'], self.global_step)\n",
    "\n",
    "            self.step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите и сохраните предобученную модель с помощью `torch.save`. \n",
    "\n",
    "**Важно:** тензорборд логи успешного обучения необходимо сложить в архив и приложить вместе с решенным заданием.\n",
    "\n",
    "Про гиперпараметры:\n",
    "* `weight_decay` - $0.1, 0.01, 0.001$ и т.д.\n",
    "* расписание lr - bert-mini не очень чувствителен к линейному вормапу, поэтому существенное влияние оказывают только пиковое и финальное значение lr. Пиковое значение стоит поискать где-то в масштабе 1e-3 - 1e-4, финальный lr можно сделать очень маленьким.\n",
    "* конкретное значение для клиппинга нормы особо ни на что не влияет, как правило (и в оригинальной статье тоже) его всегда ставят единицой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (_backbone): Bert(\n",
       "    (_embeddings): BertEmbeddings(\n",
       "      (_token_embeddings): Embedding(30000, 256)\n",
       "      (position_embeddings): Embedding(32, 256)\n",
       "      (sentence_embeddings): Embedding(2, 256)\n",
       "      (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-3): 4 x BertLayer(\n",
       "        (_multihead_attention): MultiHeadSelfAttention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "        )\n",
       "        (_feedforward): FeedForward(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_mlm_head): MlmHead(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=256, out_features=30000, bias=True)\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       "  (_classifier_head): ClassifierHead(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel(\n",
    "    hidden_size=256, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=MAXLEN+2,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    "    act_func='gelu',\n",
    "    input_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    eps=1e-3, \n",
    "    ignore_index=-100,\n",
    ").to(device)\n",
    "model.apply(init_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2668"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer=optimizer, \n",
    "    init_lr=1e-5, \n",
    "    peak_lr=1e-4, \n",
    "    final_lr=1e-8, \n",
    "    num_warmup_steps=int(len(dl)*0.1), \n",
    "    num_training_steps=len(dl)*n_epochs // 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer = amp.initialize(model, optimizer, opt_level='O1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    model=model, \n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device,\n",
    "    num_accum_steps=2,\n",
    "    logdir=\"logs_pretrain\",\n",
    "    max_grad_norm=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2668it [02:16, 19.53it/s]\n",
      "2668it [02:16, 19.54it/s]\n",
      "2668it [02:14, 19.83it/s]\n",
      "2668it [02:18, 19.25it/s]\n",
      "2668it [02:18, 19.32it/s]\n",
      "2668it [02:17, 19.42it/s]\n",
      "2668it [02:15, 19.70it/s]\n",
      "2668it [02:20, 19.00it/s]\n",
      "2668it [02:20, 18.98it/s]\n",
      "2668it [02:19, 19.16it/s]\n",
      "2668it [02:20, 18.94it/s]\n",
      "2668it [02:18, 19.30it/s]\n",
      "2668it [02:18, 19.26it/s]\n",
      "2668it [02:18, 19.33it/s]\n",
      "2668it [02:18, 19.29it/s]\n",
      "2668it [02:17, 19.36it/s]\n",
      "2668it [02:17, 19.41it/s]\n",
      "2668it [02:18, 19.31it/s]\n",
      "2668it [02:18, 19.33it/s]\n",
      "2668it [02:23, 18.56it/s]\n",
      "2668it [02:18, 19.24it/s]\n",
      "2668it [02:17, 19.38it/s]\n",
      "2668it [02:16, 19.50it/s]\n",
      "2668it [02:18, 19.31it/s]\n",
      "2668it [02:17, 19.42it/s]\n",
      "2668it [02:18, 19.28it/s]\n",
      "2668it [02:17, 19.36it/s]\n",
      "2668it [02:17, 19.39it/s]\n",
      "2668it [02:22, 18.71it/s]\n",
      "2668it [02:16, 19.49it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dl, n_epochs=n_epochs)\n",
    "    \n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    'pretrained_weights.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После предобучения вам придется перезапустить ноутбук и снова перепрогнать блоки, нужные для дообучения. Использование apex'а ломает обучение других моделей (которые не передавались в `amp.initialize`) в одном запуске. Если не перезапустить, скор получится гораздо хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. Дообучение (5 баллов)\n",
    "\n",
    "Самая сложная часть уже позади, осталось чуть-чуть :)\n",
    "\n",
    "Так как для дообучения доступно гораздо меньше данных, оно занимает гораздо меньше времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.loc[data['split'] == 'train'].reset_index(drop=True).copy()\n",
    "val = data.loc[data['split'] == 'val'].reset_index(drop=True).copy()\n",
    "test = data.loc[data['split'] == 'test'].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38608, 3), (4826, 3), (4827, 3), (3155764, 3))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape, data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет для дообучения выглядит стандартно: нужно токенизировать и запомнить тексты и соответствующие им метки, и затем в методе `__getitem__` их выдавать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            texts, \n",
    "            targets, \n",
    "            tokenizer,\n",
    "            maxlen, \n",
    "            presort=False,\n",
    "            verbose=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            texts: list of strings. Тексты чеков\n",
    "            targets: list of ints. Категории товаров\n",
    "            tokenizer: токенизатор\n",
    "            maxlen: максимальная длина текста\n",
    "            presort: отсортировать тексты по длине\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self._tokenizer = tokenizer\n",
    "        self._maxlen = maxlen\n",
    "        self.verbose = verbose\n",
    "        self.presort = presort\n",
    "\n",
    "        self._texts = []\n",
    "        self._targets = []\n",
    "        for text, target in tqdm(zip(texts, targets), disable = not verbose):\n",
    "            tokens = tokenizer(text)\n",
    "            if len(tokens) <= maxlen:\n",
    "                self._texts.append(tokens)\n",
    "                self._targets.append(target)\n",
    "                \n",
    "        if presort:\n",
    "            sorted_lists = sorted(zip(self._texts, self._targets), key=lambda x: len(x[0]))\n",
    "            self._texts, self._targets = zip(*sorted_lists)\n",
    "            # self._texts.sort(key=len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: input_ids - индексы токенов токенизированного текста, target - категория\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        input_ids, target = self._texts[idx], self._targets[idx]\n",
    "        return input_ids, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте датасеты для обучения и валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38608it [00:01, 29798.12it/s]\n",
      "4826it [00:00, 29715.70it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = FinetuneDataset(\n",
    "    train['text'].values, \n",
    "    train['label'].values, \n",
    "    maxlen=30, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "val_ds = FinetuneDataset(\n",
    "    val['text'].values, \n",
    "    val['label'].values, \n",
    "    maxlen=30, \n",
    "    tokenizer=tokenizer, \n",
    "    presort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллатор для дообучения делает только паддинг и конвертацию таргетов в тензоры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_idx):\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    input_ids = []\n",
    "    targets = []\n",
    "    for sample in batch:\n",
    "        text, target = sample\n",
    "        input_ids.append(torch.tensor(text))\n",
    "        targets.append(target)\n",
    "    input_ids_pad = pad_sequence(input_ids, batch_first=True, padding_value=pad_idx)\n",
    "    return input_ids_pad, torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте даталоадеры для обучения и валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer.pad_token_id)\n",
    "    ),\n",
    "    'eval': DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer.pad_token_id)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели теперь отсутствует MLM голова, а вместо SOP задачи голова классификации решает задачу определения категорий товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFinetuneModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            max_seqlen,\n",
    "            num_hidden_layers,\n",
    "            intermediate_size,\n",
    "            num_attention_heads,\n",
    "            num_classes,\n",
    "            act_func='relu',\n",
    "            input_dropout_prob=0.,\n",
    "            hidden_dropout_prob=0., \n",
    "            attention_probs_dropout_prob=0.,\n",
    "            eps=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._backbone = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            max_seqlen=max_seqlen,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            intermediate_size=intermediate_size, \n",
    "            num_attention_heads=num_attention_heads, \n",
    "            input_dropout_prob=input_dropout_prob,\n",
    "            dropout_prob=hidden_dropout_prob, \n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            act_func=act_func,\n",
    "            eps=eps\n",
    "        )\n",
    "        self._classifier_head = ClassifierHead(hidden_size, num_classes, hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        hidden_states = self._backbone(x, attention_mask)\n",
    "        return self._classifier_head(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте ту же архитектуру, которую вы выбрали при предобучении. Количество классов - 96:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertFinetuneModel(\n",
       "  (_backbone): Bert(\n",
       "    (_embeddings): BertEmbeddings(\n",
       "      (_token_embeddings): Embedding(30000, 256)\n",
       "      (position_embeddings): Embedding(32, 256)\n",
       "      (sentence_embeddings): Embedding(2, 256)\n",
       "      (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-3): 4 x BertLayer(\n",
       "        (_multihead_attention): MultiHeadSelfAttention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "        )\n",
       "        (_feedforward): FeedForward(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_classifier_head): ClassifierHead(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=96, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertFinetuneModel(\n",
    "    hidden_size=256, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=MAXLEN+2,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    "    num_classes=96,\n",
    "    act_func='gelu',\n",
    "    input_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    eps=1e-3, \n",
    ").to(device)\n",
    "model.apply(init_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузить предобученные веса можно с помощью следующей функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of found weights: 55\n"
     ]
    }
   ],
   "source": [
    "def load_weights(self, path):\n",
    "    found = []\n",
    "    with open(path, 'rb') as f:\n",
    "        weights = torch.load(f)\n",
    "    for name, param in weights.items():\n",
    "        if name in self.state_dict():\n",
    "            if param.shape == self.state_dict()[name].shape:\n",
    "                self.state_dict()[name].copy_(param)\n",
    "                found.append(name)\n",
    "\n",
    "    return found\n",
    "\n",
    "found = load_weights(model, 'pretrained_weights.pt')\n",
    "\n",
    "print('Amount of found weights: {}'.format(len(found)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте оптимизатор и расписание лр. Про гиперпараметры:\n",
    "* при дообучении используют маленький batch_size $\\in \\{32, 64\\}$\n",
    "* маленький learning rate:  $\\{1e-5, 2e-5, 4e-5\\}$ для больших моделей, для моделей вида bert-mini можно использовать и побольше: $\\{1e-4, 2e-4, 4e-4\\}$ \n",
    "* финальное значение все также маленькое\n",
    "* вормап можно делать где-то 0.06 от всех шагов обучения\n",
    "* количество эпох для дообучения - больше шести здесь не нужно\n",
    "* weight decay здесь потенциально можно использовать побольше, чем при предобучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=1e-2)\n",
    "n_epochs = 10\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer=optimizer, \n",
    "    init_lr=1e-6, \n",
    "    peak_lr=2e-5, \n",
    "    final_lr=1e-9, \n",
    "    num_warmup_steps=int(len(dl)*0.05), \n",
    "    num_training_steps=len(dl)*n_epochs // 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось создать пайплайн обучения:\n",
    "* apex использовать не нужно, дообучение быстрое и не требует больших батчей\n",
    "* аккумуляция градиентов не нужна т.к. батчи  маленькие\n",
    "* лосс теперь считается вне модели, в Trainer нужно использовать torch.nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneTrainer:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            scheduler,\n",
    "            pad_token_id,\n",
    "            device,\n",
    "            logdir=None,\n",
    "            max_grad_norm=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса BertModel\n",
    "            optimizer: оптимизатор\n",
    "            scheduler: расписание learning rate. Нужно вызывать scheduler.step() ПОСЛЕ optimizer.step()\n",
    "            pad_token_id: индекс паддинга. Нужен для создания attention mask\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "            num_accum_steps: количество шагов аккумуляции\n",
    "            logdir: директория для записи логов\n",
    "            max_grad_norm: максимум нормы градиентов, для клиппинга\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.device = device\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.model = self.model.to(device)\n",
    "        self.writer = SummaryWriter(logdir) if logdir else None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.step = 0\n",
    "        self._n_epoch = 0\n",
    "\n",
    "    def train(self, dataloaders, n_epochs, scorer=None):\n",
    "        \"\"\"\n",
    "            dataloaders: dict of dataloaders, keys 'train', 'eval' should be present.\n",
    "            n_epochs: int. Num epochs to train for.\n",
    "            scorer: takes trainer, outputs metric name and value as a tuple.\n",
    "        \"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders['train'])\n",
    "            val_loss = self._eval_step(dataloaders['eval'])\n",
    "\n",
    "            if self.writer is not None:\n",
    "                self.writer.add_scalar('eval', val_loss, global_step=self._n_epoch)\n",
    "                \n",
    "                if scorer is not None:\n",
    "                    name, value = scorer(self)\n",
    "                    self.writer.add_scalar(name, value, global_step=self._n_epoch)\n",
    "                    \n",
    "            self._n_epoch += 1\n",
    "\n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: training dataloader.\n",
    "            \n",
    "            returns: train_loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        for step, batch in tqdm(enumerate(dataloader)):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, targets = batch\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            attention_mask = (input_ids != self.pad_token_id).float()\n",
    "\n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == targets).sum().item()\n",
    "            accuracy = correct / len(targets)\n",
    "                \n",
    "            if self.max_grad_norm is not None:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                self.writer.add_scalar(\"Gradient Norm\", grad_norm.detach().cpu(), self.step)\n",
    "                \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "                \n",
    "            self.writer.add_scalar(\"Loss train\", loss.item(), self.step)\n",
    "            self.writer.add_scalar(\"Accuracy train\", accuracy, self.step)\n",
    "            self.writer.add_scalar(\"Learning Rate\", self.optimizer.param_groups[0]['lr'], self.step)\n",
    "\n",
    "            self.step += 1\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "    def _eval_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: evaluation dataloader.\n",
    "            \n",
    "            returns: eval loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        predicted_all, targets_all = np.array([]), np.array([])\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids, targets = batch\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                attention_mask = (input_ids != self.pad_token_id).float()\n",
    "    \n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                targets_all = np.append(targets_all, targets.detach().cpu().numpy())\n",
    "                predicted_all = np.append(predicted_all, predicted.detach().cpu().numpy())\n",
    "        \n",
    "        correct = (predicted_all == targets_all).sum()\n",
    "        accuracy = correct / len(targets_all)\n",
    "        self.writer.add_scalar(\"Accuracy val\", accuracy, self._n_epoch)\n",
    "        return total_loss / len(dataloader)\n",
    "        \n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: inference dataloader. Should not have targets.\n",
    "            \n",
    "            returns: np.array c предсказанными категориями\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        predicted_all = np.array([])\n",
    "        for input_ids in dataloader:\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            attention_mask = (input_ids != self.pad_token_id).float()\n",
    "\n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_all = np.append(predicted_all, predicted.detach().cpu().numpy())\n",
    "        return predicted_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для мониторинга целевой метрики используйте предоставленный scorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, maxlen, tokenizer):\n",
    "        \"\"\"\n",
    "            texts: list of str. Сырые тексты чеков\n",
    "            maxlen: максимальная длина текста\n",
    "            tokenizer: токенизатор\n",
    "        \"\"\"\n",
    "        self._texts = [tokenizer(text) if tokenizer is not None else text for text in texts]\n",
    "        self._maxlen = maxlen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: тензор из индексов токенов токенизированного текста\n",
    "        \"\"\"\n",
    "        text = self._texts[idx]\n",
    "        if self._maxlen is not None:\n",
    "            text = text[:self._maxlen]\n",
    "        return torch.tensor(text, dtype=torch.long)\n",
    "    \n",
    "def make_scorer(texts, targets, tokenizer, maxlen):\n",
    "    inference_ds = InferenceDataset(texts, maxlen=maxlen, tokenizer=tokenizer)\n",
    "    inference_dl = DataLoader(inference_ds, batch_size=32, shuffle=False, collate_fn=inference_collate_fn)\n",
    "    def get_score(trainer):\n",
    "        preds = trainer.predict(inference_dl)\n",
    "        return 'f1', f1_score(targets, preds, average='weighted')\n",
    "    return get_score\n",
    "\n",
    "\n",
    "inference_collate_fn = lambda x: pad_sequence(x, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "val_scorer = make_scorer(val['text'].values, val['label'].values, tokenizer, maxlen=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация претрейном, файн-тюн 1 эпоха"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of found weights: 55\n"
     ]
    }
   ],
   "source": [
    "model = model = BertFinetuneModel(\n",
    "    hidden_size=256, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=MAXLEN+2,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    "    num_classes=96,\n",
    "    act_func='gelu',\n",
    "    input_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    eps=1e-3, \n",
    ").to(device)\n",
    "\n",
    "found = load_weights(model, 'pretrained_weights.pt')\n",
    "print('Amount of found weights: {}'.format(len(found)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=1e-2)\n",
    "n_epochs = 1\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer=optimizer, \n",
    "    init_lr=5e-6, \n",
    "    peak_lr=4e-5, \n",
    "    final_lr=1e-9, \n",
    "    num_warmup_steps=int(len(dl)*0.05), \n",
    "    num_training_steps=len(dl)*n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = FinetuneTrainer(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device,\n",
    "    logdir=f\"logs_ft/fine-tune {n_epochs} epochs\",\n",
    "    max_grad_norm=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f1', 0.0018818747300944601)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_scorer(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "602it [00:04, 144.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('f1', 0.561665532006793)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(dataloaders, n_epochs=n_epochs, scorer=val_scorer)\n",
    "test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=30)\n",
    "\n",
    "test_scorer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация претрейном, файн-тюн 10 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of found weights: 55\n"
     ]
    }
   ],
   "source": [
    "model = model = BertFinetuneModel(\n",
    "    hidden_size=256, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=MAXLEN+2,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    "    num_classes=96,\n",
    "    act_func='gelu',\n",
    "    input_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    eps=1e-3, \n",
    ").to(device)\n",
    "\n",
    "found = load_weights(model, 'pretrained_weights.pt')\n",
    "print('Amount of found weights: {}'.format(len(found)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=1e-2)\n",
    "n_epochs = 10\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer=optimizer, \n",
    "    init_lr=5e-6, \n",
    "    peak_lr=4e-5, \n",
    "    final_lr=1e-9, \n",
    "    num_warmup_steps=int(len(dl)*0.05), \n",
    "    num_training_steps=len(dl)*n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = FinetuneTrainer(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device,\n",
    "    logdir=f\"logs_ft/fine-tune {n_epochs} epochs\",\n",
    "    max_grad_norm=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скор на валидационной выборке до обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f1', 0.013726065922180659)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_scorer(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "602it [00:04, 140.96it/s]\n",
      "602it [00:04, 144.14it/s]\n",
      "602it [00:04, 142.85it/s]\n",
      "602it [00:04, 148.26it/s]\n",
      "602it [00:04, 144.69it/s]\n",
      "602it [00:04, 146.86it/s]\n",
      "602it [00:04, 144.57it/s]\n",
      "602it [00:04, 144.74it/s]\n",
      "602it [00:04, 145.33it/s]\n",
      "602it [00:04, 142.69it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloaders, n_epochs=n_epochs, scorer=val_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из критериев получения полного балла за задание является получение на тесте значения метрики $\\geqslant 0.7$. Скор на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f1', 0.8103909052489793)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=30)\n",
    "\n",
    "test_scorer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забудьте также приложить вместе со сделанным заданием тензорборд дообучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация претрейном, файн-тюн 50 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of found weights: 55\n"
     ]
    }
   ],
   "source": [
    "model = model = BertFinetuneModel(\n",
    "    hidden_size=256, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=MAXLEN+2,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    "    num_classes=96,\n",
    "    act_func='gelu',\n",
    "    input_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    eps=1e-3, \n",
    ").to(device)\n",
    "\n",
    "found = load_weights(model, 'pretrained_weights.pt')\n",
    "print('Amount of found weights: {}'.format(len(found)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=1e-2)\n",
    "n_epochs = 50\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer=optimizer, \n",
    "    init_lr=5e-6, \n",
    "    peak_lr=4e-5, \n",
    "    final_lr=1e-9, \n",
    "    num_warmup_steps=int(len(dl)*0.05), \n",
    "    num_training_steps=len(dl)*n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = FinetuneTrainer(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device,\n",
    "    logdir=f\"logs_ft/fine-tune {n_epochs} epochs\",\n",
    "    max_grad_norm=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "602it [00:05, 110.95it/s]\n",
      "602it [00:09, 66.08it/s] \n",
      "602it [00:06, 86.29it/s] \n",
      "602it [00:04, 142.09it/s]\n",
      "602it [00:04, 144.48it/s]\n",
      "602it [00:04, 144.98it/s]\n",
      "602it [00:04, 145.63it/s]\n",
      "602it [00:04, 147.58it/s]\n",
      "602it [00:04, 144.76it/s]\n",
      "602it [00:04, 144.67it/s]\n",
      "602it [00:04, 145.42it/s]\n",
      "602it [00:04, 142.88it/s]\n",
      "602it [00:04, 144.13it/s]\n",
      "602it [00:04, 143.69it/s]\n",
      "602it [00:04, 143.58it/s]\n",
      "602it [00:04, 144.22it/s]\n",
      "602it [00:04, 142.87it/s]\n",
      "602it [00:04, 144.85it/s]\n",
      "602it [00:04, 146.20it/s]\n",
      "602it [00:04, 147.13it/s]\n",
      "602it [00:04, 132.76it/s]\n",
      "602it [00:04, 147.45it/s]\n",
      "602it [00:04, 146.20it/s]\n",
      "602it [00:04, 141.90it/s]\n",
      "602it [00:04, 137.93it/s]\n",
      "602it [00:04, 145.87it/s]\n",
      "602it [00:04, 146.25it/s]\n",
      "602it [00:04, 146.15it/s]\n",
      "602it [00:04, 145.75it/s]\n",
      "602it [00:04, 145.72it/s]\n",
      "602it [00:04, 142.06it/s]\n",
      "602it [00:04, 146.54it/s]\n",
      "602it [00:04, 144.99it/s]\n",
      "602it [00:04, 147.39it/s]\n",
      "602it [00:04, 149.22it/s]\n",
      "602it [00:04, 146.34it/s]\n",
      "602it [00:04, 144.44it/s]\n",
      "602it [00:04, 144.39it/s]\n",
      "602it [00:04, 145.92it/s]\n",
      "602it [00:04, 143.96it/s]\n",
      "602it [00:04, 143.23it/s]\n",
      "602it [00:04, 144.06it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloaders, n_epochs=n_epochs, scorer=val_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f1', 0.8113025475747502)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=30)\n",
    "\n",
    "test_scorer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация фром скретч, обучаем 10 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertFinetuneModel(\n",
       "  (_backbone): Bert(\n",
       "    (_embeddings): BertEmbeddings(\n",
       "      (_token_embeddings): Embedding(30000, 256)\n",
       "      (position_embeddings): Embedding(32, 256)\n",
       "      (sentence_embeddings): Embedding(2, 256)\n",
       "      (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-3): 4 x BertLayer(\n",
       "        (_multihead_attention): MultiHeadSelfAttention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "        )\n",
       "        (_feedforward): FeedForward(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_classifier_head): ClassifierHead(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=96, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertFinetuneModel(\n",
    "    hidden_size=256, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=MAXLEN+2,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    "    num_classes=96,\n",
    "    act_func='gelu',\n",
    "    input_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    eps=1e-3, \n",
    ").to(device)\n",
    "model.apply(init_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=1e-2)\n",
    "n_epochs = 10\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer=optimizer, \n",
    "    init_lr=1e-6, \n",
    "    peak_lr=2e-5, \n",
    "    final_lr=1e-9, \n",
    "    num_warmup_steps=int(len(dl)*0.05), \n",
    "    num_training_steps=len(dl)*n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = FinetuneTrainer(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device,\n",
    "    logdir=f\"logs_ft/from scratch {n_epochs} epochs\",\n",
    "    max_grad_norm=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "602it [00:11, 51.19it/s] \n",
      "602it [00:04, 130.89it/s]\n",
      "602it [00:04, 122.61it/s]\n",
      "602it [00:05, 114.62it/s]\n",
      "602it [00:05, 100.45it/s]\n",
      "602it [00:11, 53.56it/s] \n",
      "602it [00:10, 56.88it/s] \n",
      "602it [00:08, 68.57it/s] \n",
      "602it [00:12, 48.80it/s] \n",
      "602it [00:04, 142.21it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloaders, n_epochs=n_epochs, scorer=val_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f1', 0.6279356422833497)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=30)\n",
    "\n",
    "test_scorer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация фром скретч, обучаем 50 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertFinetuneModel(\n",
       "  (_backbone): Bert(\n",
       "    (_embeddings): BertEmbeddings(\n",
       "      (_token_embeddings): Embedding(30000, 256)\n",
       "      (position_embeddings): Embedding(32, 256)\n",
       "      (sentence_embeddings): Embedding(2, 256)\n",
       "      (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-3): 4 x BertLayer(\n",
       "        (_multihead_attention): MultiHeadSelfAttention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "        )\n",
       "        (_feedforward): FeedForward(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((256,), eps=0.001, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_classifier_head): ClassifierHead(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=96, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertFinetuneModel(\n",
    "    hidden_size=256, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seqlen=MAXLEN+2,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    "    num_classes=96,\n",
    "    act_func='gelu',\n",
    "    input_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    eps=1e-3, \n",
    ").to(device)\n",
    "model.apply(init_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, weight_decay=1e-2)\n",
    "n_epochs = 50\n",
    "\n",
    "scheduler = Scheduler(\n",
    "    optimizer=optimizer, \n",
    "    init_lr=1e-6, \n",
    "    peak_lr=2e-5, \n",
    "    final_lr=1e-9, \n",
    "    num_warmup_steps=int(len(dl)*0.05), \n",
    "    num_training_steps=len(dl)*n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = FinetuneTrainer(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    device=device,\n",
    "    logdir=f\"logs_ft/from scratch {n_epochs} epochs\",\n",
    "    max_grad_norm=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "602it [00:04, 141.92it/s]\n",
      "602it [00:04, 139.78it/s]\n",
      "602it [00:04, 138.46it/s]\n",
      "602it [00:04, 137.50it/s]\n",
      "602it [00:04, 140.14it/s]\n",
      "602it [00:04, 140.92it/s]\n",
      "602it [00:04, 137.28it/s]\n",
      "602it [00:04, 141.43it/s]\n",
      "602it [00:04, 140.52it/s]\n",
      "602it [00:04, 141.09it/s]\n",
      "602it [00:04, 139.53it/s]\n",
      "602it [00:04, 137.26it/s]\n",
      "602it [00:04, 136.95it/s]\n",
      "602it [00:04, 141.75it/s]\n",
      "602it [00:04, 135.46it/s]\n",
      "602it [00:04, 138.36it/s]\n",
      "602it [00:04, 138.72it/s]\n",
      "602it [00:04, 139.64it/s]\n",
      "602it [00:04, 142.06it/s]\n",
      "602it [00:04, 141.49it/s]\n",
      "602it [00:04, 140.86it/s]\n",
      "602it [00:04, 144.73it/s]\n",
      "602it [00:04, 139.40it/s]\n",
      "602it [00:04, 143.47it/s]\n",
      "602it [00:04, 141.83it/s]\n",
      "602it [00:04, 140.90it/s]\n",
      "602it [00:04, 140.40it/s]\n",
      "602it [00:04, 139.44it/s]\n",
      "602it [00:04, 139.30it/s]\n",
      "602it [00:04, 140.31it/s]\n",
      "602it [00:04, 143.84it/s]\n",
      "602it [00:04, 144.85it/s]\n",
      "602it [00:05, 119.00it/s]\n",
      "602it [00:04, 143.20it/s]\n",
      "602it [00:04, 144.01it/s]\n",
      "602it [00:04, 145.13it/s]\n",
      "602it [00:04, 145.16it/s]\n",
      "602it [00:04, 144.23it/s]\n",
      "602it [00:04, 144.87it/s]\n",
      "602it [00:04, 144.35it/s]\n",
      "602it [00:04, 143.93it/s]\n",
      "602it [00:04, 135.93it/s]\n",
      "602it [00:04, 145.89it/s]\n",
      "602it [00:04, 145.17it/s]\n",
      "602it [00:04, 145.56it/s]\n",
      "602it [00:04, 145.76it/s]\n",
      "602it [00:04, 144.24it/s]\n",
      "602it [00:04, 144.05it/s]\n",
      "602it [00:04, 145.35it/s]\n",
      "602it [00:04, 145.08it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloaders, n_epochs=n_epochs, scorer=val_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f1', 0.7439281384793099)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scorer = make_scorer(test['text'].values, test['label'].values, tokenizer, maxlen=30)\n",
    "\n",
    "test_scorer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопросы:**\n",
    "1. Попробуйте также обучить модель без предобученных весов (просто закомментировав загрузку весов). Насколько сильно просело качество?\n",
    "2. Влияет ли длительность предобучения (количество эпох) как-то существенно на дообучение, или достаточно одной эпохи?\n",
    "\n",
    "**Ваши ответы напишите здесь:**\n",
    "1. Как можно заметить на 10 эпохах сильная просадка по метрике, почти 0.2. Если обучать 50 эпох, то тоже наблюдается просадка, около 0.07. На 10 эпохах такая огрмная просадка из-за того, что предобученная модель уже имеет \"хорошие веса\" и ей нужно гораздо меньше эпох, чтобы выйти в оптимум. Чего не скажешь об обучении с нуля. Хотя на 50 эпохах судя по графикам модель выходит на плато, но результаты хуже. Отсюда можно сделать вывод, что предобучение довольно важная штука.\n",
    "2. Влияет. За 1 эпоху модель не успела сойтись. По графикам видно, что на плато она выходит как раз примерно на 10 эпохе. Далее обучение ничего не улучшает, на 50 эпохах она даже немного переобучилась."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть. Большие модели (максимум 3 балла)\n",
    "\n",
    "Предлагается обучить модель побольше:\n",
    "* `hidden_size` $\\in \\{512, 768, 1024\\}$\n",
    "* `num_hidden_layers` $\\in \\{8, 12, 24\\}$\n",
    "\n",
    "Например, BERT-base архитектура выглядит как `hidden_size=768, num_hidden_layers=12`.\n",
    "\n",
    "Для большой модели придется также использовать другие гиперпараметры - нужен learning rate поменьше, weight decay побольше, дропаут больше. Возможно потребуется больше эпох предобучения.\n",
    "\n",
    "За выполнение этой части можно получить **до пяти бонусных баллов**, бонус зависит от полученных на тесте значений метрики (должно быть видно существенное улучшение)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-kurtsev_sft_pipeline]",
   "language": "python",
   "name": "conda-env-.mlspace-kurtsev_sft_pipeline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
