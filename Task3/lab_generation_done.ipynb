{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUL0HkgCgcbj"
   },
   "source": [
    "# Практическое задание 3\n",
    "# Генерация bash команды по текстовому запросу\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "### ФИО:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr4csiUigcbl"
   },
   "source": [
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы построите систему, выдающую пользователю последовательность утилит командной строки linux (с нужными флагами) по его текстовому запросу. Вам дан набор пар текстовый запрос - команда на выходе.\n",
    "\n",
    "Решение этого задания будет построено на encoder-decoder архитектуре и модели transformer.\n",
    "\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    "* pytorch\n",
    "* transformers\n",
    "* sentencepiece (bpe токенизация)\n",
    "* clai utils (скачать с гитхаба отсюда https://github.com/IBM/clai/tree/nlc2cmd/utils)\n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "В качестве обучающей выборке используются данные, сгенерированные автоматически по запросам с сайта stack overflow. В качестве тестовых данных используются пары запросов, размеченные асессорами.\n",
    "\n",
    "Данные можно скачать по ссылке: https://drive.google.com/file/d/1n457AAgrMwd5VbT6mGZ_rws3g2wwdEfX/view?usp=sharing\n",
    "\n",
    "### Метрика качества\n",
    "\n",
    "Ваш алгоритм должен выдавать пять вариантов ответа для каждого запроса.\n",
    "Для упрощения задачи метрика качества будет учитывать утилиты и флаги ответа, но не учитывать подставленные значения. Пусть $\\{ u_1, \\ldots, u_T \\}$, $\\{ f_1, \\ldots, f_T \\}$ --- список утилит и множества их флагов ответа алгоритма, $\\{v_1, \\ldots, v_T \\}$, $\\{ \\phi_1, \\ldots, \\phi_T \\}$ --- список утилит и множества их флагов эталонного ответа. Если ответы отличаются по длине, они дополняются `None` утилитой.\n",
    "\n",
    "$$ S = \\frac{1}{T} \\sum_{i=1}^{T} \\left(\\mathbb{I}[u_i = v_i]\\left( 1 + \\frac{1}{2}s(f_i, \\phi_i)\\right) - 1\\right)$$\n",
    "\n",
    "$$ s(f, \\phi) = 1 + \\frac{2 |f \\cap \\phi| - |f \\cup \\phi|}{\\max(|f|, |\\phi|)} $$\n",
    "\n",
    "Метрика учитывает, что предсказать правильную утилиту важнее чем правильный флаг. При этом порядок флагов не важен (однако, чтобы корректно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXPysIwzgcbm"
   },
   "source": [
    "## Предобработка данных (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WED6TGldgcbm"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "PATH_TO_CLAI_UTILS = \"nlp/task_last/clai/utils\"\n",
    "# sys.path.append(PATH_TO_CLAI_UTILS)\n",
    "sys.path.insert(0, PATH_TO_CLAI_UTILS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "KuOn0E2Ggcbn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bashlint.data_tools import bash_parser, pretty_print, cmd2template\n",
    "from metric.metric_utils import compute_metric\n",
    "from functools import partial\n",
    "\n",
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPDxvJ8jgcbn"
   },
   "source": [
    "Считаем данные. В столбце `invocation` находится текстовый запрос, в столбце `cmd` находится релевантная команда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "id": "P_ixsl2fgcbo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>copy loadable kernel module \"mymodule.ko\" to t...</td>\n",
       "      <td>sudo cp mymodule.ko /lib/modules/$(uname -r)/k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>display all lines containing \"ip_mroute\" in th...</td>\n",
       "      <td>cat /boot/config-`uname -r` | grep IP_MROUTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>display current running kernel's compile-time ...</td>\n",
       "      <td>cat /boot/config-`uname -r`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find all loadable modules for current kernel, ...</td>\n",
       "      <td>find /lib/modules/`uname -r` -regex .*perf.*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look for any instance of \"highmem\" in the curr...</td>\n",
       "      <td>grep “HIGHMEM” /boot/config-`uname -r`</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          invocation  \\\n",
       "0  copy loadable kernel module \"mymodule.ko\" to t...   \n",
       "1  display all lines containing \"ip_mroute\" in th...   \n",
       "2  display current running kernel's compile-time ...   \n",
       "3  find all loadable modules for current kernel, ...   \n",
       "4  look for any instance of \"highmem\" in the curr...   \n",
       "\n",
       "                                                 cmd  \n",
       "0  sudo cp mymodule.ko /lib/modules/$(uname -r)/k...  \n",
       "1       cat /boot/config-`uname -r` | grep IP_MROUTE  \n",
       "2                        cat /boot/config-`uname -r`  \n",
       "3       find /lib/modules/`uname -r` -regex .*perf.*  \n",
       "4             grep “HIGHMEM” /boot/config-`uname -r`  "
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTYR1skvgcbo"
   },
   "source": [
    "В тестовых данных столбец `origin` отвечает за источник данных, значения `handrafted` соответствуют парам, составленными людьми, а `mined` парам, собранным автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "id": "WyIuvXUjgcbp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>create ssh connection to specified ip from spe...</td>\n",
       "      <td>ssh user123@176.0.13.154</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>search for commands containing string \"zeppeli...</td>\n",
       "      <td>history | grep zeppelin</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>search for location of specified file or appli...</td>\n",
       "      <td>whereis python3</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grant all rights to root folder</td>\n",
       "      <td>sudo chmod 777 -R /</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>search in running processes for specified name</td>\n",
       "      <td>ps -aux | grep zepp</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          invocation  \\\n",
       "0  create ssh connection to specified ip from spe...   \n",
       "1  search for commands containing string \"zeppeli...   \n",
       "2  search for location of specified file or appli...   \n",
       "3                    grant all rights to root folder   \n",
       "4     search in running processes for specified name   \n",
       "\n",
       "                        cmd       origin  \n",
       "0  ssh user123@176.0.13.154  handcrafted  \n",
       "1   history | grep zeppelin  handcrafted  \n",
       "2           whereis python3  handcrafted  \n",
       "3       sudo chmod 777 -R /  handcrafted  \n",
       "4       ps -aux | grep zepp  handcrafted  "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdYgRqVRgcbp"
   },
   "source": [
    "**Задание**. Проведите предобработку текста. Рекомендуется:\n",
    "* перевести всё в нижний регистр\n",
    "* удалить стоп-слова (специфичные для выборки)\n",
    "* провести стемминг токенов\n",
    "* удалить все символы кроме латинских букв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "id": "lEGi7vshgcbp"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def clean_text(text):\n",
    "    ### YOUR CODE HERE ###\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text.lower())\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "id": "zJgPZssQgcbp"
   },
   "outputs": [],
   "source": [
    "train_data['text_cleaned'] = train_data['invocation'].apply(clean_text)\n",
    "test_data['text_cleaned'] = test_data['invocation'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4vu9Sc4gcbq"
   },
   "source": [
    "Для обработки кода воспользуемся функцией `cmd2template`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "id": "-gAZL7M7gcbq"
   },
   "outputs": [],
   "source": [
    "train_data['cmd_cleaned'] = train_data['cmd'].apply(partial(cmd2template, loose_constraints=True))\n",
    "test_data['cmd_cleaned'] = test_data['cmd'].apply(partial(cmd2template, loose_constraints=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j09aHCUDgcbq"
   },
   "source": [
    "Разделим данные на обучение и валидацию. Т.к. данных очень мало, то для валидационной выборки выделим только 100 примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "id": "jH3FdRjYgcbq"
   },
   "outputs": [],
   "source": [
    "valid_data = train_data.iloc[-100:]\n",
    "train_data = train_data.iloc[:-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFCd9selgcbq"
   },
   "source": [
    "**Задание**. Стандартный формат входных данных для трансформеров — BPE токены. Воспользуйтесь библиотекой sentencepiece для обучения токенайзеров для текста и кода. Используйте небольшое число токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "id": "mWcmAe14gcbq"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "id": "iegdQsmegcbr"
   },
   "outputs": [],
   "source": [
    "train_data[\"text_cleaned\"].to_csv(\"text_cleaned.txt\", index=False, header=False)\n",
    "train_data[\"cmd_cleaned\"].to_csv(\"cmd_cleaned.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text_cleaned.txt\n",
      "  input_format: \n",
      "  model_prefix: text_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: text_cleaned.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9843 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=571304\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 9843 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 9843\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 4071\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16624 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4964 size=20 all=1044 active=1017 piece=ar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2503 size=40 all=1531 active=1504 piece=am\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1550 size=60 all=1930 active=1903 piece=▁b\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1090 size=80 all=2328 active=2301 piece=mp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=848 size=100 all=2647 active=2620 piece=an\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=843 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=666 size=120 all=2952 active=1257 piece=▁modi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=553 size=140 all=3205 active=1510 piece=nam\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=488 size=160 all=3374 active=1679 piece=▁day\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=414 size=180 all=3712 active=2017 piece=▁who\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=381 size=200 all=3872 active=2177 piece=▁remove\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=378 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=337 size=220 all=4121 active=1249 piece=▁my\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=291 size=240 all=4344 active=1472 piece=ard\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=263 size=260 all=4452 active=1580 piece=▁char\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=241 size=280 all=4576 active=1704 piece=ire\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=212 size=300 all=4724 active=1852 piece=pt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=211 min_freq=19\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=200 size=320 all=4822 active=1083 piece=▁sho\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=183 size=340 all=4951 active=1212 piece=▁al\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=167 size=360 all=5032 active=1293 piece=und\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=380 all=5117 active=1378 piece=▁gz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=400 all=5233 active=1494 piece=amp\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=144 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=134 size=420 all=5388 active=1146 piece=▁less\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=126 size=440 all=5475 active=1233 piece=▁exp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=116 size=460 all=5518 active=1276 piece=▁starting\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=107 size=480 all=5555 active=1313 piece=ap\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=500 all=5676 active=1434 piece=ect\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=102 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=95 size=520 all=5783 active=1096 piece=▁inf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=540 all=5846 active=1159 piece=▁subdirectories\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=560 all=5885 active=1198 piece=▁like\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=76 size=580 all=5976 active=1289 piece=▁def\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=600 all=6026 active=1339 piece=ep\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=71 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=620 all=6084 active=1050 piece=▁error\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63 size=640 all=6175 active=1141 piece=▁null\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=660 all=6208 active=1174 piece=▁pdf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=680 all=6259 active=1225 piece=▁myfile\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=700 all=6305 active=1271 piece=▁contains\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=56 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=720 all=6340 active=1036 piece=▁numer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=740 all=6385 active=1081 piece=▁gets\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=760 all=6424 active=1120 piece=▁resi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=780 all=6495 active=1191 piece=▁sk\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=800 all=6536 active=1232 piece=ically\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=43 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=820 all=6573 active=1033 piece=sv\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=840 all=6614 active=1074 piece=▁compressed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=860 all=6658 active=1118 piece=▁entr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=880 all=6699 active=1159 piece=▁ut\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=900 all=6750 active=1210 piece=▁common\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=34 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=920 all=6791 active=1042 piece=▁needed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=940 all=6832 active=1083 piece=▁following\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=960 all=6864 active=1115 piece=glob\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=980 all=6941 active=1192 piece=side\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1000 all=6966 active=1217 piece=la\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1020 all=6984 active=1012 piece=▁numerically\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1040 all=7017 active=1045 piece=▁messages\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1060 all=7077 active=1105 piece=▁columns\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1080 all=7087 active=1115 piece=▁occurrence\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1100 all=7133 active=1161 piece=▁dump\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1120 all=7136 active=1003 piece=▁description\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1140 all=7168 active=1035 piece=▁python\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1160 all=7192 active=1059 piece=▁wee\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1180 all=7225 active=1092 piece=lease\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1200 all=7215 active=1082 piece=▁configuration\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1220 all=7243 active=1029 piece=atever\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1240 all=7276 active=1062 piece=▁rs\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1260 all=7293 active=1079 piece=▁passwd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1280 all=7316 active=1102 piece=▁sql\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1300 all=7322 active=1108 piece=▁basename\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1320 all=7392 active=1071 piece=reen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1340 all=7409 active=1088 piece=▁requests\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1360 all=7449 active=1128 piece=▁dat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1380 all=7455 active=1134 piece=▁alphabet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1400 all=7496 active=1175 piece=ware\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1420 all=7500 active=1002 piece=▁lookup\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=1440 all=7537 active=1039 piece=▁tex\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=1460 all=7546 active=1048 piece=▁password\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=1480 all=7601 active=1103 piece=term\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=1500 all=7604 active=1106 piece=▁epoch\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=1520 all=7595 active=992 piece=▁different\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=1540 all=7646 active=1043 piece=dout\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=1560 all=7669 active=1066 piece=▁win\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=1580 all=7674 active=1071 piece=▁somed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=1600 all=7661 active=1058 piece=▁nullglob\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=1620 all=7688 active=1028 piece=▁tm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=1640 all=7720 active=1060 piece=▁mbox\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=1660 all=7725 active=1065 piece=▁copying\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=1680 all=7710 active=1050 piece=▁searching\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=1700 all=7740 active=1080 piece=pat\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=1720 all=7784 active=1040 piece=▁ins\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=1740 all=7780 active=1036 piece=▁semic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=1760 all=7770 active=1026 piece=▁profile\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=1780 all=7797 active=1053 piece=aps\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=1800 all=7845 active=1101 piece=rary\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=1820 all=7856 active=1010 piece=unnel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=1840 all=7862 active=1016 piece=▁fstab\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=1860 all=7863 active=1017 piece=▁headers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=1880 all=7854 active=1008 piece=nu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=1900 all=7883 active=1037 piece=rint\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=1920 all=7901 active=1013 piece=▁pad\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=1940 all=7906 active=1018 piece=▁park\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=1960 all=7905 active=1017 piece=▁trace\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: text_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: text_tokenizer.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: cmd_cleaned.txt\n",
      "  input_format: \n",
      "  model_prefix: cmd_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: cmd_cleaned.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9843 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=388781\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9532% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999532\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 9843 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 9843\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 815\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=23971 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3742 size=20 all=780 active=710 piece=ame\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1525 size=40 all=910 active=840 piece=▁s\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1018 size=60 all=981 active=911 piece=▁Program\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=641 size=80 all=1105 active=1035 piece=ermission\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=453 size=100 all=1152 active=1082 piece=▁echo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=447 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=346 size=120 all=1184 active=1033 piece=max\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=262 size=140 all=1198 active=1047 piece=prune\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=180 size=160 all=1248 active=1097 piece=▁read\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=151 size=180 all=1297 active=1146 piece=uniq\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=200 all=1368 active=1217 piece=mind\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=122 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=100 size=220 all=1409 active=1041 piece=and\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=240 all=1425 active=1057 piece=▁basename\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=260 all=1434 active=1066 piece=cl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=280 all=1459 active=1091 piece=fig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=300 all=1456 active=1088 piece=exclude\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=49 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=320 all=1470 active=1014 piece=less\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=340 all=1470 active=1014 piece=▁whoami\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=360 all=1469 active=1013 piece=▁pstree\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=380 all=1506 active=1050 piece=▁ps\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=400 all=1523 active=1067 piece=ot\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=420 all=1564 active=1036 piece=▁source\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: cmd_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: cmd_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE ###\n",
    "\n",
    "text_tokenizer = spm.SentencePieceTrainer.train(\n",
    "    input=\"text_cleaned.txt\",\n",
    "    model_prefix=\"text_tokenizer\",\n",
    "    vocab_size=2000,\n",
    "    model_type=\"bpe\",\n",
    "    pad_id=0,\n",
    "    unk_id=3,\n",
    ")\n",
    "\n",
    "cmd_tokenizer = spm.SentencePieceTrainer.train(\n",
    "    input=\"cmd_cleaned.txt\",\n",
    "    model_prefix=\"cmd_tokenizer\",\n",
    "    vocab_size=500,\n",
    "    model_type=\"bpe\",\n",
    "    pad_id=0,\n",
    "    unk_id=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-FuiyWkgcbr"
   },
   "source": [
    "**Задание**. Задайте датасеты и лоадеры для ваших данных. Каждая последовательность должна начинаться с BOS токена и заканчиваться EOS токеном. Рекомендуется ограничить длину входных и выходных последовательностей!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "id": "mxu5Vp7ogcbr"
   },
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "\n",
    "\n",
    "MAX_TEXT_LENGTH = 256\n",
    "MAX_CODE_LENGTH = 40\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "id": "s071TM0Rgcbr"
   },
   "outputs": [],
   "source": [
    "class TextToBashDataset(Dataset):\n",
    "    ## YOUR CODE HERE ###\n",
    "    def __init__(self, corpus_text, corpus_cmd, text_tokenizer, cmd_tokenizer):\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.cmd_tokenizer = cmd_tokenizer\n",
    "        self.corpus_text, self.corpus_cmd = [], []\n",
    "        for text, cmd in zip(corpus_text, corpus_cmd):\n",
    "            tokens_text = self.text_tokenizer.encode_as_ids(text)[:MAX_TEXT_LENGTH - 2]\n",
    "            tokens_cmd = self.cmd_tokenizer.encode_as_ids(cmd)[:MAX_CODE_LENGTH - 2]\n",
    "\n",
    "            self.corpus_text.append([BOS_ID] + tokens_text + [EOS_ID])\n",
    "            self.corpus_cmd.append([BOS_ID] + tokens_cmd + [EOS_ID])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.corpus_text[idx]), torch.tensor(self.corpus_cmd[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, cmds = [], []\n",
    "    for sample in batch:\n",
    "        text, cmd = sample\n",
    "        texts.append(text)\n",
    "        cmds.append(cmd)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=PAD_ID)\n",
    "    cmds = pad_sequence(cmds, batch_first=True, padding_value=PAD_ID)\n",
    "    \n",
    "    labels = torch.empty_like(cmds)\n",
    "    labels.copy_(cmds)\n",
    "    return texts, cmds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "id": "ALHK0KNEgcbr"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ###\n",
    "\n",
    "text_tokenizer = spm.SentencePieceProcessor(model_file=\"text_tokenizer.model\")\n",
    "cmd_tokenizer = spm.SentencePieceProcessor(model_file=\"cmd_tokenizer.model\")\n",
    "train_ds = TextToBashDataset(train_data[\"text_cleaned\"].tolist(), train_data[\"cmd_cleaned\"].tolist(), text_tokenizer, cmd_tokenizer)\n",
    "valid_ds = TextToBashDataset(valid_data[\"text_cleaned\"].tolist(), valid_data[\"cmd_cleaned\"].tolist(), text_tokenizer, cmd_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "id": "14slMAPtgcbr"
   },
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train': DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True),\n",
    "    'valid': DataLoader(valid_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sgpe-nN0gcbs"
   },
   "source": [
    "## Обучение бейзлайна (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "id": "0Tv-uPk-gcbs"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, EncoderDecoderConfig, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxSJa2-jgcbs"
   },
   "source": [
    "**Задание.** Реализуйте модель encoder-decoder ниже. В качестве моделей энкодера и декодера рекомендуется использовать BertModel из библиотеки transformers, заданную через BertConfig. В случае декодера необходимо выставить параметры is_decoder=True и add_cross_attention=True. В качестве модели, <<сцепляющей>> энкодер и декодер, в одну архитектуру рекомендуется использовать EncoderDecoderModel.\n",
    "\n",
    "**Обратите внимание!** EncoderDecoderModel поддерживает использование кэшированных результатов при последовательной генерации. Это пригодится при реализации beam-search ниже.\n",
    "\n",
    "Для того, чтобы удобнее задавать модели, рекомендуется реализовать задание модели через конфиг. Ниже представлены базовые параметры, при которых модель должна работать быстро и с приемлемым качеством."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "id": "1meXHP5wgcbs"
   },
   "outputs": [],
   "source": [
    "text_model_config = {\n",
    "    'vocab_size': text_tokenizer.vocab_size(),\n",
    "    'hidden_size': 256,\n",
    "    'num_hidden_layers': 2,\n",
    "    'num_attention_heads': 8,\n",
    "    'intermediate_size': 256 * 4,\n",
    "    'hidden_dropout_prob': 0.25,\n",
    "    'pad_token_id': PAD_ID,\n",
    "}\n",
    "\n",
    "cmd_model_config = {\n",
    "    'vocab_size': cmd_tokenizer.vocab_size(),\n",
    "    'hidden_size': 256,\n",
    "    'num_hidden_layers': 2,\n",
    "    'num_attention_heads': 8,\n",
    "    'intermediate_size': 256 * 4,\n",
    "    'hidden_dropout_prob': 0.25,\n",
    "    'pad_token_id': PAD_ID,\n",
    "    'decoder_start_token_id': BOS_ID,\n",
    "    'eos_token_id': EOS_ID,\n",
    "    'is_decoder': True,\n",
    "    'add_cross_attention': True \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "id": "zg0GGP1Lgcbs"
   },
   "outputs": [],
   "source": [
    "class TextToBashModel(nn.Module):\n",
    "    def __init__(self, text_model_config, cmd_model_config):\n",
    "        super(TextToBashModel, self).__init__()\n",
    "        ## YOUR CODE HERE ##\n",
    "        encoder_config = BertConfig(**text_model_config)\n",
    "        decoder_config = BertConfig(**cmd_model_config)\n",
    "\n",
    "        self.config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n",
    "        self.model = EncoderDecoderModel(config=self.config)\n",
    "        \n",
    "        self.model.config.decoder_start_token_id = decoder_config.decoder_start_token_id\n",
    "        self.model.config.pad_token_id = decoder_config.pad_token_id\n",
    "        self.model.config.eos_token_id = decoder_config.eos_token_id\n",
    "        self.model.config.vocab_size = cmd_tokenizer.vocab_size()\n",
    "\n",
    "    def forward(self, input_ids, decoder_input_ids, train=True):\n",
    "        ## YOUR CODE HERE ##\n",
    "        if train:\n",
    "             return self.model(input_ids=input_ids, labels=decoder_input_ids)\n",
    "        else:\n",
    "            return self.model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "    def generate(self, input_ids, attention_mask, num_beams=5, temperature=1.0, max_length=20):\n",
    "        return self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=num_beams,\n",
    "            temperature=temperature,\n",
    "            max_length=max_length,\n",
    "            early_stopping=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz5BSTAIgcbs"
   },
   "source": [
    "**Задание**. Обучите вашу модель ниже.\n",
    "\n",
    "Рекомендуется:\n",
    "* в качестве лосса использовать стандартную кросс-энтропию, не забывайте игнорировать PAD токены\n",
    "* использовать Adam для оптимизации\n",
    "* не использовать scheduler для бейзлайна (модель легко переобучается с ним)\n",
    "* использовать early stopping по валидационному лоссу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "id": "go3vQCyagcbs"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_n\n",
    "\n",
    "def train(model, criterion, optimizer, dataloaders, n_epochs=20, device='cuda'):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in tqdm_n(range(n_epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in loaders[\"train\"]:\n",
    "            input_ids, cmd_ids, _ = batch\n",
    "            input_ids, cmd_ids = input_ids.to(device), cmd_ids.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, decoder_input_ids=cmd_ids)\n",
    "            loss = outputs[\"loss\"]\n",
    "            # loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().cpu().item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(loaders[\"train\"])\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in loaders[\"valid\"]:\n",
    "                input_ids, cmd_ids, _ = batch\n",
    "                input_ids, cmd_ids = input_ids.to(device), cmd_ids.to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, decoder_input_ids=cmd_ids)\n",
    "                loss = outputs[\"loss\"]\n",
    "                # loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "                val_loss += loss.detach().cpu().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(loaders[\"valid\"])\n",
    "        print(f\"Epoch {epoch} Loss Train/Val: {avg_train_loss:.4f}/{avg_val_loss:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f\"best_model.pt\")\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"model_epoch_{epoch}.pt\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc7d01e1b1b48cca7bb1f7bad08637e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss Train/Val: 1.8340/0.7221\n",
      "Epoch 1 Loss Train/Val: 0.6429/0.5354\n",
      "Epoch 2 Loss Train/Val: 0.5163/0.4438\n",
      "Epoch 3 Loss Train/Val: 0.4463/0.4027\n",
      "Epoch 4 Loss Train/Val: 0.3926/0.3669\n",
      "Epoch 5 Loss Train/Val: 0.3603/0.3488\n",
      "Epoch 6 Loss Train/Val: 0.3351/0.3368\n",
      "Epoch 7 Loss Train/Val: 0.3119/0.3297\n",
      "Epoch 8 Loss Train/Val: 0.2884/0.3054\n",
      "Epoch 9 Loss Train/Val: 0.2755/0.2984\n",
      "Epoch 10 Loss Train/Val: 0.2557/0.3001\n",
      "Epoch 11 Loss Train/Val: 0.2426/0.3001\n",
      "Epoch 12 Loss Train/Val: 0.2320/0.2981\n",
      "Epoch 13 Loss Train/Val: 0.2196/0.2850\n",
      "Epoch 14 Loss Train/Val: 0.2073/0.2844\n",
      "Epoch 15 Loss Train/Val: 0.1984/0.2875\n",
      "Epoch 16 Loss Train/Val: 0.1896/0.2886\n",
      "Epoch 17 Loss Train/Val: 0.1767/0.2784\n",
      "Epoch 18 Loss Train/Val: 0.1718/0.2865\n",
      "Epoch 19 Loss Train/Val: 0.1639/0.2896\n",
      "Epoch 20 Loss Train/Val: 0.1612/0.2858\n",
      "Epoch 21 Loss Train/Val: 0.1520/0.2752\n",
      "Epoch 22 Loss Train/Val: 0.1473/0.2812\n",
      "Epoch 23 Loss Train/Val: 0.1401/0.2744\n",
      "Epoch 24 Loss Train/Val: 0.1354/0.2817\n",
      "Epoch 25 Loss Train/Val: 0.1297/0.2808\n",
      "Epoch 26 Loss Train/Val: 0.1264/0.2877\n",
      "Epoch 27 Loss Train/Val: 0.1203/0.2793\n",
      "Epoch 28 Loss Train/Val: 0.1172/0.2919\n",
      "Epoch 29 Loss Train/Val: 0.1138/0.2898\n"
     ]
    }
   ],
   "source": [
    "model = TextToBashModel(text_model_config, cmd_model_config).to('cuda')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "model_trained = train(model, criterion, optimizer, loaders, n_epochs=30, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAD20fkPgcbs"
   },
   "source": [
    "## Генерация команд (2 балла)\n",
    "\n",
    "**Задание**. Реализуйте алгоритм beam-search в классе BeamSearchGenerator ниже. Ваша реализация должна поддерживать задание температуры софтмакса. Выходы модели, полученные на предыдущих итерациях, необходимо кэшировать для повышения скорости алгоритма. Вместо подсчёта произведения любых вероятностей необходимо считать сумму их логарифмов.\n",
    "\n",
    "Алгоритм должен возвращать список пар из получившихся выходных последовательностей и логарифмов их вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "id": "7ouLaM_Rgcbt"
   },
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    def __init__(\n",
    "            self, pad_id, eos_id, bos_id,\n",
    "            max_length=20, beam_width=5, temperature=1,\n",
    "            device='cuda',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        pad_id : int\n",
    "        eos_id : int\n",
    "        bos_id : int\n",
    "        max_length : int\n",
    "            Maximum length of output sequence\n",
    "        beam_width : int\n",
    "            Width of the beam\n",
    "        temperature : float\n",
    "            Softmax temperature\n",
    "        device : torch.device\n",
    "            Your model device\n",
    "        \"\"\"\n",
    "        self.pad_id = pad_id\n",
    "        self.eos_id = eos_id\n",
    "        self.bos_id = bos_id\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.beam_width = beam_width\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def get_result_generate(self, model, input_text_tokens):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : TextToBashModel\n",
    "        input_text_tokens : torch.tensor\n",
    "            One object input tensor\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE ##\n",
    "        attention_mask = (input_text_tokens != self.pad_id).float()\n",
    "        return [model.generate(input_text_tokens, attention_mask, num_beams=self.beam_width,\n",
    "                              temperature=self.temperature, max_length=self.max_length) for _ in range(self.beam_width)]\n",
    "\n",
    "\n",
    "    def get_result(self, model, input_text_tokens):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : TextToBashModel\n",
    "        input_text_tokens : torch.tensor\n",
    "            One object input tensor\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE ##\n",
    "        attention_mask = (input_text_tokens != self.pad_id).float()\n",
    "        \n",
    "        generated_ids = [(torch.tensor([self.bos_id], device=self.device), 0.0)]\n",
    "        cache = {}\n",
    "\n",
    "        for _ in range(self.max_length):\n",
    "            all_candidates = []\n",
    "\n",
    "            for generated_seq, log_prob in generated_ids:\n",
    "                if generated_seq[-1].item() == self.eos_id:\n",
    "                    all_candidates.append((generated_seq, log_prob))\n",
    "                    continue\n",
    "\n",
    "                seq_key = tuple(generated_seq.tolist())\n",
    "                if seq_key in cache:\n",
    "                    logits = cache[seq_key]\n",
    "                else:\n",
    "                    decoder_attention_mask = (generated_seq != self.pad_id).float().unsqueeze(0)\n",
    "                    logits = model(input_text_tokens, generated_seq.unsqueeze(0), train=False)\n",
    "                    logits = logits[\"logits\"].squeeze(0)\n",
    "                    cache[seq_key] = logits\n",
    "\n",
    "                logits = logits / self.temperature\n",
    "                probs = F.log_softmax(logits[-1], dim=-1)\n",
    "                for tok_id in range(probs.size(0)):\n",
    "                    new_seq = torch.cat([generated_seq, torch.tensor([tok_id], device=self.device)])\n",
    "                    new_log_prob = log_prob + probs[tok_id].item()\n",
    "                    all_candidates.append((new_seq, new_log_prob))\n",
    "\n",
    "            generated_ids = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:self.beam_width]\n",
    "            if all(gen_seq[-1].item() == self.eos_id for gen_seq, _ in generated_ids):\n",
    "                break\n",
    "        return [(gen_seq.tolist(), log_prob) for gen_seq, log_prob in generated_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IeMMcrtgcbt"
   },
   "source": [
    "Протестируйте на нескольких примерах работу вашего алгоритма. Если всё реализовано правильно, то как минимум на трёх примерах из 5 всё должно работать правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "id": "PGvmHpOBgcbt"
   },
   "outputs": [],
   "source": [
    "beam_search_engine = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=10,\n",
    "    temperature=1, device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TextToBashModel(text_model_config, cmd_model_config)\n",
    "# model.load_state_dict(torch.load(\"model_epoch_19.pt\"))\n",
    "# model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "id": "tJqMLfWjgcbt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text: write \"deb blah ... blah\" to standard output and append to \"/etc/apt/sources.list\" as root\n",
      "true: echo 'deb blah ... blah' | sudo tee --append /etc/apt/sources.list\n",
      "true cleaned: echo Regex | tee --append File\n",
      "echo Regex | tee -a File -0.8335579037666321\n",
      "echo Regex | tee File -1.3231375217437744\n",
      "cat File | tee File -3.460297107696533\n",
      "echo -e Regex | tee -a File -4.422310829162598\n",
      "echo Regex Regex | tee -a File -4.563908100128174\n",
      "cat File | tee -a File -4.879418849945068\n",
      "echo Regex | tee -a File File -4.90671443939209\n",
      "mv File File -4.956876277923584\n",
      "cut -d Regex -f Number File -5.345022201538086\n",
      "cut -d Regex -f Number | tee File -5.722093105316162\n",
      "0.5\n",
      "\n",
      "text: find all the files in the entire filesystem which belong to the group root and display the ten files.\n",
      "true: find / -group root | head\n",
      "true cleaned: find Path -group Regex | head\n",
      "find Path -group Regex | head -0.880927562713623\n",
      "find Path -group Regex -perm -Permission | head -1.5331900119781494\n",
      "find Path -group Regex -perm -Permission -3.706540107727051\n",
      "find Path -user Regex -perm -Permission | head -3.9070072174072266\n",
      "find Path -group Regex -print | head -4.0691728591918945\n",
      "find Path -group Regex | tail -4.184520721435547\n",
      "find Path -user Regex | head -4.280677318572998\n",
      "find Path -group Regex | head - Quantity -4.670697212219238\n",
      "find Path -group Regex -or -group Regex | head -4.675293445587158\n",
      "find Path -nouser -or -group Regex | head -4.727911949157715\n",
      "1.0\n",
      "\n",
      "text: counts lines in each *.php file sorted by file path.\n",
      "true: find . -name '*.php' -type f | sort | xargs wc -l\n",
      "true cleaned: find Path -name Regex -type f | sort | xargs -I {} wc -l {}\n",
      "wc -l File -3.5008292198181152\n",
      "wc -l File File -3.5763418674468994\n",
      "cat File File | comm -2 -3 File File File -4.7135467529296875\n",
      "cat File File | sort -o File File -4.994835376739502\n",
      "find Path -name Regex -exec wc -l {} + | sort -n -6.4231157302856445\n",
      "find Path -type f -name Regex -exec wc -l {} + | sort -n -7.592339038848877\n",
      "find Path -type f -name Regex -exec sed --in-place Regex {} + -8.450703620910645\n",
      "find Path -type f -name Regex -exec sed --in-place Regex {} + | comm -n Program -8.745849609375\n",
      "find Path -type f -name Regex -exec sed --in-place Regex {} + | sed -n Program -8.901455879211426\n",
      "find Path -type f -name Regex -exec sed --in-place Regex {} + | sort -n -k Number -11.014602661132812\n",
      "-0.625\n",
      "\n",
      "text: find all *.c files on the system and feed the output to wc\n",
      "true: find / -name *.c | wc\n",
      "true cleaned: find Path -name Regex | wc\n",
      "find Path -name Regex -1.4791297912597656\n",
      "find Path -type f -name Regex -2.3899755477905273\n",
      "find Path -name Regex | tee File -2.7057747840881348\n",
      "find Path -type f \\( -name Regex -or -name Regex \\) -3.51383113861084\n",
      "find Path -print | tee -a File -3.75118088722229\n",
      "find Path -user Regex -3.7925095558166504\n",
      "find Path -name Regex -print | tee -a File -4.049143314361572\n",
      "find Path -name Regex -print | tee File -4.095728397369385\n",
      "find Path -type f -name Regex | tee File -4.335371971130371\n",
      "find Path -type f \\( -name Regex -or -name Regex -or -name Regex \\) -4.685107707977295\n",
      "0.0\n",
      "\n",
      "text: print sed commands that would replace all occurrences of 'previousword' with 'newword' in all regular files with '.cpp' extension under '/myprojects' directory tree\n",
      "true: find /myprojects -type f -name '*.cpp' -print0 |    xargs -0 echo sed -i 's/previousword/newword/g'\n",
      "true cleaned: find Path -type f -name Regex -print0 | xargs -0 -I {} echo Regex Regex Regex {}\n",
      "find Path -type f -name Regex -print0 | xargs -0 -I {} sed -i Program {} -1.7900446653366089\n",
      "find Path -type f -name Regex -exec sed -i Program {} \\; -2.104804515838623\n",
      "find Path -type f -print0 | xargs -0 -I {} sed -i Program {} -3.0101749897003174\n",
      "find Path -type f -name Regex -exec sed --in-place Regex {} \\; -3.1379013061523438\n",
      "find Path -type f -name Regex -exec sed --in-place Regex {} + -3.348323345184326\n",
      "find Path -type f -exec sed -i Program {} \\; -3.386105537414551\n",
      "find Path -type f -name Regex -exec sed Program {} \\; -3.6271324157714844\n",
      "find Path -type f -name Regex -exec sed -i Program {} + -3.8061256408691406\n",
      "find Path -type f -exec sed -i Program {} + -4.0156121253967285\n",
      "find Path -type f -name Regex -print0 | xargs -0 -n Quantity -I {} sed -i Program {} -4.200589179992676\n",
      "0.22222222222222218\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print('text:', valid_data.invocation.iloc[i])\n",
    "        print('true:', valid_data.cmd.iloc[i])\n",
    "        print('true cleaned:', valid_data.cmd_cleaned.iloc[i])\n",
    "\n",
    "        src = torch.tensor(text_tokenizer.encode(valid_data.text_cleaned.tolist()[i])).to(\"cuda\").unsqueeze(0)\n",
    "        pred = beam_search_engine.get_result_generate(model, src)\n",
    "        score = compute_metric(pred, 1, valid_data.cmd.iloc[i])\n",
    "        # print(\"default generate: \", cmd_tokenizer.decode(pred.tolist())[0], score)\n",
    "\n",
    "        pred = beam_search_engine.get_result(model, src)\n",
    "        scores = []\n",
    "        for x, proba in pred:\n",
    "            pred_cmd = cmd_tokenizer.decode(list(map(int, x)))\n",
    "            score = compute_metric(pred_cmd, 1, valid_data.cmd.iloc[i])\n",
    "            scores.append(score)\n",
    "            print(pred_cmd, proba)\n",
    "        print(max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjSlUHqbgcbt"
   },
   "source": [
    "**Задание**. Дополните функцию для подсчёта качества. Посчитайте качество вашей модели на валидационном и тестовых датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "id": "sJiW5_5dgcbt"
   },
   "outputs": [],
   "source": [
    "def compute_all_scores(model, df, beam_engine):\n",
    "    all_scores = []\n",
    "\n",
    "    for text, target_cmd in tqdm(zip(df.text_cleaned.values, df.cmd.values), total=len(df.cmd.values)):\n",
    "        ## YOUR CODE HERE ##\n",
    "        input_tokens = torch.tensor([BOS_ID] + text_tokenizer.encode_as_ids(text) + [EOS_ID]).to(\"cuda\").unsqueeze(0)\n",
    "        predictions = beam_engine.get_result(model, input_tokens)\n",
    "\n",
    "        # get only 5 top results\n",
    "        predictions = predictions[:5]\n",
    "        object_scores = []\n",
    "        for output_tokens, proba in predictions:\n",
    "            output_cmd = cmd_tokenizer.decode(list(map(int, output_tokens)))\n",
    "            score = compute_metric(output_cmd, 1, target_cmd)\n",
    "            object_scores.append(score)\n",
    "\n",
    "        all_scores.append(max(object_scores))\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_scores_generate(model, df, beam_engine):\n",
    "    all_scores = []\n",
    "\n",
    "    for text, target_cmd in tqdm(zip(df.text_cleaned.values, df.cmd.values), total=len(df.cmd.values)):\n",
    "        ## YOUR CODE HERE ##\n",
    "        input_tokens = torch.tensor([BOS_ID] + text_tokenizer.encode_as_ids(text) + [EOS_ID]).to(\"cuda\").unsqueeze(0)\n",
    "        predictions = beam_engine.get_result_generate(model, input_tokens)\n",
    "\n",
    "        predictions = predictions[:5]\n",
    "        object_scores = []\n",
    "        for output_tokens in predictions:\n",
    "            output_cmd = cmd_tokenizer.decode(output_tokens.tolist())\n",
    "            score = compute_metric(output_cmd, 1, target_cmd)\n",
    "            object_scores.append(score)\n",
    "\n",
    "        all_scores.append(max(object_scores))\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnj7FQ-Jgcbt"
   },
   "source": [
    "Ваша цель при помощи подбора параметров модели и генерации получить средний скор на валидации >= 0.2, скор `handcrafted` части теста >= 0.1. На `mined` части датасета скор может быть низкий, т.к. некоторых команд из датасета нет в обучении.\n",
    "\n",
    "**Обратите внимание.** Так как датасет для обучения не очень большой, а данные достаточно нестабильные, подбор параметров может очень сильно влиять на модель. Некоторые полезные советы:\n",
    "* Отслеживайте качество модели после каждой эпохи, не забывайте про early stopping\n",
    "* Вы можете сразу приступить к следующей части. Побитие скора в этой части задания при помощи трюков из бонусной части считается валидным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "id": "4lumgYyKgcbx"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4288035714285715"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores = compute_all_scores(model, valid_data, beam_search_engine)\n",
    "sum(all_scores) / len(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [01:01<00:00,  2.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16147717484926788"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handcrafted = compute_all_scores(model, test_data[test_data[\"origin\"] == \"handcrafted\"], beam_search_engine)\n",
    "sum(handcrafted) / len(handcrafted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 592/592 [04:27<00:00,  2.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.32590726887601895"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mined = compute_all_scores(model, test_data[test_data[\"origin\"] == \"mined\"], beam_search_engine)\n",
    "sum(mined) / len(mined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kE7ysJNgcbx"
   },
   "source": [
    "## Улучшение модели (4 балла)\n",
    "\n",
    "Вы реализовали бейзлайн, пришло время улучшить качество модели. Т.к. это последнее задание, мы не будем предлагать конкретные шаги, а только дадим несколько советов.\n",
    "\n",
    "1. Большой источник информации о работе командной строке — её документация, man. Один из способов улучшения модели - использование мана для генерации новых примеров. Структурированный ман можно найти по ссылке https://github.com/IBM/clai/blob/nlc2cmd/docs/manpage-data.md.\n",
    "2. Ещё один способ улучшить модель, разделить предсказание утилит и флагов. Т.к. задача предсказания утилит более важная, вы можете натренировать модель, которая предсказывает последовательность утилит, а затем к каждой утилите генерировать флаги.\n",
    "3. Можно аугментировать данные, чтобы увеличить выборку.\n",
    "4. Можно в качество входа подавать не только текстовый запрос, но и описание из мана. Т.к. всё описание достаточно большое, нужно сделать дополнительную модель, которая будет выбирать команды, для которых нужно вытащить описание.\n",
    "5. Найти дополнительные данные, улучшающие обучение\n",
    "6. Как всегда можно просто сделать больше слоёв, увеличить размер скрытого слоя и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPWM_wz9gcbx"
   },
   "source": [
    "От вас ожидается скор на валидации >= 0.25, `mined` >= 0, `handrafted` >= 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTqXP4-Zgcbx"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVm9eOKzgcbx"
   },
   "source": [
    "## Бонусные баллы (до 3 баллов)\n",
    "\n",
    "При существенном улучшении качества будут назначаться бонусные баллы. На тестовых датасетах реально выбить качество >= 0.3 на каждом, но усилий потребуется немало..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-kurtsev_sft_pipeline]",
   "language": "python",
   "name": "conda-env-.mlspace-kurtsev_sft_pipeline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
