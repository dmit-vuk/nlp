{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kBtvvxxmi1q"
   },
   "source": [
    "# Практическое задание 1\n",
    "\n",
    "# Ранжирование вопросов StackOverflow с помощью векторных представлений слов\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: <впишите>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNyJ5tTtmi1s"
   },
   "source": [
    "## Введение\n",
    "\n",
    "В этом задании вы научитесь вычислять близость текстов и применить этот метод для поиска похожих вопросов на [StackOverflow](https://stackoverflow.com).\n",
    "\n",
    "### Используемые библиотеки\n",
    "\n",
    "В данном задании потребуются следующие библиотеки:\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — инструмент для решения различных задач NLP (тематическое моделирование, представление текстов, ...).\n",
    "- [Numpy](http://www.numpy.org) — библиотека для научных вычислений.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — библилиотека с многими реализованными алгоритмами машинного обучения для анализа данных.\n",
    "- [Nltk](http://www.nltk.org) — инструмент для работы с естественными языками.\n",
    "- [Pytorch](https://pytorch.org/) — инструмент для обучения нейросетей.\n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве `StackOverflowData.zip`, который состоит из:\n",
    "- `train.tsv` - обучающая выборка. В каждой строке через табуляцию записаны дублирующие друг друга предложения;\n",
    "- `test.tsv` - тестовая выборка. В каждой строке через табуляцию записаны: *<вопрос>, <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...*\n",
    "\n",
    "Скачать архив можно здесь: [ссылка на google диск](https://drive.google.com/open?id=1QqT4D0EoqJTy7v9VrNCYD-m964XZFR7_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n92qpqhbmi1t"
   },
   "source": [
    "#### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F95YvtPXmi1u"
   },
   "outputs": [],
   "source": [
    "from tests import TaskTests\n",
    "\n",
    "task_tests = TaskTests.from_json(path='test_gt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1668454090250,
     "user": {
      "displayName": "Artem Popov",
      "userId": "09282986084580850099"
     },
     "user_tz": 0
    },
    "id": "esgRlre3mwcP",
    "outputId": "6829ecd6-73c6-440c-f34c-371d375b4464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 23 18:18:57 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              70W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2A:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              71W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              71W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              70W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              70W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              69W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              70W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E4:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              69W / 700W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-H5EjDymi1v"
   },
   "source": [
    "### Вектора слов\n",
    "\n",
    "Для решения вам потребуются предобученная модель векторных представлений слов. Используйте [модель эмбеддингов](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit), которая была обучена с помощью пакета word2vec на данных Google News (100 миллиардов слов). Модель содержит 300-мерные вектора для 3 миллионов слов и фраз. Вы можете скачать их, запустив блок кода ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XNWbeWGgmi1v"
   },
   "outputs": [],
   "source": [
    "# from download_utils import download_google_vectors\n",
    "\n",
    "\n",
    "# download_google_vectors(target_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBp8EzO5mi1v"
   },
   "source": [
    "## Часть 1. Предобученные векторные представления слов (2 балла)\n",
    "\n",
    "Скачайте предобученные вектора и загрузите их с помощью функции [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) библиотеки Gensim с параметром *binary=True*. Если суммарный размер векторов больше, чем доступная память, то вы можете загрузите только часть векторов, указав параметр *limit* (рекомендуемое значение: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIdkBeR6mi1w"
   },
   "source": [
    "### Как пользоваться этими векторами?\n",
    "\n",
    "Как только вы загрузите векторные представления слов в память, убедитесь, что имеете к ним доступ. Сначала вы можете проверить, содержится ли какое-то слово в загруженных эмбедингах:\n",
    "\n",
    "    'word' in wv_embeddings\n",
    "\n",
    "Затем, чтобы получить соответствующий вектор, вы можете использовать оператор доступа по ключу:\n",
    "\n",
    "    wv_embeddings['word']\n",
    "\n",
    "### Проверим, корректны ли векторные представления\n",
    "\n",
    "Чтобы предотвратить возможные ошибки во время первого этапа, можно проверить, что загруженные вектора корректны. Для этого проверьте три пункта:\n",
    "1. Используя метод `.most_similar(positive=..., negative=...)`, найти слово, похожее на `woman`, `king` и непохожее на `man`.\n",
    "2. Используя метод `.doesnt_match(...)`, найти \"белую ворону\" в списке `['breakfast, 'dinner', 'lunch', 'cereal']`.\n",
    "3. Используя метод `.most_similar_to_given(word, [...])`, найти наиболее похожее на `music` слово из списка `['water', 'sound', 'backpack', 'mouse']`.\n",
    "\n",
    "Прокомментируйте полученные результаты: считаете ли вы их верными и почему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NrwPu--Omi1w"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'word' in wv_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_embeddings['word'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454),\n",
       " ('royal_palace', 0.5087166428565979)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_embeddings.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_embeddings.doesnt_match(['breakfast', 'dinner', 'lunch', 'cereal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sound'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqkpTaoEmi1w"
   },
   "source": [
    "### Ранжирование вопросов StackOverflow\n",
    "\n",
    "Давайте посмотрим на данные, которые мы будем использовать в рамках задания. Выборка уже разбита на обучающую и тестовую. Все файлы используют табуляцию в качестве разделителя, но они имеют разный формат:\n",
    "\n",
    "- *обучающая* выборка (train.tsv) содержит похожие друг на друга предложения в каждой строке;\n",
    "- *тестовая* выборка (validation.tsv) содержит в каждой строке: *вопрос, похожий вопрос, отрицательный пример 1, отрицательный пример 2, ...*\n",
    "\n",
    "Считайте тестовую (валидационную) выборку. Ответьте на следующие вопросы:\n",
    "1. Сколько пар-дубликатов предоставлено в выборке?\n",
    "2. Сколько в среднем на каждую пару предоставлено отрицательных примеров?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QGNP0Lpfmi1x"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data\n",
    "\n",
    "train = read_corpus('train.tsv')\n",
    "validation = read_corpus('validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4YKujsq8mi1x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,\n",
       " ['converting string to list',\n",
       "  'Convert Google results object (pure js) to Python object'])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3760,\n",
       " 'How to print a binary heap tree without recursion?',\n",
       " 'How do you best convert a recursive function to an iterative one?',\n",
       " 'How can i use ng-model with directive in angular js')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation), validation[0][0], validation[0][1], validation[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3760, 998.8106382978723)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(validation)\n",
    "amount_of_negatives_per_sample = 0\n",
    "for example in validation:\n",
    "    amount_of_negatives_per_sample += len(example) - 2\n",
    "amount_of_negatives_per_sample /= num_samples\n",
    "num_samples, amount_of_negatives_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1mA5RWtxmi1x"
   },
   "outputs": [],
   "source": [
    "task_tests.test_validation_corpus(\n",
    "    num_samples,\n",
    "    amount_of_negatives_per_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3wpXb7Imi1x"
   },
   "source": [
    "### Векторные представления текста\n",
    "\n",
    "Чтобы перейти от отдельных слов к векторным представлениям вопросов, предлагается подсчитать **среднее** векторов всех слов в вопросе. Если для какого-то слова нет предобученного вектоора, то его нужно пропустить. Если вопрос не содержит ни одного известного слова, то нужно вернуть нулевой вектор.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xkwnK0Lgmi1x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    \n",
    "    def __init__(self, embeddings, dim):\n",
    "        \"\"\"\n",
    "            embeddings: word2vec эмбеддинги\n",
    "            dim: размерность word2vec эмбеддингов. Нужна для задания нулего вектора для пустых вопросов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.embeddings = embeddings\n",
    "        self.dim = dim\n",
    "        \n",
    "    def __call__(self, text, normalize=False):\n",
    "        \"\"\"\n",
    "            Принимает на вход текст и преобразует его в вектор.\n",
    "            \n",
    "            text: строка с вопросом\n",
    "            normalize: при True нужно перед возвращением нормализовать вектор\n",
    "            \n",
    "            returns: вектор вопроса\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        words = text.split()\n",
    "        sum_embed = np.zeros((self.dim))\n",
    "        cnt = 0\n",
    "        for word in words:\n",
    "            if word in self.embeddings:\n",
    "                sum_embed += self.embeddings[word]\n",
    "                cnt += 1\n",
    "        if cnt > 0:\n",
    "            sum_embed /= cnt\n",
    "            if normalize:\n",
    "                sum_embed /= np.linalg.norm(sum_embed)\n",
    "        return sum_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Jb5z6BI6mi1y"
   },
   "outputs": [],
   "source": [
    "embedder = Embedder(wv_embeddings, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tests.test_embedder(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk6vB6cZmi1y"
   },
   "source": [
    "Теперь у нас есть метод для создания векторного представления любого предложения. Оценим, как будет работать это решение.\n",
    "\n",
    "### Оценка близости текстов\n",
    "\n",
    "В качестве метрики схожести вопросов будем использовать косинусную близость.\n",
    "\n",
    "В валидационном датасете для каждой пары вопросов-дубликатов у нас есть случайные отрицательные примеры. Для каждого триплета (вопрос, дубликат, отрицательные примеры) будет ранжировать с помощью нашей модели и косинусной близости дубликат и отрицательные примеры и смотреть на позицию дубликата.\n",
    "\n",
    "#### Hits@K\n",
    "Довольно простой и легко интерпретируемой метрикой будет количество корректных попаданий дубликата в top \"выдачи\" для какого-то *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)],$$\n",
    "где $q_i$ - $i$-ый вопрос, $dup_i$ - его дубликат, $topK(q_i)$ - первые *K* элементов в ранжированном списке, который выдает наша модель.\n",
    "\n",
    "#### Пример оценок\n",
    "\n",
    "Пусть $N = 1$, вопрос $q_1$ это \"Что такое python\", а его дубликат $dup_1$ это \"Что такое язык python\". Пусть модель выдала следующий ранжированный список кандидатов:\n",
    "\n",
    "1. *\"Как узнать с++\"*\n",
    "2. *\"Что такое язык python\"*\n",
    "3. *\"Хочу учить Java\"*\n",
    "4. *\"Не понимаю Tensorflow\"*\n",
    "\n",
    "Вычислим метрику *Hits@K* для *K = 1, 4*:\n",
    "\n",
    "- [K = 1] $\\text{Hits@1} =  [dup_1 \\in top1(q_1)] = 0$\n",
    "- [K = 4] $\\text{Hits@4} =  [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "#### Подсчет метрики Hits@k сразу для нескольких k\n",
    "\n",
    "Чтобы посчитать метрику для нескольких k, не нужно повторно ранжировать нашей моделью вопросы для одного и того же сэмпла. Достаточно посчитать для сэмпла количество **сложных негативов** - отрицательных примеров, оказавшихся в выдаче выше, чем дубликат. Тогда\n",
    "$$Hits@k = \\begin{cases}\n",
    "    1, & N < k \\\\\n",
    "    0, & иначе\n",
    "   \\end{cases},$$\n",
    "где **N** - количество сложных негативов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtWxrbtrmi1y"
   },
   "source": [
    "Реализуйте подсчет Hits@k для произвольного набора значений k и заданной валидационной выборки, используя предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N6CdditEmi1y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "    \n",
    "class Scorer:\n",
    "    \n",
    "    def __init__(self, k, embedder):\n",
    "        \"\"\"\n",
    "            k: список значений k, для которых нужно посчитать hits@k\n",
    "            embedder: объект класса Embedder, умеющий преобразовать текст в вектор\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.k = k\n",
    "        self.embedder = embedder\n",
    "        \n",
    "    def _get_hard_negatives(self, q, pos, negs):\n",
    "        \"\"\"\n",
    "            q: текст вопроса\n",
    "            pos: текст дубликата\n",
    "            negs: список из текстов случайных вопросов\n",
    "            \n",
    "            result: количество сложных отрицательных примеров, оказавшихся выше положительного\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        q_embedding = self.embedder(q).reshape(1, -1)\n",
    "        pos_embedding = self.embedder(pos).reshape(1, -1)\n",
    "        pos_sim = cosine_similarity(q_embedding, pos_embedding)\n",
    "        cnt = 0\n",
    "        for neg in negs:\n",
    "            neg_embedding = self.embedder(neg).reshape(1, -1)\n",
    "            neg_sim = cosine_similarity(q_embedding, neg_embedding)\n",
    "            if neg_sim > pos_sim:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "    \n",
    "    def __call__(self, samples, verbose=False):\n",
    "        \"\"\"\n",
    "            samples: список из списков вида [q, pos, neg1, neg2, ...]. Наша валидационная выборка\n",
    "            verbose: выводить progressbar подсчета метрики с помощью tqdm\n",
    "            \n",
    "            result: словарь вида {k: hits@k}\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        n = np.array([])\n",
    "        for sample in tqdm.tqdm(samples, disable = not verbose):\n",
    "            n = np.append(n, self._get_hard_negatives(sample[0], sample[1], sample[2:]))\n",
    "        ans = {}\n",
    "        for k in self.k:\n",
    "            ans[k] = (n < k).mean()\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "rz8wXCmrmi1z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [12:15<00:00,  5.11it/s]\n"
     ]
    }
   ],
   "source": [
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(validation, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.21436170212765956,\n",
       " 5: 0.3125,\n",
       " 10: 0.36622340425531913,\n",
       " 100: 0.5640957446808511,\n",
       " 500: 0.8207446808510638,\n",
       " 1000: 1.0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "wPmvASpmmi1z"
   },
   "outputs": [],
   "source": [
    "task_tests.test_scorer(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFarZRFkmi1z"
   },
   "source": [
    "### Предобработка текста\n",
    "\n",
    "Как вы могли заметить, мы имеем дело с сырыми данными. Это означает, что там присутствует много опечаток, спецсимволов и заглавных букв. В нашем случае это все может привести к ситуации, когда для данных токенов нет предобученных векторов. Поэтому необходима предобработка.\n",
    "\n",
    "Вам требуется:\n",
    "- Перевести символы в нижний регистр;\n",
    "- Заменить символы пунктуации и всевозможные плохие символы на пробелы;\n",
    "- Удалить стопслова.\n",
    "- Удалить слова с длиной меньше трех букв\n",
    "\n",
    "Реализуйте предобработку текста, используя предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x-baELEDmi1z"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "    \n",
    "    \n",
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self, characters, min_word_length=0, stopwords=None):\n",
    "        \"\"\"\n",
    "            characters: список плохих символов\n",
    "            min_word_length: минимальная допустимая длина для слов\n",
    "            stopwords: множество фоновых слов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.characters = characters\n",
    "        self.min_word_length = min_word_length\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: текст для обработки\n",
    "            \n",
    "            returns: обработанный текст\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        bad_chars_regex = '[' + re.escape(''.join(self.characters)) + ']'\n",
    "        text = re.sub(bad_chars_regex, ' ', text.lower())\n",
    "        words = text.split()\n",
    "        return ' '.join([word for word in words if len(word) >= self.min_word_length and word not in self.stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "ymtpueNTmi10"
   },
   "outputs": [],
   "source": [
    "task_tests.test_text_preprocessor(TextPreprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtazkHFSmi10"
   },
   "source": [
    "Множество фоновых слов можно взять из **nltk** с помощью `nltk.corpus.stopwords.words`, выкидываемые плохие символы и пунктуацию следует подобрать самостоятельно.\n",
    "\n",
    "Обработайте текст и продемонстрируйте улучшение качества:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0EI1Zf5Rmi10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "    \n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "characters = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:09<00:00, 109179.48it/s]\n",
      "100%|██████████| 3760/3760 [00:12<00:00, 300.16it/s]\n"
     ]
    }
   ],
   "source": [
    "text_preprocessor = TextPreprocessor(characters, 3, stop_words)\n",
    "train_preproc = [[text_preprocessor(text) for text in sample] for sample in tqdm.tqdm(train)]\n",
    "val_preproc = [[text_preprocessor(text) for text in sample] for sample in tqdm.tqdm(validation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [00:12<00:00, 301.72it/s]\n",
      "100%|██████████| 3760/3760 [11:58<00:00,  5.24it/s]\n"
     ]
    }
   ],
   "source": [
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(val_preproc, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.3406914893617021,\n",
       " 5: 0.47819148936170214,\n",
       " 10: 0.5289893617021276,\n",
       " 100: 0.7007978723404256,\n",
       " 500: 0.8555851063829787,\n",
       " 1000: 1.0}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCzTgxwmmi10"
   },
   "source": [
    "Одним из критериев получения полных баллов является значение **hits@500** $\\geqslant 0.82$ до предобработки текста и $\\geqslant 0.85$ после предобработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbmczi7smi10"
   },
   "source": [
    "## Часть 2. Представления для неизвестных слов. (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn8KCgiXmi10"
   },
   "source": [
    "Для того, что получить представления для неизвестного слова, воспользуемся следующим подходом:\n",
    "    \n",
    "1. Будем восстанавливать эмбеддинг неизвестного слова как сумму эмбеддингов буквенных триграмм. Например, слово where должно представляться суммой триграмм #wh, whe, her, ere, re#\n",
    "\n",
    "2. В качестве обучающих данных будем использовать слова, для которых есть эмбеддинг в модели. Будем обучать эмбеддинги триграмм по выборке эмбеддингов с помощью функционала MSE:\n",
    "\n",
    "$$L = \\sum_{w \\in W_{known}}\\| f_{\\theta}(w) - v_w \\|^2 \\to \\min_{\\theta}$$\n",
    "\n",
    "где:\n",
    "\n",
    "* $W_{known}$ — множество известных модели слов\n",
    "* $f_{\\theta}(w)$ — сумма эмбеддингов триграмм слова $w$\n",
    "* $v_w$ — эмбеддинг слова $w$\n",
    "* $\\theta$ — веса эмбеддингов триграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIPdrbS2mi10"
   },
   "source": [
    "### Создание триграммного токенизатора\n",
    "\n",
    "Для начала, нам нужно:\n",
    "1. Пройтись по известным в word2vec словам и составить множество триграмм, для которых будем обучать векторы\n",
    "2. Составить маппинг из триграмм в индексы\n",
    "3. Реализовать преобразование произвольного слова в список триграмм\n",
    "4. Реализовать преобразование произвольного слова в список индексов триграмм\n",
    "\n",
    "Для реализации всех этих пунктов предлагается использовать шаблон, приведенный ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "id": "DPIGR-9wmi10"
   },
   "outputs": [],
   "source": [
    "class TrigramTokenizer:\n",
    "    \n",
    "    def __init__(self, words):\n",
    "        \"\"\"\n",
    "            Формируем множество всевозможных триграмм, встречающихся в словах из words.\n",
    "            Делаем маппинг триграмм в индексы.\n",
    "            \n",
    "            words: список слов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.words = words\n",
    "        trigrams = set()\n",
    "        for word in words:\n",
    "            trigrams.update(self._get_trigrams(word))\n",
    "        self.trigrams = {trigram: idx for idx, trigram in enumerate(trigrams)}\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: колчиество триграмм, для которых мы завели индекс.\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.trigrams)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_trigrams(word):\n",
    "        \"\"\"\n",
    "            word: слово\n",
    "            \n",
    "            returns: список триграмм для word\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        trigrams = [word[i : i+3] for i in range(len(word) - 2)]\n",
    "        return ['#' + word[:2]] + trigrams + [ word[-2:] + '#']\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        \"\"\"\n",
    "            word: слово\n",
    "            \n",
    "            returns: список индексов триграмм для слова word, которые нашлись в маппинге\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        trigrams = self._get_trigrams(word)\n",
    "        return [self.trigrams[trigram] for trigram in trigrams if trigram in self.trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "sEfxFzXXmi11"
   },
   "outputs": [],
   "source": [
    "task_tests.test_trigram_tokenizer(TrigramTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MBEKAI1mi11"
   },
   "source": [
    "Для создания токенизатора используйте обработанный с помощью TextProcessor текст. \n",
    "\n",
    "**Важно:** в токенизатор нужно подавать только слова, известные word2vec'у."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "zzz4nSA5mi11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [03:46<00:00, 16.61it/s]\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "\n",
    "val_preproc_known_words = []\n",
    "for sample in tqdm.tqdm(val_preproc):\n",
    "    for text in sample:\n",
    "        for word in text.split():\n",
    "            if word not in val_preproc_known_words and word in wv_embeddings:\n",
    "                val_preproc_known_words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_tokenizer = TrigramTokenizer(val_preproc_known_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtY6SiFAmi11"
   },
   "source": [
    "### Создание датасета с w2v векторами и списками индексов триграмм\n",
    "\n",
    "Мы будем обучать триграммную модель в нейросетевом фреймворке pytorch. Для этого нам нужно создать свой датасет.\n",
    "\n",
    "Он должен:\n",
    "1. Принимать список слов, word2vec и уже созданный триграммный токенизатор.\n",
    "2. Выдавать пары вида (эмбеддинг для слова из word2vec, список индексов триграмм для этого слова)\n",
    "\n",
    "Реализовать датасет нужно в шаблоне, приведенном ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "UULcdIj1mi11"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainTrigramDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, w2v_embeddings, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Формируем выборку для обучения триграммной модели.\n",
    "            ЗАРАНЕЕ считаем маппинг в список индексов для всех известных в word2vec слов.\n",
    "            \n",
    "            vocab: список слов\n",
    "            w2v_embeddings: no comments\n",
    "            tri_tokenizer: токенизатор триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.vocab = vocab\n",
    "        self.w2v_embeddings = w2v_embeddings\n",
    "        self.tri_tokenizer = tri_tokenizer\n",
    "        \n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "                \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: возвращает количество слов, вошедших в маппинг (размер словаря)\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            returns: w2v эмбеддинг для idx-го слова в датасете, список соответствующих ему триграмм (тензоры)\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        word = self.vocab[idx]\n",
    "        return self.w2v_embeddings[word], self.tri_tokenizer(word)\n",
    "    \n",
    "w2v_vocab = val_preproc_known_words\n",
    "ds = TrainTrigramDataset(w2v_vocab, wv_embeddings, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "pUx3iBf_mi11"
   },
   "outputs": [],
   "source": [
    "task_tests.test_dataset(ds, w2v_vocab, wv_embeddings, tri_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AOZ1uK8mi12"
   },
   "source": [
    "### Создание DataLoader'а и Collator'а\n",
    "\n",
    "Нас интересуют в первую очередь четыре параметра при создании DataLoader:\n",
    "1. Датасет. Реализует интерфейс массива - можно узнать длину и получить элемент с индексом, меньшим длины.\n",
    "2. batch_size. Задает размера батча (количества сэмплов, идущих одновременно в модель).\n",
    "3. shuffle. При shuffle == True каждую эпоху при итерировании по даталоадеру мы будем получать сэмплы в произвольном порядке.\n",
    "4. collate_fn. Этот параметр позволяет задать кастомную логику \"склеивания\" сэмплов из датасета в батч.\n",
    "\n",
    "В качестве модели мы будем использовать слой **torch.nn.EmbeddingBag**. Он принимает на вход список индексов и список сдвигов, начинающийся с нуля.\n",
    "\n",
    "Нужно наш список списков индексов триграмм превратить в соответствующий формат, преобразовать векторы слов и два списка (индексов и сдвигов) в pytorch тензоры (torch.tensor).\n",
    "\n",
    "Реализуйте следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "n4XNxOhimi12"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        batch: список из элементов датасета, e.g. [ds[i] for i in [2, 3, 1, 15]]\n",
    "        \n",
    "        returns: w2v эмбеддинги, индексы триграмм, сдвиги для триграмм\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    w2v = []\n",
    "    trigram_idx = []\n",
    "    trigram_offsets = []\n",
    "\n",
    "    for item in batch:\n",
    "        trigram_offsets.append(len(trigram_idx))\n",
    "        w2v.append(item[0])\n",
    "        trigram_idx += item[1]\n",
    "    return torch.tensor(w2v), torch.tensor(trigram_idx), torch.tensor(trigram_offsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "NkyhRfZxmi12"
   },
   "outputs": [],
   "source": [
    "task_tests.test_dataloader(ds, collate_fn, embedding_dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbCH3tCYmi12"
   },
   "source": [
    "### Создание модели\n",
    "\n",
    "При создании модели мы обычно наследуемся от **torch.nn.Module** и создаем нужные нам слои как атрибуты объекта нашего класса.\n",
    "\n",
    "В данном случае предлагается для формирования эмбеддингов использовать **torch.nn.EmbeddingBag**.\n",
    "\n",
    "Реализуйте предложенный шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "rpdiEWjrmi12"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TrigramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\"\n",
    "            num_embeddings: количество триграмм, для которых обучаются эмбеддинги\n",
    "            embedding_dim: размерность эмбеддингов триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        super(TrigramModel, self).__init__()\n",
    "        self.num_embed = num_embeddings\n",
    "        self.embed_dim = embedding_dim\n",
    "        self.embed = nn.EmbeddingBag(num_embeddings, embedding_dim)\n",
    "        \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        \"\"\"\n",
    "            returns: размерность эмбеддингов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.embed_dim\n",
    "    \n",
    "    @property\n",
    "    def num_embeddings(self):\n",
    "        \"\"\"\n",
    "            returns: количество эмбеддингов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.num_embed\n",
    "    \n",
    "    def forward(self, trigrams, offsets):\n",
    "        \"\"\"\n",
    "            trigrams: список индексов триграмм (тензор)\n",
    "            offsets: список сдвигов (тензор)\n",
    "            \n",
    "            returns: эмбеддинги слов, составленные из триграмм\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return self.embed(trigrams, offsets)\n",
    "    \n",
    "model = TrigramModel(tri_tokenizer.vocab_size, embedding_dim=wv_embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "G6q25jV7mi12"
   },
   "outputs": [],
   "source": [
    "task_tests.test_trigram_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk6iMenxmi13"
   },
   "source": [
    "### Создание пайплайна обучения\n",
    "\n",
    "Далее необходимо совместить все наработки в единый пайплайн обучения, добавив также критерий для оптимизации и оптимизатор. \n",
    "\n",
    "Предлагается:\n",
    "\n",
    "1. В качестве оптимизатора использовать Adam (можно попробовать подобрать learning rate / weight decay)\n",
    "2. В качестве критерия оптимизации взять nn.MSELoss (можно также закодить лосс самому)\n",
    "3. Для даталоадера выбрать небольшой батч сайз (32, 64, 128, 256)\n",
    "4. Десяти эпох должно быть достаточно для хорошего качества\n",
    "\n",
    "Реализуйте предложенный шаблон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "jjCxLgHVmi13"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        \"\"\"\n",
    "            model: триграммная модель\n",
    "            criterion: функционал ошибки, принимает на вход w2v эмбеддинги и триграммные эмбеддинги\n",
    "            optimizer: оптимизатор для модели\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            Делаем один проход по даталоадеру, с бэкпропом\n",
    "            \n",
    "            dataloader: даталоадер с тренировочными данными\n",
    "            \n",
    "            returns: лосс\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "    \n",
    "        for w2v_true, trigrams, offsets in dataloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            w2v_pred = self.model(trigrams, offsets)\n",
    "            loss = self.criterion(w2v_pred, w2v_true)\n",
    "            loss.backward()  # Распространение ошибки назад\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()    \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def train(self, dataloader, n_epochs, verbose=False):\n",
    "        \"\"\"\n",
    "            dataloader: тренировочный даталоадер\n",
    "            n_epochs: количество эпох\n",
    "            verbose: выводить лосс каждую эпоху или нет\n",
    "            \n",
    "            returns: список лоссов\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self._train_step(dataloader)\n",
    "            losses.append(loss)\n",
    "            if verbose:\n",
    "                print(f'epoch: {epoch + 1:>2}, loss: {loss:.4f}, time: {time.time() - start:.4f}')\n",
    "        return losses\n",
    "\n",
    "\n",
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "dataloader = DataLoader(ds, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1, loss: 0.1327, time: 0.5750\n",
      "epoch:  2, loss: 0.1305, time: 1.1567\n",
      "epoch:  3, loss: 0.1287, time: 1.7666\n",
      "epoch:  4, loss: 0.1265, time: 2.3674\n",
      "epoch:  5, loss: 0.1246, time: 2.9553\n",
      "epoch:  6, loss: 0.1225, time: 3.5455\n",
      "epoch:  7, loss: 0.1205, time: 4.1376\n",
      "epoch:  8, loss: 0.1187, time: 4.7131\n",
      "epoch:  9, loss: 0.1169, time: 5.2720\n",
      "epoch: 10, loss: 0.1152, time: 5.8208\n",
      "epoch: 11, loss: 0.1134, time: 6.3497\n",
      "epoch: 12, loss: 0.1120, time: 6.8827\n",
      "epoch: 13, loss: 0.1099, time: 7.4351\n",
      "epoch: 14, loss: 0.1086, time: 8.0171\n",
      "epoch: 15, loss: 0.1069, time: 8.5667\n",
      "epoch: 16, loss: 0.1052, time: 9.1143\n",
      "epoch: 17, loss: 0.1038, time: 9.6541\n",
      "epoch: 18, loss: 0.1023, time: 10.2090\n",
      "epoch: 19, loss: 0.1009, time: 10.7496\n",
      "epoch: 20, loss: 0.0996, time: 11.2839\n",
      "epoch: 21, loss: 0.0982, time: 11.8403\n",
      "epoch: 22, loss: 0.0967, time: 12.3765\n",
      "epoch: 23, loss: 0.0955, time: 12.9305\n",
      "epoch: 24, loss: 0.0942, time: 13.4568\n",
      "epoch: 25, loss: 0.0929, time: 14.0159\n",
      "epoch: 26, loss: 0.0916, time: 14.5564\n",
      "epoch: 27, loss: 0.0906, time: 15.1082\n",
      "epoch: 28, loss: 0.0891, time: 15.6440\n",
      "epoch: 29, loss: 0.0880, time: 16.1721\n",
      "epoch: 30, loss: 0.0869, time: 16.7405\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, criterion, optimizer)\n",
    "losses = trainer.train(dataloader, n_epochs=30, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-LszyP2mi13"
   },
   "source": [
    "### Получение векторов неизвестных слов. Инференс модели\n",
    "\n",
    "Теперь, когда мы обучили модель, нам необходимо применить её для всех неизвестных слов, т.е. получить для них эмбеддинги.\n",
    "\n",
    "Т.к. для этих слов у нас нет word2vec эмбеддингов, то dataset и collator для обучения не подходят для инференса. Необходимо реализовать датасет и коллатор для инференса по следующим шаблонам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "nwlcTMWvmi13"
   },
   "outputs": [],
   "source": [
    "class InferenceTrigramDataset:\n",
    "    \n",
    "    def __init__(self, vocab, tri_tokenizer):\n",
    "        \"\"\"\n",
    "            Датасет с неизвестными словами\n",
    "            \n",
    "            vocab: список слов\n",
    "            tri_tokenizer: триграммный токенизатор\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.vocab = vocab\n",
    "        self.tri_tokenizer = tri_tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        word = self.vocab[idx]\n",
    "        return idx, self.tri_tokenizer(word)\n",
    "    \n",
    "    \n",
    "def inference_collate_fn(trigrams):\n",
    "    \"\"\"\n",
    "        trigrams: список списков индексов триграмм\n",
    "        \n",
    "        returns: список индексов, список сдвигов триграмм\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    trigram_idx = []\n",
    "    trigram_offsets = []\n",
    "    idx = []\n",
    "\n",
    "    for (i, trigram) in trigrams:\n",
    "        trigram_offsets.append(len(trigram_idx))\n",
    "        trigram_idx += trigram\n",
    "        idx += [i]\n",
    "    return torch.tensor(idx), torch.tensor(trigram_idx), torch.tensor(trigram_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt3hBXBDmi13"
   },
   "source": [
    "Теперь у нас есть всё необходимое, чтобы осуществить инференс. Не забудь перед инференсом перевести модель в режим эвала (**model.eval**), а также использовать контекстный менеджер **torch.no_grad**.\n",
    "\n",
    "После инференса сформируйте словарь из известных в word2vec слов и их эмбеддингов, затем дополните его эмбеддингами для неизвестных слов, полученными после инференса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2077/3760 [08:07<06:38,  4.22it/s]"
     ]
    }
   ],
   "source": [
    "val_preproc_unknown_words = []\n",
    "for sample in tqdm.tqdm(val_preproc):\n",
    "    for text in sample:\n",
    "        for word in text.split():\n",
    "            if word not in val_preproc_unknown_words and word not in wv_embeddings:\n",
    "                val_preproc_unknown_words.append(word)\n",
    "\n",
    "tri_tokenizer_unknown = TrigramTokenizer(val_preproc_unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9527"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_preproc_unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab_unknown = val_preproc_unknown_words\n",
    "ds_unknown = InferenceTrigramDataset(w2v_vocab_unknown, tri_tokenizer)\n",
    "dataloader_known = DataLoader(ds, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "dataloader_unknown = DataLoader(ds_unknown, batch_size=64, shuffle=False,  collate_fn=inference_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.eval()\n",
    "unknown_pred = np.array([])\n",
    "word2vec_embeddings = {}\n",
    "with torch.no_grad():\n",
    "    for idx, trigrams, offsets in dataloader_unknown:\n",
    "        w2v_pred = model(trigrams, offsets)\n",
    "        unknown_pred = np.append(unknown_pred, w2v_pred.numpy())\n",
    "\n",
    "        for i, embed in zip(idx, w2v_pred):\n",
    "            word = w2v_vocab_unknown[i]\n",
    "            word2vec_embeddings[word] = embed.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "id": "akeYx9Efmi13"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "\n",
    "for word in val_preproc_known_words:\n",
    "    word2vec_embeddings[word] = wv_embeddings[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZk_gfHTmi13"
   },
   "source": [
    "Используя **Scorer** и **Embedder**, получите новые значения метрик для валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "id": "mxgQk7bKmi14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [11:46<00:00,  5.32it/s]\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "\n",
    "embedder = Embedder(word2vec_embeddings, dim=300)\n",
    "\n",
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(val_preproc, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.4332446808510638,\n",
       " 5: 0.5694148936170212,\n",
       " 10: 0.6172872340425531,\n",
       " 100: 0.7656914893617022,\n",
       " 500: 0.9071808510638298,\n",
       " 1000: 1.0}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKcv-XM-mi14"
   },
   "source": [
    "Одним из критериев получения полных баллов является значение метрики **hits@500** $\\geqslant 0.89$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ze8ffBU4mi14"
   },
   "source": [
    "## Часть 3. Обучение векторных представлений для целевой задачи. (4 баллов)\n",
    "\n",
    "Предполагается, что в этой части используются TextPreprocessor, Embedder, Scorer из предыдущих частей.\n",
    "\n",
    "Для обучения на целевую задачу нам понадобится обучающая выборка. Считайте её с диска, предобработайте текст вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BBJpej_Jmi14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:09<00:00, 107936.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "text_preprocessor = TextPreprocessor(characters, 3, stop_words)\n",
    "for questions in tqdm.tqdm(read_corpus('train.tsv')):\n",
    "    train.append([text_preprocessor(text) for text in questions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVuDblKrmi14"
   },
   "source": [
    "Необходимо создать **токенизатор для текста** - составить словарь и сделать маппинг из слов в индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "s9tI5rF8mi14"
   },
   "outputs": [],
   "source": [
    "class TextTokenizer:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"\n",
    "            vocab: множество слов, встретившихся в обучающей выборке\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "            returns: количество слов в словаре\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "            text: текст\n",
    "            \n",
    "            returns: список индексов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        indices = []\n",
    "        for word in text.split():\n",
    "            indices.append(self.vocab[word])\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI-LDoKqmi14"
   },
   "source": [
    "Составление словаря и токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lUo0ObkWmi14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:07<00:00, 129991.20it/s]\n",
      "100%|██████████| 3760/3760 [00:12<00:00, 295.90it/s]\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "train_preproc = [[text_preprocessor(text) for text in sample] for sample in tqdm.tqdm(train)]\n",
    "val_preproc = [[text_preprocessor(text) for text in sample] for sample in tqdm.tqdm(validation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:01<00:00, 640496.97it/s]\n"
     ]
    }
   ],
   "source": [
    "train_preproc_vocab = {}\n",
    "idx = 0\n",
    "for sample in tqdm.tqdm(train_preproc):\n",
    "    for text in sample:\n",
    "        for word in text.split():\n",
    "            if word not in train_preproc_vocab:\n",
    "                train_preproc_vocab[word] = idx\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131030"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_preproc_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextTokenizer(train_preproc_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFoPf0Ymmi15"
   },
   "source": [
    "Нам также понадобится **новый датасет для обучения**. Для применения метода NT-Exent нам не нужно \"майнить негативы\", поэтому датасет надо сформировать как массив из пар-дубликатов.\n",
    "\n",
    "Так как данные в обучающей выборке содержат множества дубликатов (т.е. все дубликаты сгруппированы в списки), есть несколько способов сформировать итоговый датасет:\n",
    "1. Оставить из каждого множества дубликатов какие-нибудь случайные два (или просто первые два вопроса)\n",
    "2. Для первого вопроса в множестве взять все остальные как дубликаты (N вопросов-дубликатов - N-1 пара). Тогда мы увидим каждый вопрос хотя бы один раз при обучении\n",
    "3. Составить всевозможные уникальные пары-дубликаты из этих множеств (т.е. первый вопрос и все остальные вопросы, второй вопрос и все остальные, кроме первого).\n",
    "\n",
    "Каждый следующий способ, начиная с первого, раздувает выборку по размеру, но возможно дает прирост к качеству решения задачи.\n",
    "\n",
    "Реализуйте выбранный вами подход, используя предолженный шаблон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JY03RRwDmi15"
   },
   "outputs": [],
   "source": [
    "class QuestionDuplicatesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, question_pairs, tokenizer):\n",
    "        \"\"\"\n",
    "            question_pairs: список из пар вопросов-дубликатов\n",
    "            tokenizer: объект класса TextTokenizer\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.question_pairs = question_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            returns: количество пар-дубликатов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        return len(self.question_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" \n",
    "            returns: (вопрос, дубликат), idx-ю пару в датасете\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        question = self.tokenizer(self.question_pairs[idx][0])\n",
    "        duplicate = self.tokenizer(self.question_pairs[idx][1])\n",
    "        return question, duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpBloJMAmi15"
   },
   "source": [
    "Также нужно подготовить **даталоадер** (а именно - коллатор для даталоадера) по аналогии со второй частью задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VF2xcyshmi15"
   },
   "outputs": [],
   "source": [
    "def get_ids_and_offsets(question):\n",
    "    \"\"\"\n",
    "        questions: список из токенизированных вопросов\n",
    "        \n",
    "        returns: (ids, offsets), где ids - вытянутый список индексов слов в вопросах из батча, offsets - сдвиги\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    ids = []\n",
    "    offsets = []\n",
    "    for q in question:\n",
    "        ids += q\n",
    "        offsets.append(len(ids))\n",
    "    return ids, offsets\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        batch: список из пар токенизированных вопросов-дубликатов [(question, duplicate), ...]\n",
    "        \n",
    "        returns: (question_ids, question_offsets), (duplicate_ids, duplicate_offsets)\n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################\n",
    "    questions, duplicates = [], []\n",
    "    for (question, duplicate) in batch:\n",
    "        questions.append(question)\n",
    "        duplicates.append(duplicate)\n",
    "    question_ids, question_offsets = get_ids_and_offsets(questions)\n",
    "    duplicate_ids, duplicate_offsets = get_ids_and_offsets(duplicates)\n",
    "\n",
    "    return (torch.tensor(question_ids), question_offsets), (torch.tensor(duplicate_ids), duplicate_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmDtp-8fmi15"
   },
   "source": [
    "Поделите выборку на трейн и валидацию, используя train_test_split, затем **создайте датасеты и даталоадеры** для обучения и валидации. Сколько пар-дубликатов получилось в датасете для обучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9KpC3rtqmi15"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = QuestionDuplicatesDataset(train_preproc, tokenizer)\n",
    "train_train, val_train = train_test_split(train_ds, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_train, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_train, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 200000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_train), len(val_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m-pko99mi15"
   },
   "source": [
    "С помощью предложенного шаблона **задайте модель** для преобразования вопросов в векторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cFk-psiLmi16"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DssmLikeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        \"\"\"\n",
    "            num_embeddings: количество слов, для которых обучаем эмбеддинги\n",
    "            embedding_dim: размерность эмбеддинга\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings+1, embedding_dim)\n",
    "        \n",
    "    def forward(self, ids, offsets):\n",
    "        \"\"\"\n",
    "            ids: вытянутая посл-ть индексов слов вопросов, попавших в батч\n",
    "            offsets: сдвиги для вопросов, попавших в батч\n",
    "            \n",
    "            returns: векторы вопросов\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        embedded_ids = self.embedding(ids)\n",
    "        question_vectors = []\n",
    "        offset_prev = 0\n",
    "        for offset in offsets:\n",
    "            if offset - offset_prev == 0:\n",
    "                question_vectors.append(self.embedding(torch.tensor(self.num_embeddings).to(device)))\n",
    "            else:\n",
    "                question_vectors.append(embedded_ids[offset_prev:offset].mean(dim=0))\n",
    "            offset_prev = offset\n",
    "        question_vectors = torch.stack(question_vectors)\n",
    "        return question_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQOTSiiVmi16"
   },
   "source": [
    "Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Q6WffKqsmi16"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "model_dssm = DssmLikeModel(len(train_preproc_vocab), 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEUJBMtGmi16"
   },
   "source": [
    "**Критерий оптимизации** для NTExentLoss выглядит как:\n",
    "\n",
    "$$\\mathcal{L}(Q, D) = -0.5 \\log diag(softmax(QD^T / \\alpha)) - 0.5 \\log diag(softmax(DQ^T / \\alpha)),$$\n",
    "\n",
    "где:\n",
    "* $Q \\in \\mathbb{R}^{b \\times d}$ - эмбеддинги вопросов, \n",
    "* $D \\in \\mathbb{R}^{b \\times d}$ - эмбеддинги соответствующих вопросам дубликатов,\n",
    "* $b$ - количество пар (вопрос, дубликат), $d$ - размерность эмбеддингов, $\\alpha$ - гиперпараметр лосса. \n",
    "* Softmax берется по рядам\n",
    "* Матрицы $Q, D$ содержат нормированные эмбеддинги, т.е. считается именно косинус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GCCHxOk_mi16"
   },
   "outputs": [],
   "source": [
    "class NTExentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=1., eps=1e-8):\n",
    "        \"\"\"\n",
    "            alpha: коэффициент, на который мы делим скоры перед софтмаксом\n",
    "            eps: ||v|| = min(eps, ||v||)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        \n",
    "    def _normalize(self, embeddings):\n",
    "        \"\"\"\n",
    "            embeddings: матрица размера [batch_size, embedding_dim]\n",
    "            \n",
    "            returns: матрица такого же размера, но с нормироваными векторами\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        norm = embeddings.norm(dim=1, keepdim=True)\n",
    "        return embeddings / (norm + self.eps)\n",
    "    \n",
    "    def forward(self, embeddings, positives):\n",
    "        \"\"\"\n",
    "            embeddings: матрица размера [batch_size, embedding_dim]\n",
    "            positives: матрица такого же размера, с позитивами для векторов из матрицы embeddings\n",
    "            \n",
    "            returns: NT-Exent loss\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        embeddings_norm, positives_norm = self._normalize(embeddings), self._normalize(positives)\n",
    "        scores = embeddings_norm @ positives_norm.T / self.alpha\n",
    "        loss = -0.5 * (torch.log_softmax(scores, dim=1).diag() + torch.log_softmax(scores.T, dim=1).diag())\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9xWJF4Lmi16"
   },
   "source": [
    "**Создайте пайплайн** для обучения и валидации, используя предложенный шаблон.\n",
    "\n",
    "Залогируйте с помощью **torch.utils.tensorboard.SummaryWriter** две величины:\n",
    "1. Лосс для каждого батча\n",
    "2. Лосс на валидации для каждой эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "v0oqgrclmi16"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "    \n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            model, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            logdir=None, \n",
    "            device=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            model: объект класса DssmModel\n",
    "            optimizer: оптимизатор\n",
    "            criterion: критерий оптимизации\n",
    "            logdir: директория, в которую SummaryWriter должен писать логи\n",
    "            device: девайс (cpu или cuda), на котором надо производить вычисления\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.logdir = logdir\n",
    "        self.device = device\n",
    "\n",
    "        self._writer = SummaryWriter(log_dir=logdir)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def _calculate_loss(self, batch):\n",
    "        \"\"\"\n",
    "            batch: батч из индексов и сдвигов для вопросов и их дубликатов\n",
    "            \n",
    "            returns: посчитанный для батча лосс\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        questions, duplicates = batch\n",
    "        questions_embed = self.model(questions[0].to(self.device), questions[1])\n",
    "        duplicates_embed = self.model(duplicates[0].to(self.device), duplicates[1])\n",
    "        return self.criterion(questions_embed, duplicates_embed)\n",
    "    \n",
    "    def _train_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: даталоадер для обучения\n",
    "            \n",
    "            returns: лосс на датасете для обучения\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self._calculate_loss(batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        if self._writer is not None:\n",
    "            self._writer.add_scalar('train/loss', avg_loss, global_step=self._n_epoch)\n",
    "        return avg_loss\n",
    "    \n",
    "    def _eval_step(self, dataloader):\n",
    "        \"\"\"\n",
    "            dataloader: даталоадер для валидации\n",
    "            \n",
    "            returns: лосс на валидации\n",
    "        \"\"\"\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss = self._calculate_loss(batch)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    def train(self, dataloaders, n_epochs, verbose=False):\n",
    "        \"\"\"\n",
    "            dataloaders: словарь вида {'train': train_dataloader, 'eval': eval_dataloader}\n",
    "            n_epochs: количество эпох обучения\n",
    "            verbose: нужно ли выводить каждую эпоху информацию про лоссы\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        self._n_epoch = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = self._train_step(dataloaders['train'])\n",
    "            \n",
    "            eval_loss = self._eval_step(dataloaders['eval'])\n",
    "            if self._writer is not None:\n",
    "                self._writer.add_scalar('eval/loss', eval_loss, global_step=self._n_epoch)\n",
    "                \n",
    "            if verbose:\n",
    "                print(\n",
    "                    'epoch: {:>2}, train loss: {:.4f}, eval loss: {:.4f}, time: {:.4f}' \\\n",
    "                        .format(epoch + 1, train_loss, eval_loss, time.time() - start)\n",
    "                )\n",
    "                    \n",
    "            self._n_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEFY1E8ami17"
   },
   "source": [
    "Предлагается использовать для оптимизации Адам и обучать модель 10-60 эпох.\n",
    "\n",
    "Для этой части задания GPU даёт существенное ускорение при обучении, поэтому стоит по возможности делать обучение с большим batch size'ом и на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "n6vroNnAmi17"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "train_dataloader = DataLoader(train_train, batch_size=1024, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_train, batch_size=1024, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dssm = DssmLikeModel(len(train_preproc_vocab), 300)\n",
    "\n",
    "optimizer = optimizer = torch.optim.Adam(model_dssm.parameters(), lr=3e-4)\n",
    "criterion = NTExentLoss()\n",
    "device=\"cuda\"\n",
    "trainer = Trainer(model_dssm, optimizer, criterion, logdir=\"logs\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_dataloader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m: val_dataloader}\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 103\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, dataloaders, n_epochs, verbose)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 103\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_step(dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[31], line 66\u001b[0m, in \u001b[0;36mTrainer._train_step\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_loss(batch)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/kuleshov/conda_env/sft-pipeline/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kuleshov/conda_env/sft-pipeline/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataloaders = {'train': train_dataloader, 'eval': val_dataloader}\n",
    "trainer.train(dataloaders, n_epochs=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучил, логи стёрлись... печаль.. обучалось 75 эпох до трейн лосса где-то около 6.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6HXpA1ami17"
   },
   "source": [
    "Осталось достать из модели обученные под задачу векторы слов, составить маппинг слов в векторы, создать **Embedder** и **Scorer** и провалидировать качество на нашей исходной валидации, которой мы пользовались в первых двух частях.\n",
    "\n",
    "Чтобы достать из модели веса, можно использовать `model._embeddings.weight.cpu().detach().numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "N1F3WGZ2mi17"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "embeddings = trainer.model.embedding.weight.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embeddings = {}\n",
    "for word, idx in train_preproc_vocab.items():\n",
    "    word2vec_embeddings[word] = embeddings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [00:02<00:00, 1635.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for sample in tqdm.tqdm(val_preproc):\n",
    "    for text in sample:\n",
    "        for word in text.split():\n",
    "            if word not in word2vec_embeddings:\n",
    "                word2vec_embeddings[word] = embeddings[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131030, 133192)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_preproc_vocab), len(word2vec_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [11:43<00:00,  5.35it/s]\n"
     ]
    }
   ],
   "source": [
    "embedder = Embedder(word2vec_embeddings, dim=300)\n",
    "\n",
    "scorer = Scorer(\n",
    "    k=[1, 5, 10, 100, 500, 1000],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "hits = scorer(val_preproc, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.39654255319148934,\n",
       " 5: 0.5888297872340426,\n",
       " 10: 0.6696808510638298,\n",
       " 100: 0.8981382978723405,\n",
       " 500: 0.9848404255319149,\n",
       " 1000: 1.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfX6M1Rsmi17"
   },
   "source": [
    "Одним из критериев получения полных баллов является значение метрики **hits@500** $\\geqslant 0.98$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6QyJM80mi17"
   },
   "source": [
    "## Дополнительная часть (до 3 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OsrlnXymi17"
   },
   "source": [
    "Каждый из пунктов при успешном выполнении гарантирует как минимум один дополнительный балл. Максимум вам будет зачтено три пункта. \n",
    "1. Обучить триграммную модель на косинусную близость вместо евклидового расстояния, получить прирост качества (относительно триграммной модели с MSE)\n",
    "2. Обучить в качестве триграммной модели char-biLSTM вместо мешка векторов триграмм, получить прирост качества (относительно триграммной модели с таким же критерием оптимизации)\n",
    "3. Усложнить модель в части 3, добавить к мешку вектора словесных униграмм также мешок векторов словесных биграмм и мешок векторов буквенных триграмм (сделать модель более похожей на настоящий dssm), получить прирост качества\n",
    "4. Модифицировать модель в части 3 произвольным образом (добавить MLP, нормализации, дропаут, сделать bilstm поверх последовательности векторов слов, трансформер и т.д.), получить прирост качества\n",
    "5. Сделать модель с ранним связыванием (early fusion) - векторы вопросов конкатенируются и проходят через MLP (с возможными модификациями) перед созданием предсказания. Hint: возможно стоит предобучить эмбеддинги слов с помощью NT-Exent перед обучением финальной модели."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:sft-pipeline]",
   "language": "python",
   "name": "conda-env-sft-pipeline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
