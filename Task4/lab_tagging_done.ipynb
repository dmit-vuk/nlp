{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "MpcGyAm0siPL",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Практическое задание 4\n",
    "\n",
    "# Распознавание именованных сущностей из Twitter с помощью LSTM\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: <впишите>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaOgyzqjsiPM"
   },
   "source": [
    "## Введение\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы будете использовать рекуррентные нейронные сети для решения проблемы распознавания именованных сущностей (NER). Примерами именованных сущностей являются имена людей, названия организаций, адреса и т.д. В этом задании вы будете работать с данными twitter.\n",
    "\n",
    "Например, вы хотите извлечь имена и названия организаций. Тогда для текста\n",
    "\n",
    "    Yan Goodfellow works for Google Brain\n",
    "\n",
    "модель должна извлечь следующую последовательность:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "где префиксы *B-* и *I-* означают начало и конец именованной сущности, *O* означает слово без тега. Такая префиксная система введена, чтобы различать последовательные именованные сущности одного типа.\n",
    "\n",
    "Решение этого задания будет основано на нейронных сетях, а именно на Bi-Directional Long Short-Term Memory Networks (BiLSTMs). В базовой части задания вам также нужно будет улучшить модель при помощи необучаемого пост-процессинга, основанного на алгоритме Витерби и графической модели CRF. В бонусной части вам будет предложено полноценно использовать связку BiLSTM и CRF, обучая обе модели одновременно.\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    " - [Pytorch](https://pytorch.org/).\n",
    " - [Numpy](http://www.numpy.org).\n",
    "\n",
    "### Данные\n",
    "\n",
    "Все данные содержатся в папке `./data`: `./data/train.txt`, `./data/validation.txt`, `./data/test.txt`.\n",
    "\n",
    "Скачать архив можно здесь: [ссылка на google диск](https://drive.google.com/open?id=1s1rFOFMZTBqtJuQDcIvW-8djA78iUDcx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nIms1x0YsiPN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Wbkl91jsiPO"
   },
   "source": [
    "## Часть 1. Подготовка данных (2 балла)\n",
    "\n",
    "#### Баллы за эту часть можно получить только при успешном выполнении части 2.\n",
    "\n",
    "### Загрузка данных\n",
    "\n",
    "Мы будем работать с данными, которые содержат твиты с тегами именованных сущностей. Каждая строка файла содержит пару токен (слово или пунктуация) и тег, разделенные пробелом. Различные твиты разделены пустой строкой.\n",
    "\n",
    "Функция *read_data* считывает корпус из *file_path* и возвращает два списка списков: один со списками токенов по твитам и один со списками соответствующих токенам тегов. Также она заменяет все ники (токены, которые начинаются на символ *@*) на токен `<USR>` и url-ы (токены, которые начинаются на *http://* или *https://*) на токен `<URL>`.\n",
    "\n",
    "**<font color='red'>Задание. Реализуйте функцию read_data.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "xsqWq8PUsiPO"
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    tokens_twit = []\n",
    "    tags_twit = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for pair in file:\n",
    "            if len(pair.strip()) == 0:\n",
    "                tokens.append(tokens_twit)\n",
    "                tags.append(tags_twit)\n",
    "                \n",
    "                tokens_twit = []\n",
    "                tags_twit = []\n",
    "            else:\n",
    "                token, tag = pair.strip().split()\n",
    "                tokens_twit.append(token)\n",
    "                tags_twit.append(tag)\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CizxRJHsiPO"
   },
   "source": [
    "Теперь мы можем загрузить 3 части данных:\n",
    " - *train* для тренировки модели;\n",
    " - *validation* для валидации и подбора гиперпараметров;\n",
    " - *test* для финального тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "QztrsmJtsiPP"
   },
   "outputs": [],
   "source": [
    "train_sentences, train_tags = read_data('data/train.txt')\n",
    "val_sentences, val_tags = read_data('data/validation.txt')\n",
    "test_sentences, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['RT',\n",
       "  '@TheValarium',\n",
       "  ':',\n",
       "  'Online',\n",
       "  'ticket',\n",
       "  'sales',\n",
       "  'for',\n",
       "  'Ghostland',\n",
       "  'Observatory',\n",
       "  'extended',\n",
       "  'until',\n",
       "  '6',\n",
       "  'PM',\n",
       "  'EST',\n",
       "  'due',\n",
       "  'to',\n",
       "  'high',\n",
       "  'demand',\n",
       "  '.',\n",
       "  'Get',\n",
       "  'them',\n",
       "  'before',\n",
       "  'they',\n",
       "  'sell',\n",
       "  'out',\n",
       "  '...'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-musicartist',\n",
       "  'I-musicartist',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0], train_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW0LIrWnsiPP"
   },
   "source": [
    "Всегда полезно знать, с какими данными вы работаете. Выведем небольшую часть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "NZyOibdasiPQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "@TheValarium\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "http://t.co/2zgQ99nmuf\tO\n",
      "http://t.co/eQmogqqABK\tO\n",
      "\n",
      "Happy\tO\n",
      "Birthday\tO\n",
      "@AshForeverAshey\tO\n",
      "!\tO\n",
      "May\tO\n",
      "Allah\tB-person\n",
      "s.w.t\tO\n",
      "bless\tO\n",
      "you\tO\n",
      "with\tO\n",
      "goodness\tO\n",
      "and\tO\n",
      "happiness\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for token, one_tag in zip(train_sentences[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, one_tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQcEDckFsiPQ"
   },
   "source": [
    "### Подготовка словарей\n",
    "\n",
    "Чтобы обучать нейронную сеть, мы будем использовать два отображения.\n",
    "\n",
    "- {token}$\\to${token id}: устанавливает соответствие между токеном и строкой в embedding матрице;\n",
    "- {tag}$\\to${tag id}: one hot encoding тегов.\n",
    "\n",
    "\n",
    "Теперь вам необходимо реализовать функцию *build_dict*, которая должна возвращать словарь {token or tag}$\\to${index} и контейнер, задающий обратное отображение.\n",
    "\n",
    "**<font color='red'>Задание. Реализуйте функцию build_dict.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "J_8E6EFYsiPQ"
   },
   "outputs": [],
   "source": [
    "def build_dict(entities, special_entities):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        entities: a list of lists of tokens\n",
    "        special_entities: a list of special tokens\n",
    "\n",
    "    Returns:\n",
    "        entity_to_idx : mapping to index\n",
    "        idx_to_entity : mapping from index\n",
    "    \"\"\"\n",
    "    entity_to_idx = dict()\n",
    "    idx_to_entity = []\n",
    "\n",
    "    # Create mappings from tokens to indices and vice versa\n",
    "    # Add special tokens to dictionaries\n",
    "    # The first special token must have index 0\n",
    "\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    for idx, special in enumerate(special_entities):\n",
    "        entity_to_idx[special] = idx\n",
    "        idx_to_entity.append(special)\n",
    "    \n",
    "    current_idx = len(special_entities)\n",
    "    for entity_list in entities:\n",
    "        for token in entity_list:\n",
    "            if token not in entity_to_idx:\n",
    "                entity_to_idx[token] = current_idx\n",
    "                idx_to_entity.append(token)\n",
    "                current_idx += 1\n",
    "\n",
    "    return entity_to_idx, idx_to_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtABCmyMsiPQ"
   },
   "source": [
    "После реализации функции *build_dict* вы можете создать словари для токенов и тегов. В нашем случае специальными токенами будут:\n",
    " - `<UNK>` токен для обозначаения слов, которых нет в словаре;\n",
    " - `<PAD>` токен для дополнения предложений одного батча до одинаковой длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "xbxsRFZXsiPR"
   },
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = []\n",
    "\n",
    "# Create dictionaries\n",
    "token_to_idx, idx_to_token = build_dict(train_sentences + val_sentences, special_tokens)\n",
    "tag_to_idx, idx_to_tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb13Uy1AsiPR"
   },
   "source": [
    "### Подготовка датасета и загрузчика\n",
    "\n",
    "Обычно нейронные сети обучаются батчами. Это означает, что каждое обновление весов нейронной сети происходит на основе нескольких последовательностей. Технической деталью является необходимость дополнить все последовательности внутри батча до одной длины.\n",
    "\n",
    "Для начала необходимо реализовать <<датасет>> для хранения ваших данных. Датасет должен наследоваться от стандартного pytorch класса `Dataset` и переопределять методы `__getitem__` и `__len__`. Метод `__getitem__` должен возвращать индексированную последовательность и её теги. Не забудьте про `<UNK>` токен для неизвестных слов!\n",
    "\n",
    "**<font color='red'>Задание. Реализуйте класс TaggingDataset.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "tVNfgR8vsiPR"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TaggingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tags, token_to_idx, tag_to_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentences: a list of lists of tokens\n",
    "            tags: a list of lists of corresponding tags\n",
    "            token_to_idx: mapping from token to token indexes\n",
    "            tag_to_idx: mapping from tag to tag indexes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx : int\n",
    "\n",
    "        Returns:\n",
    "            sentence_idx : torch.tensor of token indexes\n",
    "            tag_idx : torch.tensor of tag indexes\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "        sentence_idx = []\n",
    "        tag_idx = []\n",
    "        for token, tag in zip(sentence, tags):\n",
    "            if token in self.token_to_idx:\n",
    "                sentence_idx.append(self.token_to_idx[token])\n",
    "            else:\n",
    "                sentence_idx.append(self.token_to_idx[\"<UNK>\"])\n",
    "            tag_idx.append(self.tag_to_idx[tag])\n",
    "\n",
    "        sentence_idx = torch.tensor(sentence_idx, dtype=torch.int64)\n",
    "        tag_idx = torch.tensor(tag_idx, dtype=torch.int64)\n",
    "        \n",
    "        return sentence_idx, tag_idx\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        return len(self.sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAo3KQbpsiPR"
   },
   "source": [
    "Для того, чтобы дополнять последовательности паддингом, будем использовать параметр `collate_fn` класса `DataLoader`. Принимая последовательность пар тензоров для предложений и тегов, необходимо дополнить все последовательности до последовательности максимальной длины в батче. Используйте специальные теги `<PAD>` и `O` для дополнения.\n",
    "\n",
    "**<font color='red'>Задание. Реализуйте класс PaddingCollator.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "QWFE4eCFsiPR"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class PaddingCollator:\n",
    "    def __init__(self,  pad_token_id, pad_tag_id, batch_first=True):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.pad_tag_id = pad_tag_id\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: list of tuples of torch.tensors\n",
    "\n",
    "        Returns:\n",
    "            new_sentences: torch.tensor\n",
    "            new_tags: torch.tensor\n",
    "                Both tensors have the same size\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        sentences, tags = zip(*batch)\n",
    "\n",
    "        new_sentences = pad_sequence(sentences, batch_first=self.batch_first, padding_value=self.pad_token_id)\n",
    "        new_tags = pad_sequence(tags, batch_first=self.batch_first, padding_value=self.pad_tag_id)\n",
    "        return new_sentences, new_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM99UVWZsiPS"
   },
   "source": [
    "Теперь всё готово, чтобы задать DataLoader. Протестируйте на примере ниже, что всё работает правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "bdhBbkCfsiPS"
   },
   "outputs": [],
   "source": [
    "small_dataset = TaggingDataset(\n",
    "    sentences=train_sentences[:7],\n",
    "    tags=train_tags[:7],\n",
    "    token_to_idx=token_to_idx,\n",
    "    tag_to_idx=tag_to_idx,\n",
    ")\n",
    "\n",
    "small_loader = DataLoader(\n",
    "    small_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=PaddingCollator(\n",
    "        pad_token_id=token_to_idx['<PAD>'],\n",
    "        pad_tag_id=tag_to_idx['O'],\n",
    "        batch_first=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "batch_lengths = [3, 3, 1]\n",
    "sequence_lengths = [26, 25, 8]\n",
    "some_pad_tensor = torch.LongTensor([token_to_idx['<PAD>']] * 12)\n",
    "some_outside_tensor = torch.LongTensor([[tag_to_idx['O']] * 12])\n",
    "\n",
    "for i, (tokens_batch, tags_batch) in enumerate(small_loader):\n",
    "    assert tokens_batch.dtype == torch.int64, 'tokens_batch is not LongTensor'\n",
    "    assert tags_batch.dtype == torch.int64, 'tags_batch is not LongTensor'\n",
    "\n",
    "    assert len(tokens_batch) == batch_lengths[i], 'wrong batch length'\n",
    "\n",
    "    for one_token_sequence in tokens_batch:\n",
    "        assert len(one_token_sequence) == sequence_lengths[i], 'wrong length of sequence in batch'\n",
    "\n",
    "    if i == 0:\n",
    "        assert torch.all(tokens_batch[2][-12:] == some_pad_tensor), \"wrong padding\"\n",
    "        assert torch.all(tags_batch[2][-12:] == some_outside_tensor), \"wrong O tag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXT5zDLHsiPS"
   },
   "source": [
    "**<font color='red'>Задание. В ячейке ниже задайте датасеты и загрузчики для обучающих, валидационных и тестовых данных.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "LLhMqsmIsiPS"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "\n",
    "train_dataset = TaggingDataset(\n",
    "    sentences=train_sentences,\n",
    "    tags=train_tags,\n",
    "    token_to_idx=token_to_idx,\n",
    "    tag_to_idx=tag_to_idx,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    collate_fn=PaddingCollator(\n",
    "        pad_token_id=token_to_idx['<PAD>'],\n",
    "        pad_tag_id=tag_to_idx['O'],\n",
    "        batch_first=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TaggingDataset(\n",
    "    sentences=val_sentences,\n",
    "    tags=val_tags,\n",
    "    token_to_idx=token_to_idx,\n",
    "    tag_to_idx=tag_to_idx,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=PaddingCollator(\n",
    "        pad_token_id=token_to_idx['<PAD>'],\n",
    "        pad_tag_id=tag_to_idx['O'],\n",
    "        batch_first=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TaggingDataset(\n",
    "    sentences=test_sentences,\n",
    "    tags=test_tags,\n",
    "    token_to_idx=token_to_idx,\n",
    "    tag_to_idx=tag_to_idx,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=PaddingCollator(\n",
    "        pad_token_id=token_to_idx['<PAD>'],\n",
    "        pad_tag_id=tag_to_idx['O'],\n",
    "        batch_first=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Rf1neysiPS"
   },
   "source": [
    "## Часть 2. BiLSTM-теггер (4 балла)\n",
    "\n",
    "Определите архитектуру сети, используя библиотеку pytorch.\n",
    "\n",
    "Ваша архитектура в этом пункте должна соответствовать стандартному теггеру (см. лекцию):\n",
    "* Embedding слой на входе\n",
    "* Двунаправленный LSTM слой для обработки последовательности\n",
    "* Используйте dropout (заданный отдельно или встроенный в LSTM) для уменьшения переобучения\n",
    "* Linear слой на выходе\n",
    "\n",
    "Для обучения сети используйте поэлементную кросс-энтропийную функцию потерь.\n",
    "**Обратите внимание**, что `<PAD>` токены не должны участвовать в подсчёте функции потерь. В качестве оптимизатора рекомендуется использовать Adam. Для получения значений предсказаний по выходам модели используйте функцию $\\arg\\max$.\n",
    "\n",
    "**<font color='red'>Задание. Задайте архитектуру сети и требуемые методы.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "_x7xCtU3siPS"
   },
   "outputs": [],
   "source": [
    "class BiLSTMModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size,\n",
    "        tag_space_size,\n",
    "        pad_token_idx,\n",
    "        embedding_dim,\n",
    "        lstm_hidden_size,\n",
    "        dropout_zeroed_probability,\n",
    "        device='cuda',\n",
    "        max_norm=None,\n",
    "    ):\n",
    "        '''\n",
    "        Defines neural network structure.\n",
    "\n",
    "        architecture: input -> Embedding -> BiLSTM with Dropout -> Linear\n",
    "\n",
    "        ----------\n",
    "        Parameters\n",
    "\n",
    "        vocabulary_size: int, number of words in vocabulary.\n",
    "        tag_space_size: int, number of tags.\n",
    "        pad_token_idx: int, index of padding character. Used for loss masking.\n",
    "        embedding_dim: int, dimension of words' embeddings.\n",
    "        lstm_hidden_size: int, number of hidden units in each LSTM cell\n",
    "        dropout_zeroed_probability: float, dropout zeroed probability for Dropout layer.\n",
    "        device: str, cpu or cuda:x\n",
    "        '''\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocabulary_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=pad_token_idx,\n",
    "            max_norm=max_norm,\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_zeroed_probability,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=dropout_zeroed_probability)\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            in_features=lstm_hidden_size * 2,\n",
    "            out_features=tag_space_size\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        '''\n",
    "        Makes forward pass.\n",
    "\n",
    "        ----------\n",
    "        Parameters\n",
    "        x_batch: torch.LongTensor with shape (number of samples in batch, number words in sentence).\n",
    "        '''\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        x_embedded = self.embedding(x_batch)\n",
    "        lstm_out, _ = self.lstm(x_embedded)\n",
    "        return self.linear(lstm_out)\n",
    "\n",
    "    def predict_for_batch(self, x_batch):\n",
    "        '''\n",
    "        Returns predictions for x_batch. Use argmax function.\n",
    "\n",
    "        return type: torch.LongTensor\n",
    "        return shape: (number of samples in batch, number words in sentence.\n",
    "\n",
    "        ----------\n",
    "        Parameters\n",
    "        x_batch: torch.LongTensor with shape (number of samples in batch, number words in sentence).\n",
    "        '''\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        x_batch = x_batch.to(self.device)\n",
    "        logits = self.forward(x_batch)\n",
    "        return torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn-zdPdNsiPT"
   },
   "source": [
    "Для тестирования сети мы подготовили для вас класс ScoreEvaluator с двумя полезными методами:\n",
    " - *predict_tags*: получает батч данных и трансформирует его в список из токенов и предсказанных тегов;\n",
    " - *eval_conll*: вычисляет метрики precision, recall и F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "sAgUvz44siPT"
   },
   "outputs": [],
   "source": [
    "from evaluation_ner import ScoreEvaluator\n",
    "\n",
    "evaluator = ScoreEvaluator(\n",
    "    token_to_idx=token_to_idx,\n",
    "    idx_to_tag=idx_to_tag,\n",
    "    idx_to_token=idx_to_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzLwjGgmsiPT"
   },
   "source": [
    "### Эксперименты\n",
    "\n",
    "Задайте BiLSTMModel. Рекомендуем начать с параметров:\n",
    "- *batch_size*: 32;\n",
    "- начальное значение *learning_rate*: 0.01-0.001\n",
    "- *dropout_zeroed_probability*: 0.5-0.7\n",
    "- *embedding_dim*: 100-200\n",
    "- *rnn_hidden_size*: 150-200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZzhh6WtsiPT"
   },
   "source": [
    "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1 мере на валидационной и тестовой выборках было не меньше 0.35. При некотором усердии, можно достичь результата 0.45 по F1 на обоих датасетах.\n",
    "\n",
    "Если сеть плохо обучается, попробуйте использовать следующие модификации:\n",
    "\n",
    "    * используйте gradient clipping\n",
    "    * ограничивайте норму эмбеддингов через параметр max_norm (сопоставляйте с значениями в клиппинге)\n",
    "    * на каждой итерации уменьшайте learning rate (например, в 1.1 раз)\n",
    "    * попробуйте вместо Adam другие оптимизаторы\n",
    "    * используйте l2 регуляризацию\n",
    "    * экспериментируйте с значением dropout\n",
    "\n",
    "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook).\n",
    "\n",
    "**<font color='red'>Задание. Проведите требуемые эксперименты.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "5BpgNzSysiPT"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, criterion, optimizer, train_loader, val_loader, \n",
    "          evaluator, max_grad_norm=None, scheduler=None, device=\"cuda\"):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "    \n",
    "        for tokens_batch, tags_batch in train_loader:\n",
    "            tokens_batch = tokens_batch.to(device)\n",
    "            tags_batch = tags_batch.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(tokens_batch)\n",
    "            logits_reshaped = outputs.view(-1, outputs.shape[-1])\n",
    "            tags_reshaped = tags_batch.view(-1)\n",
    "    \n",
    "            loss = criterion(logits_reshaped, tags_reshaped)\n",
    "            loss.backward()\n",
    "            if max_grad_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        total_loss /= len(train_loader)\n",
    "    \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for tokens_batch, tags_batch in val_loader:\n",
    "                tokens_batch = tokens_batch.to(device)\n",
    "                tags_batch = tags_batch.to(device)\n",
    "    \n",
    "                outputs = model(tokens_batch)\n",
    "                logits_reshaped = outputs.view(-1, outputs.shape[-1])\n",
    "                tags_reshaped = tags_batch.view(-1)\n",
    "    \n",
    "                loss = loss_fn(logits_reshaped, tags_reshaped)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(evaluator.eval_conll(model, val_loader))\n",
    "        print()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = BiLSTMModel(\n",
    "        vocabulary_size=len(token_to_idx),\n",
    "        tag_space_size=len(tag_to_idx),\n",
    "        pad_token_idx=token_to_idx['<PAD>'],\n",
    "        embedding_dim=150,\n",
    "        lstm_hidden_size=200,\n",
    "        dropout_zeroed_probability=0.5).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.4605, Val Loss: 0.3502\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 2: Train Loss: 0.3161, Val Loss: 0.2659\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 3: Train Loss: 0.2738, Val Loss: 0.2441\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 4: Train Loss: 0.2531, Val Loss: 0.2303\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 5: Train Loss: 0.2310, Val Loss: 0.2090\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 6: Train Loss: 0.2111, Val Loss: 0.1967\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 7: Train Loss: 0.1991, Val Loss: 0.1894\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 8: Train Loss: 0.1865, Val Loss: 0.1829\n",
      "{'precision': 40.0, 'recall': 0.37243947858473, 'f1': 0.7380073800738007, 'n_predicted_entities': 5, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 9: Train Loss: 0.1726, Val Loss: 0.1766\n",
      "{'precision': 32.0, 'recall': 1.48975791433892, 'f1': 2.8469750889679717, 'n_predicted_entities': 25, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 10: Train Loss: 0.1614, Val Loss: 0.1713\n",
      "{'precision': 32.83582089552239, 'recall': 4.0968342644320295, 'f1': 7.284768211920529, 'n_predicted_entities': 67, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 11: Train Loss: 0.1491, Val Loss: 0.1695\n",
      "{'precision': 40.0, 'recall': 6.33147113594041, 'f1': 10.932475884244374, 'n_predicted_entities': 85, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 12: Train Loss: 0.1354, Val Loss: 0.1645\n",
      "{'precision': 36.58536585365854, 'recall': 11.1731843575419, 'f1': 17.118402282453637, 'n_predicted_entities': 164, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 13: Train Loss: 0.1247, Val Loss: 0.1638\n",
      "{'precision': 34.63687150837989, 'recall': 11.54562383612663, 'f1': 17.318435754189945, 'n_predicted_entities': 179, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 14: Train Loss: 0.1141, Val Loss: 0.1636\n",
      "{'precision': 35.27131782945737, 'recall': 16.945996275605214, 'f1': 22.89308176100629, 'n_predicted_entities': 258, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 15: Train Loss: 0.1037, Val Loss: 0.1618\n",
      "{'precision': 35.6401384083045, 'recall': 19.180633147113593, 'f1': 24.93946731234867, 'n_predicted_entities': 289, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 16: Train Loss: 0.0935, Val Loss: 0.1643\n",
      "{'precision': 29.6398891966759, 'recall': 19.925512104283055, 'f1': 23.830734966592427, 'n_predicted_entities': 361, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 17: Train Loss: 0.0851, Val Loss: 0.1668\n",
      "{'precision': 35.60371517027864, 'recall': 21.415270018621975, 'f1': 26.74418604651163, 'n_predicted_entities': 323, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 18: Train Loss: 0.0764, Val Loss: 0.1679\n",
      "{'precision': 31.510416666666668, 'recall': 22.532588454376164, 'f1': 26.275787187839306, 'n_predicted_entities': 384, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 19: Train Loss: 0.0678, Val Loss: 0.1698\n",
      "{'precision': 29.73621103117506, 'recall': 23.09124767225326, 'f1': 25.9958071278826, 'n_predicted_entities': 417, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 20: Train Loss: 0.0604, Val Loss: 0.1754\n",
      "{'precision': 32.03125, 'recall': 22.905027932960895, 'f1': 26.710097719869708, 'n_predicted_entities': 384, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 21: Train Loss: 0.0542, Val Loss: 0.1760\n",
      "{'precision': 27.67295597484277, 'recall': 24.58100558659218, 'f1': 26.035502958579883, 'n_predicted_entities': 477, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 22: Train Loss: 0.0479, Val Loss: 0.1809\n",
      "{'precision': 28.571428571428573, 'recall': 24.20856610800745, 'f1': 26.20967741935484, 'n_predicted_entities': 455, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 23: Train Loss: 0.0415, Val Loss: 0.1883\n",
      "{'precision': 30.06993006993007, 'recall': 24.022346368715084, 'f1': 26.70807453416149, 'n_predicted_entities': 429, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 24: Train Loss: 0.0371, Val Loss: 0.1911\n",
      "{'precision': 29.908675799086758, 'recall': 24.394785847299815, 'f1': 26.871794871794872, 'n_predicted_entities': 438, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 25: Train Loss: 0.0322, Val Loss: 0.1965\n",
      "{'precision': 30.02257336343115, 'recall': 24.767225325884542, 'f1': 27.142857142857142, 'n_predicted_entities': 443, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 26: Train Loss: 0.0283, Val Loss: 0.1984\n",
      "{'precision': 27.291666666666668, 'recall': 24.394785847299815, 'f1': 25.762045231071777, 'n_predicted_entities': 480, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 27: Train Loss: 0.0250, Val Loss: 0.2056\n",
      "{'precision': 29.13907284768212, 'recall': 24.58100558659218, 'f1': 26.666666666666668, 'n_predicted_entities': 453, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 28: Train Loss: 0.0217, Val Loss: 0.2090\n",
      "{'precision': 29.333333333333332, 'recall': 24.58100558659218, 'f1': 26.747720364741642, 'n_predicted_entities': 450, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 29: Train Loss: 0.0188, Val Loss: 0.2119\n",
      "{'precision': 26.5748031496063, 'recall': 25.139664804469273, 'f1': 25.83732057416268, 'n_predicted_entities': 508, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 30: Train Loss: 0.0165, Val Loss: 0.2189\n",
      "{'precision': 27.718550106609808, 'recall': 24.20856610800745, 'f1': 25.84493041749503, 'n_predicted_entities': 469, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 31: Train Loss: 0.0143, Val Loss: 0.2201\n",
      "{'precision': 26.614481409001957, 'recall': 25.32588454376164, 'f1': 25.95419847328244, 'n_predicted_entities': 511, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 32: Train Loss: 0.0123, Val Loss: 0.2251\n",
      "{'precision': 29.805615550755938, 'recall': 25.69832402234637, 'f1': 27.6, 'n_predicted_entities': 463, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 33: Train Loss: 0.0107, Val Loss: 0.2309\n",
      "{'precision': 28.02547770700637, 'recall': 24.58100558659218, 'f1': 26.190476190476193, 'n_predicted_entities': 471, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 34: Train Loss: 0.0093, Val Loss: 0.2307\n",
      "{'precision': 26.94610778443114, 'recall': 25.139664804469273, 'f1': 26.01156069364162, 'n_predicted_entities': 501, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 35: Train Loss: 0.0083, Val Loss: 0.2361\n",
      "{'precision': 28.958333333333332, 'recall': 25.884543761638735, 'f1': 27.335299901671583, 'n_predicted_entities': 480, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 36: Train Loss: 0.0071, Val Loss: 0.2370\n",
      "{'precision': 27.254901960784313, 'recall': 25.884543761638735, 'f1': 26.552053486150907, 'n_predicted_entities': 510, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 37: Train Loss: 0.0064, Val Loss: 0.2434\n",
      "{'precision': 28.0, 'recall': 24.767225325884542, 'f1': 26.284584980237156, 'n_predicted_entities': 475, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 38: Train Loss: 0.0057, Val Loss: 0.2468\n",
      "{'precision': 27.479338842975206, 'recall': 24.767225325884542, 'f1': 26.05288932419197, 'n_predicted_entities': 484, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 39: Train Loss: 0.0052, Val Loss: 0.2490\n",
      "{'precision': 26.195028680688335, 'recall': 25.512104283054004, 'f1': 25.849056603773587, 'n_predicted_entities': 523, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 40: Train Loss: 0.0047, Val Loss: 0.2520\n",
      "{'precision': 28.481012658227847, 'recall': 25.139664804469273, 'f1': 26.706231454005938, 'n_predicted_entities': 474, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 41: Train Loss: 0.0043, Val Loss: 0.2539\n",
      "{'precision': 26.53846153846154, 'recall': 25.69832402234637, 'f1': 26.111636707663198, 'n_predicted_entities': 520, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 42: Train Loss: 0.0038, Val Loss: 0.2562\n",
      "{'precision': 27.272727272727273, 'recall': 25.69832402234637, 'f1': 26.462128475551296, 'n_predicted_entities': 506, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 43: Train Loss: 0.0034, Val Loss: 0.2583\n",
      "{'precision': 28.105906313645622, 'recall': 25.69832402234637, 'f1': 26.848249027237355, 'n_predicted_entities': 491, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 44: Train Loss: 0.0031, Val Loss: 0.2607\n",
      "{'precision': 27.38095238095238, 'recall': 25.69832402234637, 'f1': 26.512968299711815, 'n_predicted_entities': 504, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 45: Train Loss: 0.0028, Val Loss: 0.2603\n",
      "{'precision': 27.272727272727273, 'recall': 26.256983240223462, 'f1': 26.755218216318784, 'n_predicted_entities': 517, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 46: Train Loss: 0.0027, Val Loss: 0.2609\n",
      "{'precision': 26.425855513307983, 'recall': 25.884543761638735, 'f1': 26.152398871119473, 'n_predicted_entities': 526, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 47: Train Loss: 0.0025, Val Loss: 0.2660\n",
      "{'precision': 27.744510978043913, 'recall': 25.884543761638735, 'f1': 26.782273603082853, 'n_predicted_entities': 501, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 48: Train Loss: 0.0022, Val Loss: 0.2701\n",
      "{'precision': 27.84552845528455, 'recall': 25.512104283054004, 'f1': 26.627793974732754, 'n_predicted_entities': 492, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 49: Train Loss: 0.0021, Val Loss: 0.2714\n",
      "{'precision': 29.140461215932913, 'recall': 25.884543761638735, 'f1': 27.416173570019726, 'n_predicted_entities': 477, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 50: Train Loss: 0.0021, Val Loss: 0.2735\n",
      "{'precision': 28.8659793814433, 'recall': 26.070763500931097, 'f1': 27.397260273972602, 'n_predicted_entities': 485, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 51: Train Loss: 0.0020, Val Loss: 0.2738\n",
      "{'precision': 27.38095238095238, 'recall': 25.69832402234637, 'f1': 26.512968299711815, 'n_predicted_entities': 504, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 52: Train Loss: 0.0019, Val Loss: 0.2754\n",
      "{'precision': 30.021141649048626, 'recall': 26.443202979515828, 'f1': 28.11881188118812, 'n_predicted_entities': 473, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 53: Train Loss: 0.0019, Val Loss: 0.2734\n",
      "{'precision': 25.22361359570662, 'recall': 26.256983240223462, 'f1': 25.729927007299267, 'n_predicted_entities': 559, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 54: Train Loss: 0.0017, Val Loss: 0.2745\n",
      "{'precision': 26.74418604651163, 'recall': 25.69832402234637, 'f1': 26.21082621082621, 'n_predicted_entities': 516, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 55: Train Loss: 0.0017, Val Loss: 0.2760\n",
      "{'precision': 24.910394265232974, 'recall': 25.884543761638735, 'f1': 25.38812785388128, 'n_predicted_entities': 558, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 56: Train Loss: 0.0014, Val Loss: 0.2794\n",
      "{'precision': 27.131782945736433, 'recall': 26.070763500931097, 'f1': 26.590693257359927, 'n_predicted_entities': 516, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 57: Train Loss: 0.0014, Val Loss: 0.2862\n",
      "{'precision': 28.72340425531915, 'recall': 25.139664804469273, 'f1': 26.812313803376366, 'n_predicted_entities': 470, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 58: Train Loss: 0.0013, Val Loss: 0.2825\n",
      "{'precision': 26.95984703632887, 'recall': 26.256983240223462, 'f1': 26.603773584905664, 'n_predicted_entities': 523, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 59: Train Loss: 0.0013, Val Loss: 0.2821\n",
      "{'precision': 26.692456479690524, 'recall': 25.69832402234637, 'f1': 26.18595825426945, 'n_predicted_entities': 517, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 60: Train Loss: 0.0012, Val Loss: 0.2873\n",
      "{'precision': 28.04123711340206, 'recall': 25.32588454376164, 'f1': 26.614481409001954, 'n_predicted_entities': 485, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 61: Train Loss: 0.0011, Val Loss: 0.2866\n",
      "{'precision': 27.06333973128599, 'recall': 26.256983240223462, 'f1': 26.65406427221172, 'n_predicted_entities': 521, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 62: Train Loss: 0.0011, Val Loss: 0.2878\n",
      "{'precision': 26.93069306930693, 'recall': 25.32588454376164, 'f1': 26.103646833013435, 'n_predicted_entities': 505, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 63: Train Loss: 0.0010, Val Loss: 0.2921\n",
      "{'precision': 28.8659793814433, 'recall': 26.070763500931097, 'f1': 27.397260273972602, 'n_predicted_entities': 485, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 64: Train Loss: 0.0010, Val Loss: 0.2892\n",
      "{'precision': 26.705653021442494, 'recall': 25.512104283054004, 'f1': 26.095238095238095, 'n_predicted_entities': 513, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 65: Train Loss: 0.0010, Val Loss: 0.2898\n",
      "{'precision': 26.18595825426945, 'recall': 25.69832402234637, 'f1': 25.93984962406015, 'n_predicted_entities': 527, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 66: Train Loss: 0.0010, Val Loss: 0.2944\n",
      "{'precision': 28.45360824742268, 'recall': 25.69832402234637, 'f1': 27.00587084148728, 'n_predicted_entities': 485, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 67: Train Loss: 0.0009, Val Loss: 0.2930\n",
      "{'precision': 26.01156069364162, 'recall': 25.139664804469273, 'f1': 25.56818181818182, 'n_predicted_entities': 519, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 68: Train Loss: 0.0009, Val Loss: 0.2972\n",
      "{'precision': 28.252032520325205, 'recall': 25.884543761638735, 'f1': 27.016520894071917, 'n_predicted_entities': 492, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 69: Train Loss: 0.0009, Val Loss: 0.2960\n",
      "{'precision': 26.53846153846154, 'recall': 25.69832402234637, 'f1': 26.111636707663198, 'n_predicted_entities': 520, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 70: Train Loss: 0.0008, Val Loss: 0.2948\n",
      "{'precision': 26.08695652173913, 'recall': 25.69832402234637, 'f1': 25.891181988742968, 'n_predicted_entities': 529, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 71: Train Loss: 0.0008, Val Loss: 0.2978\n",
      "{'precision': 27.165354330708663, 'recall': 25.69832402234637, 'f1': 26.411483253588514, 'n_predicted_entities': 508, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 72: Train Loss: 0.0008, Val Loss: 0.2982\n",
      "{'precision': 27.131782945736433, 'recall': 26.070763500931097, 'f1': 26.590693257359927, 'n_predicted_entities': 516, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 73: Train Loss: 0.0008, Val Loss: 0.3041\n",
      "{'precision': 28.51153039832285, 'recall': 25.32588454376164, 'f1': 26.824457593688358, 'n_predicted_entities': 477, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 74: Train Loss: 0.0008, Val Loss: 0.3040\n",
      "{'precision': 28.278688524590162, 'recall': 25.69832402234637, 'f1': 26.92682926829268, 'n_predicted_entities': 488, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 75: Train Loss: 0.0008, Val Loss: 0.3005\n",
      "{'precision': 26.365348399246706, 'recall': 26.070763500931097, 'f1': 26.217228464419474, 'n_predicted_entities': 531, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 76: Train Loss: 0.0007, Val Loss: 0.2988\n",
      "{'precision': 26.55367231638418, 'recall': 26.256983240223462, 'f1': 26.40449438202247, 'n_predicted_entities': 531, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 77: Train Loss: 0.0007, Val Loss: 0.3052\n",
      "{'precision': 27.38095238095238, 'recall': 25.69832402234637, 'f1': 26.512968299711815, 'n_predicted_entities': 504, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 78: Train Loss: 0.0007, Val Loss: 0.3066\n",
      "{'precision': 28.60082304526749, 'recall': 25.884543761638735, 'f1': 27.174975562072333, 'n_predicted_entities': 486, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 79: Train Loss: 0.0007, Val Loss: 0.3053\n",
      "{'precision': 26.755218216318784, 'recall': 26.256983240223462, 'f1': 26.503759398496243, 'n_predicted_entities': 527, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 80: Train Loss: 0.0007, Val Loss: 0.3063\n",
      "{'precision': 27.524752475247524, 'recall': 25.884543761638735, 'f1': 26.679462571976966, 'n_predicted_entities': 505, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 81: Train Loss: 0.0007, Val Loss: 0.3116\n",
      "{'precision': 28.541666666666668, 'recall': 25.512104283054004, 'f1': 26.94198623402163, 'n_predicted_entities': 480, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 82: Train Loss: 0.0007, Val Loss: 0.3043\n",
      "{'precision': 24.82638888888889, 'recall': 26.629422718808193, 'f1': 25.696316262353996, 'n_predicted_entities': 576, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 83: Train Loss: 0.0007, Val Loss: 0.3013\n",
      "{'precision': 26.44927536231884, 'recall': 27.18808193668529, 'f1': 26.813590449954084, 'n_predicted_entities': 552, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 84: Train Loss: 0.0006, Val Loss: 0.3102\n",
      "{'precision': 27.5049115913556, 'recall': 26.070763500931097, 'f1': 26.768642447418735, 'n_predicted_entities': 509, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 85: Train Loss: 0.0006, Val Loss: 0.3080\n",
      "{'precision': 27.1484375, 'recall': 25.884543761638735, 'f1': 26.50142993326978, 'n_predicted_entities': 512, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 86: Train Loss: 0.0006, Val Loss: 0.3075\n",
      "{'precision': 27.151051625239006, 'recall': 26.443202979515828, 'f1': 26.792452830188676, 'n_predicted_entities': 523, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 87: Train Loss: 0.0006, Val Loss: 0.3131\n",
      "{'precision': 28.1947261663286, 'recall': 25.884543761638735, 'f1': 26.990291262135923, 'n_predicted_entities': 493, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 88: Train Loss: 0.0005, Val Loss: 0.3106\n",
      "{'precision': 27.46615087040619, 'recall': 26.443202979515828, 'f1': 26.944971537001898, 'n_predicted_entities': 517, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 89: Train Loss: 0.0005, Val Loss: 0.3131\n",
      "{'precision': 26.88588007736944, 'recall': 25.884543761638735, 'f1': 26.375711574952565, 'n_predicted_entities': 517, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 90: Train Loss: 0.0005, Val Loss: 0.3133\n",
      "{'precision': 27.09551656920078, 'recall': 25.884543761638735, 'f1': 26.476190476190474, 'n_predicted_entities': 513, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 91: Train Loss: 0.0005, Val Loss: 0.3150\n",
      "{'precision': 27.559055118110237, 'recall': 26.070763500931097, 'f1': 26.794258373205743, 'n_predicted_entities': 508, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 92: Train Loss: 0.0005, Val Loss: 0.3175\n",
      "{'precision': 27.888446215139442, 'recall': 26.070763500931097, 'f1': 26.948989412897014, 'n_predicted_entities': 502, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 93: Train Loss: 0.0006, Val Loss: 0.3128\n",
      "{'precision': 26.07913669064748, 'recall': 27.001862197392924, 'f1': 26.532479414455626, 'n_predicted_entities': 556, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 94: Train Loss: 0.0005, Val Loss: 0.3131\n",
      "{'precision': 26.499032882011605, 'recall': 25.512104283054004, 'f1': 25.996204933586338, 'n_predicted_entities': 517, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 95: Train Loss: 0.0005, Val Loss: 0.3135\n",
      "{'precision': 26.454033771106943, 'recall': 26.256983240223462, 'f1': 26.355140186915886, 'n_predicted_entities': 533, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 96: Train Loss: 0.0005, Val Loss: 0.3205\n",
      "{'precision': 27.290836653386453, 'recall': 25.512104283054004, 'f1': 26.37151106833494, 'n_predicted_entities': 502, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 97: Train Loss: 0.0005, Val Loss: 0.3198\n",
      "{'precision': 26.48752399232246, 'recall': 25.69832402234637, 'f1': 26.08695652173913, 'n_predicted_entities': 521, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 98: Train Loss: 0.0005, Val Loss: 0.3163\n",
      "{'precision': 26.34508348794063, 'recall': 26.443202979515828, 'f1': 26.394052044609662, 'n_predicted_entities': 539, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 99: Train Loss: 0.0005, Val Loss: 0.3174\n",
      "{'precision': 26.11218568665377, 'recall': 25.139664804469273, 'f1': 25.616698292220114, 'n_predicted_entities': 517, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 100: Train Loss: 0.0005, Val Loss: 0.3194\n",
      "{'precision': 27.358490566037737, 'recall': 27.001862197392924, 'f1': 27.179006560449864, 'n_predicted_entities': 530, 'n_true_entities': 537}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100\n",
    "model_trained = train(num_epochs, model, criterion, optimizer, train_loader, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 95.91286772962049, 'recall': 95.14368456226332, 'f1': 95.52672780138673, 'n_predicted_entities': 4453, 'n_true_entities': 4489}\n",
      "{'precision': 27.358490566037737, 'recall': 27.001862197392924, 'f1': 27.179006560449864, 'n_predicted_entities': 530, 'n_true_entities': 537}\n",
      "{'precision': 50.47923322683706, 'recall': 26.158940397350992, 'f1': 34.46019629225736, 'n_predicted_entities': 313, 'n_true_entities': 604}\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.eval_conll(model_trained, train_loader))\n",
    "print(evaluator.eval_conll(model_trained, val_loader))\n",
    "print(evaluator.eval_conll(model_trained, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.5693, Val Loss: 0.3094\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 2: Train Loss: 0.2981, Val Loss: 0.2579\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 3: Train Loss: 0.2719, Val Loss: 0.2461\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 4: Train Loss: 0.2431, Val Loss: 0.2142\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 5: Train Loss: 0.2010, Val Loss: 0.1930\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 6: Train Loss: 0.1666, Val Loss: 0.1745\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 7: Train Loss: 0.1396, Val Loss: 0.1769\n",
      "{'precision': 9.243697478991596, 'recall': 2.0484171322160147, 'f1': 3.3536585365853653, 'n_predicted_entities': 119, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 8: Train Loss: 0.1233, Val Loss: 0.1674\n",
      "{'precision': 9.130434782608695, 'recall': 3.910614525139665, 'f1': 5.475880052151239, 'n_predicted_entities': 230, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 9: Train Loss: 0.1117, Val Loss: 0.1812\n",
      "{'precision': 10.76923076923077, 'recall': 5.21415270018622, 'f1': 7.0263488080301135, 'n_predicted_entities': 260, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 10: Train Loss: 0.1028, Val Loss: 0.1773\n",
      "{'precision': 12.063492063492063, 'recall': 7.07635009310987, 'f1': 8.92018779342723, 'n_predicted_entities': 315, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 11: Train Loss: 0.0944, Val Loss: 0.1780\n",
      "{'precision': 14.040114613180515, 'recall': 9.124767225325884, 'f1': 11.060948081264108, 'n_predicted_entities': 349, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 12: Train Loss: 0.0852, Val Loss: 0.1846\n",
      "{'precision': 17.251461988304094, 'recall': 10.986964618249534, 'f1': 13.424345847554038, 'n_predicted_entities': 342, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 13: Train Loss: 0.0798, Val Loss: 0.1957\n",
      "{'precision': 18.862275449101798, 'recall': 11.731843575418994, 'f1': 14.46613088404133, 'n_predicted_entities': 334, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 14: Train Loss: 0.0736, Val Loss: 0.1936\n",
      "{'precision': 24.198250728862973, 'recall': 15.456238361266294, 'f1': 18.863636363636363, 'n_predicted_entities': 343, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 15: Train Loss: 0.0672, Val Loss: 0.2078\n",
      "{'precision': 27.027027027027028, 'recall': 16.75977653631285, 'f1': 20.689655172413794, 'n_predicted_entities': 333, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 16: Train Loss: 0.0597, Val Loss: 0.2026\n",
      "{'precision': 30.140845070422536, 'recall': 19.925512104283055, 'f1': 23.991031390134534, 'n_predicted_entities': 355, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 17: Train Loss: 0.0527, Val Loss: 0.2064\n",
      "{'precision': 35.88235294117647, 'recall': 22.71880819366853, 'f1': 27.82212086659065, 'n_predicted_entities': 340, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 18: Train Loss: 0.0452, Val Loss: 0.2197\n",
      "{'precision': 34.01759530791789, 'recall': 21.60148975791434, 'f1': 26.42369020501139, 'n_predicted_entities': 341, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 19: Train Loss: 0.0378, Val Loss: 0.2147\n",
      "{'precision': 36.206896551724135, 'recall': 23.463687150837988, 'f1': 28.47457627118644, 'n_predicted_entities': 348, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 20: Train Loss: 0.0311, Val Loss: 0.2180\n",
      "{'precision': 37.22222222222222, 'recall': 24.953445065176908, 'f1': 29.87736900780379, 'n_predicted_entities': 360, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 21: Train Loss: 0.0254, Val Loss: 0.2328\n",
      "{'precision': 42.675159235668794, 'recall': 24.953445065176908, 'f1': 31.492361927144543, 'n_predicted_entities': 314, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 22: Train Loss: 0.0207, Val Loss: 0.2396\n",
      "{'precision': 39.88095238095238, 'recall': 24.953445065176908, 'f1': 30.69873997709049, 'n_predicted_entities': 336, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 23: Train Loss: 0.0172, Val Loss: 0.2471\n",
      "{'precision': 40.74074074074074, 'recall': 24.58100558659218, 'f1': 30.66202090592335, 'n_predicted_entities': 324, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 24: Train Loss: 0.0143, Val Loss: 0.2377\n",
      "{'precision': 40.289855072463766, 'recall': 25.884543761638735, 'f1': 31.51927437641723, 'n_predicted_entities': 345, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 25: Train Loss: 0.0126, Val Loss: 0.2504\n",
      "{'precision': 43.11926605504587, 'recall': 26.256983240223462, 'f1': 32.638888888888886, 'n_predicted_entities': 327, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 26: Train Loss: 0.0109, Val Loss: 0.2513\n",
      "{'precision': 40.168539325842694, 'recall': 26.629422718808193, 'f1': 32.026875699888016, 'n_predicted_entities': 356, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 27: Train Loss: 0.0095, Val Loss: 0.2582\n",
      "{'precision': 39.76945244956772, 'recall': 25.69832402234637, 'f1': 31.221719457013574, 'n_predicted_entities': 347, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 28: Train Loss: 0.0079, Val Loss: 0.2632\n",
      "{'precision': 41.616766467065865, 'recall': 25.884543761638735, 'f1': 31.917336394948336, 'n_predicted_entities': 334, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 29: Train Loss: 0.0068, Val Loss: 0.2640\n",
      "{'precision': 39.83516483516483, 'recall': 27.001862197392924, 'f1': 32.186459489456155, 'n_predicted_entities': 364, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 30: Train Loss: 0.0058, Val Loss: 0.2701\n",
      "{'precision': 41.45658263305322, 'recall': 27.560521415270017, 'f1': 33.1096196868009, 'n_predicted_entities': 357, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 31: Train Loss: 0.0052, Val Loss: 0.2632\n",
      "{'precision': 40.48257372654155, 'recall': 28.119180633147113, 'f1': 33.18681318681318, 'n_predicted_entities': 373, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 32: Train Loss: 0.0047, Val Loss: 0.2838\n",
      "{'precision': 39.88095238095238, 'recall': 24.953445065176908, 'f1': 30.69873997709049, 'n_predicted_entities': 336, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 33: Train Loss: 0.0042, Val Loss: 0.2776\n",
      "{'precision': 38.547486033519554, 'recall': 25.69832402234637, 'f1': 30.837988826815646, 'n_predicted_entities': 358, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 34: Train Loss: 0.0036, Val Loss: 0.2868\n",
      "{'precision': 42.31884057971015, 'recall': 27.18808193668529, 'f1': 33.106575963718825, 'n_predicted_entities': 345, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 35: Train Loss: 0.0031, Val Loss: 0.2854\n",
      "{'precision': 40.79320113314448, 'recall': 26.81564245810056, 'f1': 32.359550561797754, 'n_predicted_entities': 353, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 36: Train Loss: 0.0029, Val Loss: 0.2858\n",
      "{'precision': 40.33613445378151, 'recall': 26.81564245810056, 'f1': 32.214765100671144, 'n_predicted_entities': 357, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 37: Train Loss: 0.0026, Val Loss: 0.2907\n",
      "{'precision': 40.61624649859944, 'recall': 27.001862197392924, 'f1': 32.438478747203575, 'n_predicted_entities': 357, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 38: Train Loss: 0.0024, Val Loss: 0.2991\n",
      "{'precision': 41.5929203539823, 'recall': 26.256983240223462, 'f1': 32.19178082191781, 'n_predicted_entities': 339, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 39: Train Loss: 0.0022, Val Loss: 0.2950\n",
      "{'precision': 38.950276243093924, 'recall': 26.256983240223462, 'f1': 31.368186874304783, 'n_predicted_entities': 362, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 40: Train Loss: 0.0021, Val Loss: 0.2988\n",
      "{'precision': 42.182890855457224, 'recall': 26.629422718808193, 'f1': 32.64840182648401, 'n_predicted_entities': 339, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 41: Train Loss: 0.0019, Val Loss: 0.2998\n",
      "{'precision': 38.44086021505376, 'recall': 26.629422718808193, 'f1': 31.46314631463146, 'n_predicted_entities': 372, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 42: Train Loss: 0.0017, Val Loss: 0.2923\n",
      "{'precision': 38.38120104438642, 'recall': 27.374301675977655, 'f1': 31.956521739130437, 'n_predicted_entities': 383, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 43: Train Loss: 0.0016, Val Loss: 0.3027\n",
      "{'precision': 37.096774193548384, 'recall': 25.69832402234637, 'f1': 30.363036303630363, 'n_predicted_entities': 372, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 44: Train Loss: 0.0016, Val Loss: 0.3101\n",
      "{'precision': 39.60674157303371, 'recall': 26.256983240223462, 'f1': 31.57894736842105, 'n_predicted_entities': 356, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 45: Train Loss: 0.0017, Val Loss: 0.3037\n",
      "{'precision': 40.389972144846794, 'recall': 27.001862197392924, 'f1': 32.36607142857142, 'n_predicted_entities': 359, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 46: Train Loss: 0.0015, Val Loss: 0.3140\n",
      "{'precision': 42.363112391930834, 'recall': 27.374301675977655, 'f1': 33.2579185520362, 'n_predicted_entities': 347, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 47: Train Loss: 0.0015, Val Loss: 0.2949\n",
      "{'precision': 39.84575835475579, 'recall': 28.864059590316575, 'f1': 33.477321814254864, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 48: Train Loss: 0.0017, Val Loss: 0.3134\n",
      "{'precision': 40.285714285714285, 'recall': 26.256983240223462, 'f1': 31.792559188275085, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 49: Train Loss: 0.0017, Val Loss: 0.2951\n",
      "{'precision': 38.3419689119171, 'recall': 27.560521415270017, 'f1': 32.06933911159263, 'n_predicted_entities': 386, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 50: Train Loss: 0.0015, Val Loss: 0.3045\n",
      "{'precision': 41.32231404958678, 'recall': 27.932960893854748, 'f1': 33.333333333333336, 'n_predicted_entities': 363, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 51: Train Loss: 0.0013, Val Loss: 0.3026\n",
      "{'precision': 39.74025974025974, 'recall': 28.491620111731844, 'f1': 33.18872017353579, 'n_predicted_entities': 385, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 52: Train Loss: 0.0012, Val Loss: 0.3207\n",
      "{'precision': 41.1764705882353, 'recall': 26.070763500931097, 'f1': 31.92702394526796, 'n_predicted_entities': 340, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 53: Train Loss: 0.0012, Val Loss: 0.3071\n",
      "{'precision': 37.01799485861183, 'recall': 26.81564245810056, 'f1': 31.101511879049678, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 54: Train Loss: 0.0012, Val Loss: 0.3118\n",
      "{'precision': 41.20879120879121, 'recall': 27.932960893854748, 'f1': 33.29633740288568, 'n_predicted_entities': 364, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 55: Train Loss: 0.0010, Val Loss: 0.3121\n",
      "{'precision': 39.425587467362924, 'recall': 28.119180633147113, 'f1': 32.82608695652174, 'n_predicted_entities': 383, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 56: Train Loss: 0.0010, Val Loss: 0.3111\n",
      "{'precision': 38.717948717948715, 'recall': 28.119180633147113, 'f1': 32.57820927723841, 'n_predicted_entities': 390, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 57: Train Loss: 0.0009, Val Loss: 0.3194\n",
      "{'precision': 40.99722991689751, 'recall': 27.560521415270017, 'f1': 32.962138084632514, 'n_predicted_entities': 361, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 58: Train Loss: 0.0008, Val Loss: 0.3226\n",
      "{'precision': 40.7202216066482, 'recall': 27.374301675977655, 'f1': 32.73942093541203, 'n_predicted_entities': 361, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 59: Train Loss: 0.0009, Val Loss: 0.3323\n",
      "{'precision': 42.433234421364986, 'recall': 26.629422718808193, 'f1': 32.723112128146454, 'n_predicted_entities': 337, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 60: Train Loss: 0.0008, Val Loss: 0.3196\n",
      "{'precision': 40.21447721179624, 'recall': 27.932960893854748, 'f1': 32.967032967032964, 'n_predicted_entities': 373, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 61: Train Loss: 0.0008, Val Loss: 0.3318\n",
      "{'precision': 39.8876404494382, 'recall': 26.443202979515828, 'f1': 31.802911534154536, 'n_predicted_entities': 356, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 62: Train Loss: 0.0009, Val Loss: 0.3135\n",
      "{'precision': 38.9873417721519, 'recall': 28.67783985102421, 'f1': 33.047210300429185, 'n_predicted_entities': 395, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 63: Train Loss: 0.0009, Val Loss: 0.3271\n",
      "{'precision': 41.14441416893733, 'recall': 28.119180633147113, 'f1': 33.407079646017706, 'n_predicted_entities': 367, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 64: Train Loss: 0.0009, Val Loss: 0.3229\n",
      "{'precision': 38.65979381443299, 'recall': 27.932960893854748, 'f1': 32.43243243243243, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 65: Train Loss: 0.0007, Val Loss: 0.3226\n",
      "{'precision': 40.266666666666666, 'recall': 28.119180633147113, 'f1': 33.1140350877193, 'n_predicted_entities': 375, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 66: Train Loss: 0.0007, Val Loss: 0.3276\n",
      "{'precision': 39.10761154855643, 'recall': 27.746741154562383, 'f1': 32.46187363834422, 'n_predicted_entities': 381, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 67: Train Loss: 0.0007, Val Loss: 0.3419\n",
      "{'precision': 42.73255813953488, 'recall': 27.374301675977655, 'f1': 33.37116912599319, 'n_predicted_entities': 344, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 68: Train Loss: 0.0007, Val Loss: 0.3255\n",
      "{'precision': 38.58267716535433, 'recall': 27.374301675977655, 'f1': 32.02614379084967, 'n_predicted_entities': 381, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 69: Train Loss: 0.0007, Val Loss: 0.3282\n",
      "{'precision': 40.84880636604775, 'recall': 28.67783985102421, 'f1': 33.69803063457331, 'n_predicted_entities': 377, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 70: Train Loss: 0.0007, Val Loss: 0.3256\n",
      "{'precision': 40.78947368421053, 'recall': 28.864059590316575, 'f1': 33.80588876772083, 'n_predicted_entities': 380, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 71: Train Loss: 0.0007, Val Loss: 0.3472\n",
      "{'precision': 46.91358024691358, 'recall': 28.30540037243948, 'f1': 35.307781649245065, 'n_predicted_entities': 324, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 72: Train Loss: 0.0007, Val Loss: 0.3281\n",
      "{'precision': 38.541666666666664, 'recall': 27.560521415270017, 'f1': 32.13897937024973, 'n_predicted_entities': 384, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 73: Train Loss: 0.0007, Val Loss: 0.3315\n",
      "{'precision': 38.04627249357326, 'recall': 27.560521415270017, 'f1': 31.965442764578828, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 74: Train Loss: 0.0006, Val Loss: 0.3293\n",
      "{'precision': 38.96103896103896, 'recall': 27.932960893854748, 'f1': 32.53796095444686, 'n_predicted_entities': 385, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 75: Train Loss: 0.0006, Val Loss: 0.3217\n",
      "{'precision': 38.613861386138616, 'recall': 29.050279329608937, 'f1': 33.156216790648244, 'n_predicted_entities': 404, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 76: Train Loss: 0.0006, Val Loss: 0.3349\n",
      "{'precision': 40.75067024128686, 'recall': 28.30540037243948, 'f1': 33.4065934065934, 'n_predicted_entities': 373, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 77: Train Loss: 0.0006, Val Loss: 0.3297\n",
      "{'precision': 38.61892583120205, 'recall': 28.119180633147113, 'f1': 32.543103448275865, 'n_predicted_entities': 391, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 78: Train Loss: 0.0007, Val Loss: 0.3557\n",
      "{'precision': 43.916913946587535, 'recall': 27.560521415270017, 'f1': 33.86727688787185, 'n_predicted_entities': 337, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 79: Train Loss: 0.0007, Val Loss: 0.3553\n",
      "{'precision': 42.94117647058823, 'recall': 27.18808193668529, 'f1': 33.29532497149373, 'n_predicted_entities': 340, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 80: Train Loss: 0.0007, Val Loss: 0.3280\n",
      "{'precision': 40.40920716112532, 'recall': 29.422718808193668, 'f1': 34.05172413793104, 'n_predicted_entities': 391, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 81: Train Loss: 0.0007, Val Loss: 0.3373\n",
      "{'precision': 40.97035040431267, 'recall': 28.30540037243948, 'f1': 33.480176211453745, 'n_predicted_entities': 371, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 82: Train Loss: 0.0005, Val Loss: 0.3425\n",
      "{'precision': 40.53333333333333, 'recall': 28.30540037243948, 'f1': 33.333333333333336, 'n_predicted_entities': 375, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 83: Train Loss: 0.0005, Val Loss: 0.3568\n",
      "{'precision': 40.17094017094017, 'recall': 26.256983240223462, 'f1': 31.756756756756758, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 84: Train Loss: 0.0005, Val Loss: 0.3380\n",
      "{'precision': 40.26315789473684, 'recall': 28.491620111731844, 'f1': 33.36968375136315, 'n_predicted_entities': 380, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 85: Train Loss: 0.0005, Val Loss: 0.3385\n",
      "{'precision': 40.31413612565445, 'recall': 28.67783985102421, 'f1': 33.51468988030468, 'n_predicted_entities': 382, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 86: Train Loss: 0.0005, Val Loss: 0.3395\n",
      "{'precision': 42.16216216216216, 'recall': 29.050279329608937, 'f1': 34.399117971334064, 'n_predicted_entities': 370, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 87: Train Loss: 0.0006, Val Loss: 0.3488\n",
      "{'precision': 39.62765957446808, 'recall': 27.746741154562383, 'f1': 32.63964950711939, 'n_predicted_entities': 376, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 88: Train Loss: 0.0006, Val Loss: 0.3409\n",
      "{'precision': 43.360433604336045, 'recall': 29.7951582867784, 'f1': 35.32008830022075, 'n_predicted_entities': 369, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 89: Train Loss: 0.0005, Val Loss: 0.3374\n",
      "{'precision': 41.02564102564103, 'recall': 29.7951582867784, 'f1': 34.519956850053944, 'n_predicted_entities': 390, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 90: Train Loss: 0.0006, Val Loss: 0.3466\n",
      "{'precision': 42.26519337016575, 'recall': 28.491620111731844, 'f1': 34.03781979977753, 'n_predicted_entities': 362, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 91: Train Loss: 0.0006, Val Loss: 0.3442\n",
      "{'precision': 42.175066312997345, 'recall': 29.608938547486034, 'f1': 34.79212253829321, 'n_predicted_entities': 377, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 92: Train Loss: 0.0006, Val Loss: 0.3347\n",
      "{'precision': 39.59390862944162, 'recall': 29.050279329608937, 'f1': 33.51235230934479, 'n_predicted_entities': 394, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 93: Train Loss: 0.0005, Val Loss: 0.3349\n",
      "{'precision': 37.83783783783784, 'recall': 28.67783985102421, 'f1': 32.62711864406781, 'n_predicted_entities': 407, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 94: Train Loss: 0.0008, Val Loss: 0.3280\n",
      "{'precision': 36.407766990291265, 'recall': 27.932960893854748, 'f1': 31.61222339304531, 'n_predicted_entities': 412, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 95: Train Loss: 0.0011, Val Loss: 0.3417\n",
      "{'precision': 45.19774011299435, 'recall': 29.7951582867784, 'f1': 35.91470258136925, 'n_predicted_entities': 354, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 96: Train Loss: 0.0015, Val Loss: 0.3225\n",
      "{'precision': 40.0, 'recall': 29.7951582867784, 'f1': 34.15154749199573, 'n_predicted_entities': 400, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 97: Train Loss: 0.0014, Val Loss: 0.3189\n",
      "{'precision': 40.966921119592875, 'recall': 29.98137802607076, 'f1': 34.62365591397849, 'n_predicted_entities': 393, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 98: Train Loss: 0.0018, Val Loss: 0.3435\n",
      "{'precision': 45.76802507836991, 'recall': 27.18808193668529, 'f1': 34.11214953271028, 'n_predicted_entities': 319, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 99: Train Loss: 0.0019, Val Loss: 0.3103\n",
      "{'precision': 44.77211796246649, 'recall': 31.098696461824954, 'f1': 36.7032967032967, 'n_predicted_entities': 373, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 100: Train Loss: 0.0017, Val Loss: 0.3162\n",
      "{'precision': 47.875354107648725, 'recall': 31.471135940409685, 'f1': 37.97752808988764, 'n_predicted_entities': 353, 'n_true_entities': 537}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = BiLSTMModel(\n",
    "        vocabulary_size=len(token_to_idx),\n",
    "        tag_space_size=len(tag_to_idx),\n",
    "        pad_token_idx=token_to_idx['<PAD>'],\n",
    "        embedding_dim=150,\n",
    "        lstm_hidden_size=200,\n",
    "        dropout_zeroed_probability=0.5,\n",
    "        max_norm=1.0).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx['<PAD>'])\n",
    "\n",
    "num_epochs=100\n",
    "model_trained = train(num_epochs, model, criterion, optimizer, train_loader, val_loader, evaluator, max_grad_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 93.98178991783256, 'recall': 94.27489418578747, 'f1': 94.12811387900355, 'n_predicted_entities': 4503, 'n_true_entities': 4489}\n",
      "{'precision': 47.875354107648725, 'recall': 31.471135940409685, 'f1': 37.97752808988764, 'n_predicted_entities': 353, 'n_true_entities': 537}\n",
      "{'precision': 51.64556962025316, 'recall': 33.77483443708609, 'f1': 40.84084084084084, 'n_predicted_entities': 395, 'n_true_entities': 604}\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.eval_conll(model_trained, train_loader))\n",
    "print(evaluator.eval_conll(model_trained, val_loader))\n",
    "print(evaluator.eval_conll(model_trained, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4687, Val Loss: 0.1793\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 2: Train Loss: 0.1475, Val Loss: 0.1670\n",
      "{'precision': 12.385321100917432, 'recall': 5.027932960893855, 'f1': 7.152317880794702, 'n_predicted_entities': 218, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 3: Train Loss: 0.1022, Val Loss: 0.1700\n",
      "{'precision': 22.22222222222222, 'recall': 14.525139664804469, 'f1': 17.567567567567565, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 4: Train Loss: 0.0734, Val Loss: 0.1679\n",
      "{'precision': 40.3125, 'recall': 24.022346368715084, 'f1': 30.10501750291715, 'n_predicted_entities': 320, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 5: Train Loss: 0.0489, Val Loss: 0.1781\n",
      "{'precision': 44.606413994169095, 'recall': 28.491620111731844, 'f1': 34.77272727272727, 'n_predicted_entities': 343, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 6: Train Loss: 0.0306, Val Loss: 0.1758\n",
      "{'precision': 45.572916666666664, 'recall': 32.588454376163874, 'f1': 38.00217155266015, 'n_predicted_entities': 384, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 7: Train Loss: 0.0188, Val Loss: 0.1804\n",
      "{'precision': 43.05882352941177, 'recall': 34.07821229050279, 'f1': 38.04573804573804, 'n_predicted_entities': 425, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 8: Train Loss: 0.0112, Val Loss: 0.1860\n",
      "{'precision': 39.33054393305439, 'recall': 35.00931098696462, 'f1': 37.044334975369466, 'n_predicted_entities': 478, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 9: Train Loss: 0.0072, Val Loss: 0.1969\n",
      "{'precision': 43.925233644859816, 'recall': 35.00931098696462, 'f1': 38.96373056994819, 'n_predicted_entities': 428, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 10: Train Loss: 0.0046, Val Loss: 0.2192\n",
      "{'precision': 46.64879356568365, 'recall': 32.402234636871505, 'f1': 38.241758241758234, 'n_predicted_entities': 373, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 11: Train Loss: 0.0029, Val Loss: 0.2307\n",
      "{'precision': 47.527472527472526, 'recall': 32.21601489757914, 'f1': 38.40177580466149, 'n_predicted_entities': 364, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 12: Train Loss: 0.0021, Val Loss: 0.2230\n",
      "{'precision': 46.61654135338346, 'recall': 34.63687150837989, 'f1': 39.743589743589745, 'n_predicted_entities': 399, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 13: Train Loss: 0.0016, Val Loss: 0.2321\n",
      "{'precision': 46.42857142857143, 'recall': 33.89199255121043, 'f1': 39.181916038751346, 'n_predicted_entities': 392, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 14: Train Loss: 0.0012, Val Loss: 0.2466\n",
      "{'precision': 48.89502762430939, 'recall': 32.960893854748605, 'f1': 39.377085650723025, 'n_predicted_entities': 362, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 15: Train Loss: 0.0012, Val Loss: 0.2423\n",
      "{'precision': 46.373056994818654, 'recall': 33.333333333333336, 'f1': 38.78656554712892, 'n_predicted_entities': 386, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 16: Train Loss: 0.0009, Val Loss: 0.2449\n",
      "{'precision': 47.354497354497354, 'recall': 33.333333333333336, 'f1': 39.125683060109296, 'n_predicted_entities': 378, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 17: Train Loss: 0.0009, Val Loss: 0.2450\n",
      "{'precision': 46.954314720812185, 'recall': 34.45065176908752, 'f1': 39.7422126745435, 'n_predicted_entities': 394, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 18: Train Loss: 0.0008, Val Loss: 0.2524\n",
      "{'precision': 48.11827956989247, 'recall': 33.333333333333336, 'f1': 39.38393839383938, 'n_predicted_entities': 372, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 19: Train Loss: 0.0007, Val Loss: 0.2441\n",
      "{'precision': 45.20884520884521, 'recall': 34.26443202979516, 'f1': 38.983050847457626, 'n_predicted_entities': 407, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 20: Train Loss: 0.0007, Val Loss: 0.2469\n",
      "{'precision': 47.48743718592965, 'recall': 35.19553072625698, 'f1': 40.42780748663102, 'n_predicted_entities': 398, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 21: Train Loss: 0.0007, Val Loss: 0.2441\n",
      "{'precision': 44.01913875598086, 'recall': 34.26443202979516, 'f1': 38.53403141361257, 'n_predicted_entities': 418, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 22: Train Loss: 0.0006, Val Loss: 0.2548\n",
      "{'precision': 48.1283422459893, 'recall': 33.5195530726257, 'f1': 39.51701427003293, 'n_predicted_entities': 374, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 23: Train Loss: 0.0006, Val Loss: 0.2499\n",
      "{'precision': 46.231155778894475, 'recall': 34.26443202979516, 'f1': 39.35828877005348, 'n_predicted_entities': 398, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 24: Train Loss: 0.0006, Val Loss: 0.2485\n",
      "{'precision': 44.90291262135922, 'recall': 34.45065176908752, 'f1': 38.98840885142255, 'n_predicted_entities': 412, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 25: Train Loss: 0.0006, Val Loss: 0.2522\n",
      "{'precision': 46.92307692307692, 'recall': 34.07821229050279, 'f1': 39.482200647249186, 'n_predicted_entities': 390, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 26: Train Loss: 0.0005, Val Loss: 0.2514\n",
      "{'precision': 46.59949622166247, 'recall': 34.45065176908752, 'f1': 39.614561027837254, 'n_predicted_entities': 397, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 27: Train Loss: 0.0006, Val Loss: 0.2536\n",
      "{'precision': 47.07379134860051, 'recall': 34.45065176908752, 'f1': 39.784946236559136, 'n_predicted_entities': 393, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 28: Train Loss: 0.0005, Val Loss: 0.2588\n",
      "{'precision': 48.138297872340424, 'recall': 33.70577281191807, 'f1': 39.64950711938664, 'n_predicted_entities': 376, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 29: Train Loss: 0.0005, Val Loss: 0.2549\n",
      "{'precision': 47.05882352941177, 'recall': 34.26443202979516, 'f1': 39.6551724137931, 'n_predicted_entities': 391, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 30: Train Loss: 0.0005, Val Loss: 0.2558\n",
      "{'precision': 46.93877551020408, 'recall': 34.26443202979516, 'f1': 39.61248654467169, 'n_predicted_entities': 392, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 31: Train Loss: 0.0005, Val Loss: 0.2577\n",
      "{'precision': 47.54521963824289, 'recall': 34.26443202979516, 'f1': 39.82683982683983, 'n_predicted_entities': 387, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 32: Train Loss: 0.0005, Val Loss: 0.2558\n",
      "{'precision': 46.96969696969697, 'recall': 34.63687150837989, 'f1': 39.871382636655945, 'n_predicted_entities': 396, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 33: Train Loss: 0.0005, Val Loss: 0.2561\n",
      "{'precision': 45.6575682382134, 'recall': 34.26443202979516, 'f1': 39.148936170212764, 'n_predicted_entities': 403, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 34: Train Loss: 0.0005, Val Loss: 0.2590\n",
      "{'precision': 47.78067885117493, 'recall': 34.07821229050279, 'f1': 39.782608695652165, 'n_predicted_entities': 383, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 35: Train Loss: 0.0005, Val Loss: 0.2576\n",
      "{'precision': 46.93877551020408, 'recall': 34.26443202979516, 'f1': 39.61248654467169, 'n_predicted_entities': 392, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 36: Train Loss: 0.0004, Val Loss: 0.2608\n",
      "{'precision': 47.90575916230367, 'recall': 34.07821229050279, 'f1': 39.82589771490751, 'n_predicted_entities': 382, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 37: Train Loss: 0.0004, Val Loss: 0.2595\n",
      "{'precision': 47.54521963824289, 'recall': 34.26443202979516, 'f1': 39.82683982683983, 'n_predicted_entities': 387, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 38: Train Loss: 0.0004, Val Loss: 0.2591\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 39: Train Loss: 0.0004, Val Loss: 0.2592\n",
      "{'precision': 47.54521963824289, 'recall': 34.26443202979516, 'f1': 39.82683982683983, 'n_predicted_entities': 387, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 40: Train Loss: 0.0004, Val Loss: 0.2609\n",
      "{'precision': 47.532467532467535, 'recall': 34.07821229050279, 'f1': 39.69631236442516, 'n_predicted_entities': 385, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 41: Train Loss: 0.0004, Val Loss: 0.2607\n",
      "{'precision': 47.54521963824289, 'recall': 34.26443202979516, 'f1': 39.82683982683983, 'n_predicted_entities': 387, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 42: Train Loss: 0.0004, Val Loss: 0.2594\n",
      "{'precision': 46.7005076142132, 'recall': 34.26443202979516, 'f1': 39.527389903329755, 'n_predicted_entities': 394, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 43: Train Loss: 0.0004, Val Loss: 0.2605\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 44: Train Loss: 0.0004, Val Loss: 0.2612\n",
      "{'precision': 47.66839378238342, 'recall': 34.26443202979516, 'f1': 39.86998916576381, 'n_predicted_entities': 386, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 45: Train Loss: 0.0004, Val Loss: 0.2611\n",
      "{'precision': 47.54521963824289, 'recall': 34.26443202979516, 'f1': 39.82683982683983, 'n_predicted_entities': 387, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 46: Train Loss: 0.0004, Val Loss: 0.2604\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 47: Train Loss: 0.0004, Val Loss: 0.2603\n",
      "{'precision': 47.05882352941177, 'recall': 34.26443202979516, 'f1': 39.6551724137931, 'n_predicted_entities': 391, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 48: Train Loss: 0.0004, Val Loss: 0.2617\n",
      "{'precision': 47.66839378238342, 'recall': 34.26443202979516, 'f1': 39.86998916576381, 'n_predicted_entities': 386, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 49: Train Loss: 0.0004, Val Loss: 0.2617\n",
      "{'precision': 47.66839378238342, 'recall': 34.26443202979516, 'f1': 39.86998916576381, 'n_predicted_entities': 386, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 50: Train Loss: 0.0004, Val Loss: 0.2607\n",
      "{'precision': 47.19387755102041, 'recall': 34.45065176908752, 'f1': 39.82777179763186, 'n_predicted_entities': 392, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 51: Train Loss: 0.0004, Val Loss: 0.2608\n",
      "{'precision': 47.19387755102041, 'recall': 34.45065176908752, 'f1': 39.82777179763186, 'n_predicted_entities': 392, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 52: Train Loss: 0.0004, Val Loss: 0.2614\n",
      "{'precision': 47.17948717948718, 'recall': 34.26443202979516, 'f1': 39.69795037756203, 'n_predicted_entities': 390, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 53: Train Loss: 0.0004, Val Loss: 0.2611\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 54: Train Loss: 0.0004, Val Loss: 0.2618\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 55: Train Loss: 0.0004, Val Loss: 0.2615\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 56: Train Loss: 0.0004, Val Loss: 0.2614\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 57: Train Loss: 0.0004, Val Loss: 0.2616\n",
      "{'precision': 47.17948717948718, 'recall': 34.26443202979516, 'f1': 39.69795037756203, 'n_predicted_entities': 390, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 58: Train Loss: 0.0004, Val Loss: 0.2615\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 59: Train Loss: 0.0004, Val Loss: 0.2619\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 60: Train Loss: 0.0004, Val Loss: 0.2620\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 61: Train Loss: 0.0004, Val Loss: 0.2619\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 62: Train Loss: 0.0004, Val Loss: 0.2619\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 63: Train Loss: 0.0004, Val Loss: 0.2619\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 64: Train Loss: 0.0004, Val Loss: 0.2620\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 65: Train Loss: 0.0004, Val Loss: 0.2617\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 66: Train Loss: 0.0004, Val Loss: 0.2620\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 67: Train Loss: 0.0004, Val Loss: 0.2619\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 68: Train Loss: 0.0004, Val Loss: 0.2621\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 69: Train Loss: 0.0004, Val Loss: 0.2622\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 70: Train Loss: 0.0004, Val Loss: 0.2621\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 71: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 72: Train Loss: 0.0004, Val Loss: 0.2622\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 73: Train Loss: 0.0004, Val Loss: 0.2621\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 74: Train Loss: 0.0004, Val Loss: 0.2622\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 75: Train Loss: 0.0004, Val Loss: 0.2622\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 76: Train Loss: 0.0004, Val Loss: 0.2622\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 77: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 78: Train Loss: 0.0004, Val Loss: 0.2622\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 79: Train Loss: 0.0004, Val Loss: 0.2622\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 80: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 81: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 82: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 83: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.422680412371136, 'recall': 34.26443202979516, 'f1': 39.78378378378378, 'n_predicted_entities': 388, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 84: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 85: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 86: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 87: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 88: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 89: Train Loss: 0.0004, Val Loss: 0.2624\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 90: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 91: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 92: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 93: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 94: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 95: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 96: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 97: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 98: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 99: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 100: Train Loss: 0.0004, Val Loss: 0.2623\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = BiLSTMModel(\n",
    "        vocabulary_size=len(token_to_idx),\n",
    "        tag_space_size=len(tag_to_idx),\n",
    "        pad_token_idx=token_to_idx['<PAD>'],\n",
    "        embedding_dim=150,\n",
    "        lstm_hidden_size=200,\n",
    "        dropout_zeroed_probability=0.5,\n",
    "        max_norm=1.0).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1/1.1)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx['<PAD>'])\n",
    "\n",
    "num_epochs=100\n",
    "model_trained = train(num_epochs, model, criterion, optimizer, train_loader, val_loader, evaluator, max_grad_norm=1.0, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 94.9508489722967, 'recall': 94.67587435954556, 'f1': 94.81316229782489, 'n_predicted_entities': 4476, 'n_true_entities': 4489}\n",
      "{'precision': 47.30077120822622, 'recall': 34.26443202979516, 'f1': 39.740820734341256, 'n_predicted_entities': 389, 'n_true_entities': 537}\n",
      "{'precision': 49.53917050691244, 'recall': 35.59602649006622, 'f1': 41.42581888246628, 'n_predicted_entities': 434, 'n_true_entities': 604}\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.eval_conll(model_trained, train_loader))\n",
    "print(evaluator.eval_conll(model_trained, val_loader))\n",
    "print(evaluator.eval_conll(model_trained, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained, \"model_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4805, Val Loss: 0.1985\n",
      "{'precision': 0, 'recall': 0.0, 'f1': 0, 'n_predicted_entities': 0, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 2: Train Loss: 0.1543, Val Loss: 0.1638\n",
      "{'precision': 9.620991253644315, 'recall': 6.145251396648045, 'f1': 7.500000000000001, 'n_predicted_entities': 343, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 3: Train Loss: 0.1005, Val Loss: 0.1667\n",
      "{'precision': 19.672131147540984, 'recall': 13.40782122905028, 'f1': 15.946843853820598, 'n_predicted_entities': 366, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 4: Train Loss: 0.0747, Val Loss: 0.1676\n",
      "{'precision': 32.25806451612903, 'recall': 20.484171322160147, 'f1': 25.056947608200456, 'n_predicted_entities': 341, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 5: Train Loss: 0.0496, Val Loss: 0.1797\n",
      "{'precision': 48.12499999999999, 'recall': 28.67783985102421, 'f1': 35.93932322053676, 'n_predicted_entities': 320, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 6: Train Loss: 0.0298, Val Loss: 0.1735\n",
      "{'precision': 43.17617866004963, 'recall': 32.402234636871505, 'f1': 37.02127659574468, 'n_predicted_entities': 403, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 7: Train Loss: 0.0186, Val Loss: 0.1762\n",
      "{'precision': 43.34140435835351, 'recall': 33.333333333333336, 'f1': 37.684210526315795, 'n_predicted_entities': 413, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 8: Train Loss: 0.0107, Val Loss: 0.1898\n",
      "{'precision': 49.44444444444444, 'recall': 33.14711359404097, 'f1': 39.68784838350056, 'n_predicted_entities': 360, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 9: Train Loss: 0.0062, Val Loss: 0.2107\n",
      "{'precision': 51.06382978723404, 'recall': 31.28491620111732, 'f1': 38.79907621247113, 'n_predicted_entities': 329, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 10: Train Loss: 0.0040, Val Loss: 0.2212\n",
      "{'precision': 51.09034267912772, 'recall': 30.540037243947857, 'f1': 38.22843822843823, 'n_predicted_entities': 321, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 11: Train Loss: 0.0027, Val Loss: 0.2116\n",
      "{'precision': 50.96952908587257, 'recall': 34.26443202979516, 'f1': 40.97995545657015, 'n_predicted_entities': 361, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 12: Train Loss: 0.0020, Val Loss: 0.2176\n",
      "{'precision': 51.4367816091954, 'recall': 33.333333333333336, 'f1': 40.451977401129945, 'n_predicted_entities': 348, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 13: Train Loss: 0.0016, Val Loss: 0.2209\n",
      "{'precision': 49.570200573065904, 'recall': 32.21601489757914, 'f1': 39.05191873589165, 'n_predicted_entities': 349, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 14: Train Loss: 0.0013, Val Loss: 0.2163\n",
      "{'precision': 49.05149051490515, 'recall': 33.70577281191807, 'f1': 39.95584988962472, 'n_predicted_entities': 369, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 15: Train Loss: 0.0012, Val Loss: 0.2200\n",
      "{'precision': 49.189189189189186, 'recall': 33.89199255121043, 'f1': 40.132304299889746, 'n_predicted_entities': 370, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 16: Train Loss: 0.0011, Val Loss: 0.2200\n",
      "{'precision': 50.41782729805014, 'recall': 33.70577281191807, 'f1': 40.401785714285715, 'n_predicted_entities': 359, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 17: Train Loss: 0.0010, Val Loss: 0.2352\n",
      "{'precision': 51.80722891566265, 'recall': 32.02979515828678, 'f1': 39.58573072497123, 'n_predicted_entities': 332, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 18: Train Loss: 0.0009, Val Loss: 0.2242\n",
      "{'precision': 48.50948509485095, 'recall': 33.333333333333336, 'f1': 39.51434878587197, 'n_predicted_entities': 369, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 19: Train Loss: 0.0009, Val Loss: 0.2301\n",
      "{'precision': 52.478134110787174, 'recall': 33.5195530726257, 'f1': 40.909090909090914, 'n_predicted_entities': 343, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 20: Train Loss: 0.0009, Val Loss: 0.2323\n",
      "{'precision': 51.461988304093566, 'recall': 32.774674115456236, 'f1': 40.045506257110354, 'n_predicted_entities': 342, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 21: Train Loss: 0.0008, Val Loss: 0.2284\n",
      "{'precision': 49.586776859504134, 'recall': 33.5195530726257, 'f1': 40.00000000000001, 'n_predicted_entities': 363, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 22: Train Loss: 0.0007, Val Loss: 0.2287\n",
      "{'precision': 50.70422535211268, 'recall': 33.5195530726257, 'f1': 40.35874439461883, 'n_predicted_entities': 355, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 23: Train Loss: 0.0007, Val Loss: 0.2252\n",
      "{'precision': 48.648648648648646, 'recall': 33.5195530726257, 'f1': 39.69128996692392, 'n_predicted_entities': 370, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 24: Train Loss: 0.0007, Val Loss: 0.2382\n",
      "{'precision': 52.083333333333336, 'recall': 32.588454376163874, 'f1': 40.09163802978236, 'n_predicted_entities': 336, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 25: Train Loss: 0.0007, Val Loss: 0.2346\n",
      "{'precision': 51.75438596491228, 'recall': 32.960893854748605, 'f1': 40.273037542662124, 'n_predicted_entities': 342, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 26: Train Loss: 0.0006, Val Loss: 0.2300\n",
      "{'precision': 50.28089887640449, 'recall': 33.333333333333336, 'f1': 40.089585666293395, 'n_predicted_entities': 356, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 27: Train Loss: 0.0006, Val Loss: 0.2370\n",
      "{'precision': 50.733137829912025, 'recall': 32.21601489757914, 'f1': 39.40774487471526, 'n_predicted_entities': 341, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 28: Train Loss: 0.0006, Val Loss: 0.2338\n",
      "{'precision': 50.578034682080926, 'recall': 32.588454376163874, 'f1': 39.637599093997736, 'n_predicted_entities': 346, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 29: Train Loss: 0.0006, Val Loss: 0.2308\n",
      "{'precision': 50.279329608938546, 'recall': 33.5195530726257, 'f1': 40.22346368715084, 'n_predicted_entities': 358, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 30: Train Loss: 0.0006, Val Loss: 0.2334\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 31: Train Loss: 0.0006, Val Loss: 0.2385\n",
      "{'precision': 50.88235294117647, 'recall': 32.21601489757914, 'f1': 39.452679589509685, 'n_predicted_entities': 340, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 32: Train Loss: 0.0006, Val Loss: 0.2304\n",
      "{'precision': 48.90710382513661, 'recall': 33.333333333333336, 'f1': 39.64562569213732, 'n_predicted_entities': 366, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 33: Train Loss: 0.0006, Val Loss: 0.2341\n",
      "{'precision': 50.997150997150996, 'recall': 33.333333333333336, 'f1': 40.315315315315324, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 34: Train Loss: 0.0005, Val Loss: 0.2302\n",
      "{'precision': 48.90710382513661, 'recall': 33.333333333333336, 'f1': 39.64562569213732, 'n_predicted_entities': 366, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 35: Train Loss: 0.0005, Val Loss: 0.2351\n",
      "{'precision': 50.427350427350426, 'recall': 32.960893854748605, 'f1': 39.86486486486486, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 36: Train Loss: 0.0006, Val Loss: 0.2368\n",
      "{'precision': 50.2906976744186, 'recall': 32.21601489757914, 'f1': 39.27355278093076, 'n_predicted_entities': 344, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 37: Train Loss: 0.0005, Val Loss: 0.2315\n",
      "{'precision': 49.72375690607735, 'recall': 33.5195530726257, 'f1': 40.04449388209121, 'n_predicted_entities': 362, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 38: Train Loss: 0.0005, Val Loss: 0.2368\n",
      "{'precision': 51.00864553314121, 'recall': 32.960893854748605, 'f1': 40.04524886877828, 'n_predicted_entities': 347, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 39: Train Loss: 0.0005, Val Loss: 0.2342\n",
      "{'precision': 50.14005602240896, 'recall': 33.333333333333336, 'f1': 40.044742729306485, 'n_predicted_entities': 357, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 40: Train Loss: 0.0005, Val Loss: 0.2381\n",
      "{'precision': 50.58139534883721, 'recall': 32.402234636871505, 'f1': 39.500567536889896, 'n_predicted_entities': 344, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 41: Train Loss: 0.0005, Val Loss: 0.2351\n",
      "{'precision': 50.56497175141243, 'recall': 33.333333333333336, 'f1': 40.17957351290684, 'n_predicted_entities': 354, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 42: Train Loss: 0.0005, Val Loss: 0.2363\n",
      "{'precision': 50.86206896551724, 'recall': 32.960893854748605, 'f1': 40.0, 'n_predicted_entities': 348, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 43: Train Loss: 0.0005, Val Loss: 0.2349\n",
      "{'precision': 50.70821529745042, 'recall': 33.333333333333336, 'f1': 40.2247191011236, 'n_predicted_entities': 353, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 44: Train Loss: 0.0005, Val Loss: 0.2366\n",
      "{'precision': 50.141643059490086, 'recall': 32.960893854748605, 'f1': 39.7752808988764, 'n_predicted_entities': 353, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 45: Train Loss: 0.0005, Val Loss: 0.2359\n",
      "{'precision': 50.0, 'recall': 32.960893854748605, 'f1': 39.73063973063973, 'n_predicted_entities': 354, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 46: Train Loss: 0.0005, Val Loss: 0.2360\n",
      "{'precision': 50.0, 'recall': 32.774674115456236, 'f1': 39.595050618672666, 'n_predicted_entities': 352, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 47: Train Loss: 0.0005, Val Loss: 0.2381\n",
      "{'precision': 50.72463768115942, 'recall': 32.588454376163874, 'f1': 39.682539682539684, 'n_predicted_entities': 345, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 48: Train Loss: 0.0005, Val Loss: 0.2372\n",
      "{'precision': 50.429799426934096, 'recall': 32.774674115456236, 'f1': 39.729119638826184, 'n_predicted_entities': 349, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 49: Train Loss: 0.0005, Val Loss: 0.2362\n",
      "{'precision': 50.282485875706215, 'recall': 33.14711359404097, 'f1': 39.95510662177328, 'n_predicted_entities': 354, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 50: Train Loss: 0.0005, Val Loss: 0.2366\n",
      "{'precision': 50.57142857142857, 'recall': 32.960893854748605, 'f1': 39.9098083427283, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 51: Train Loss: 0.0005, Val Loss: 0.2363\n",
      "{'precision': 50.42492917847026, 'recall': 33.14711359404097, 'f1': 40.0, 'n_predicted_entities': 353, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 52: Train Loss: 0.0005, Val Loss: 0.2368\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 53: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.1432664756447, 'recall': 32.588454376163874, 'f1': 39.503386004514674, 'n_predicted_entities': 349, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 54: Train Loss: 0.0005, Val Loss: 0.2374\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 55: Train Loss: 0.0005, Val Loss: 0.2363\n",
      "{'precision': 50.42492917847026, 'recall': 33.14711359404097, 'f1': 40.0, 'n_predicted_entities': 353, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 56: Train Loss: 0.0005, Val Loss: 0.2371\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 57: Train Loss: 0.0005, Val Loss: 0.2376\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 58: Train Loss: 0.0005, Val Loss: 0.2380\n",
      "{'precision': 50.1432664756447, 'recall': 32.588454376163874, 'f1': 39.503386004514674, 'n_predicted_entities': 349, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 59: Train Loss: 0.0005, Val Loss: 0.2373\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 60: Train Loss: 0.0005, Val Loss: 0.2370\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 61: Train Loss: 0.0005, Val Loss: 0.2372\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 62: Train Loss: 0.0005, Val Loss: 0.2376\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 63: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 64: Train Loss: 0.0005, Val Loss: 0.2372\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 65: Train Loss: 0.0005, Val Loss: 0.2378\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 66: Train Loss: 0.0005, Val Loss: 0.2378\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 67: Train Loss: 0.0005, Val Loss: 0.2378\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 68: Train Loss: 0.0005, Val Loss: 0.2376\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 69: Train Loss: 0.0005, Val Loss: 0.2376\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 70: Train Loss: 0.0005, Val Loss: 0.2376\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 71: Train Loss: 0.0005, Val Loss: 0.2378\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 72: Train Loss: 0.0005, Val Loss: 0.2377\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 73: Train Loss: 0.0005, Val Loss: 0.2377\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 74: Train Loss: 0.0005, Val Loss: 0.2380\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 75: Train Loss: 0.0005, Val Loss: 0.2377\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 76: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 77: Train Loss: 0.0005, Val Loss: 0.2378\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 78: Train Loss: 0.0005, Val Loss: 0.2376\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 79: Train Loss: 0.0005, Val Loss: 0.2378\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 80: Train Loss: 0.0005, Val Loss: 0.2377\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 81: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 82: Train Loss: 0.0005, Val Loss: 0.2378\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 83: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 84: Train Loss: 0.0005, Val Loss: 0.2380\n",
      "{'precision': 50.285714285714285, 'recall': 32.774674115456236, 'f1': 39.68432919954905, 'n_predicted_entities': 350, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 85: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 86: Train Loss: 0.0005, Val Loss: 0.2378\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 87: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 88: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 89: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 90: Train Loss: 0.0005, Val Loss: 0.2380\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 91: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 92: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 93: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 94: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 95: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 96: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 97: Train Loss: 0.0005, Val Loss: 0.2379\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 98: Train Loss: 0.0005, Val Loss: 0.2380\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 99: Train Loss: 0.0005, Val Loss: 0.2380\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n",
      "Epoch 100: Train Loss: 0.0005, Val Loss: 0.2380\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = BiLSTMModel(\n",
    "        vocabulary_size=len(token_to_idx),\n",
    "        tag_space_size=len(tag_to_idx),\n",
    "        pad_token_idx=token_to_idx['<PAD>'],\n",
    "        embedding_dim=150,\n",
    "        lstm_hidden_size=200,\n",
    "        dropout_zeroed_probability=0.5,\n",
    "        max_norm=1.0).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1/1.1)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx['<PAD>'])\n",
    "\n",
    "num_epochs=100\n",
    "model_trained = train(num_epochs, model, criterion, optimizer, train_loader, val_loader, evaluator, max_grad_norm=1.0, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 94.83798882681565, 'recall': 94.5422143016262, 'f1': 94.68987059348507, 'n_predicted_entities': 4475, 'n_true_entities': 4489}\n",
      "{'precision': 50.142450142450144, 'recall': 32.774674115456236, 'f1': 39.63963963963964, 'n_predicted_entities': 351, 'n_true_entities': 537}\n",
      "{'precision': 49.00221729490022, 'recall': 36.58940397350993, 'f1': 41.89573459715639, 'n_predicted_entities': 451, 'n_true_entities': 604}\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.eval_conll(model_trained, train_loader))\n",
    "print(evaluator.eval_conll(model_trained, val_loader))\n",
    "print(evaluator.eval_conll(model_trained, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained, \"model_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить из экспериментов \"базовая\" модель без всяких трюков показывает довольно плохое качество. Добавление только градиентного клиппинга и ограничения нормы эмбеддингов позволяет увеличить f меру на валидации аж на 10 пунктов, а на тесте на 16. Смена оптимизатора на AdamW и добавление шедулера с уменьшением lr в 1.1 раз каждую эпуху позволяет ещё сильнее улучшить качество работы алгоритма. Настройка регуляризации позволяет слегка улучшить работу алгоритма, уменьшив его переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7Wm4pWysiPU"
   },
   "source": [
    "## Необучаемый пост-процессинг результата (4 балла).\n",
    "\n",
    "Для обучения нейросетевой модели разметки используется поэлементная кросс-энтропия. При использовании на этапе инференса функции $\\arg \\max$ для получения выходной последовательности, мы не можем гарантировать согласованность предсказаний. Для согласованности необходимо вместо $\\arg \\max$ использовать другие функции получения предсказаний.\n",
    "\n",
    "В модели CRF для получения предсказаний используется алгоритм Витерби. Напомним, что модель CRF моделирует вероятность последовательности $y$ при условии $x$ линейной моделью с вектором весов $w \\in \\mathbb{R}^d$, которая после некоторых преобразований записывается следующим образом:\n",
    "$$\n",
    "p(y|x, w) = \\frac{1}{Z(x, w)} \\exp\\left( \\sum_{i=1}^n \\sum_{j = 1}^d w_j f_j(y_{i-1}, y_i, x_i, i) \\right) =  \\frac{1}{Z(x, w)} \\exp\\left( \\sum_{i=1}^n G_{x, i}[y_{i-1}, y_i] \\right)\n",
    "$$\n",
    "\n",
    "Модель необучаемого пост-процессинга **подробно описана** в приложении к заданию. Она сводится к следующим шагам.\n",
    "\n",
    "1. Реализовать модель CRF с двумя признаками:\n",
    "    \n",
    "    * Лог-софтмакс выходов модели (выход, соответствующий $y_i$ тэгу для i-го токена будем обозначать $S_{i,y_i}$)    \n",
    "    \n",
    "    $$\n",
    "    f_1(y_{i-1}, y_i, x_i, i) = S_{i,y_i}\n",
    "    $$\n",
    "    \n",
    "    * Логарифмы вероятностей переходов\n",
    "\n",
    "    $$\n",
    "    f_2(y_{i-1}, y_i, x_i, i) = \\log A[v=y_{i}, u=y_{i-1}] \\mathbb{I}[i > 1] + \\log C[v = y_i] \\mathbb{I}[i = 1], \\quad \\text{где:}\n",
    "    $$\n",
    "\n",
    "    $$A_{vu} = \\frac{\\sum_{y}\\sum_{i=2}^{|y|} \\mathbb{I}[y_{i} = v, y_{i - 1} = u]}{\\sum_{y}\\sum_{i=2}^{|y|} \\mathbb{I}[y_{i-1} = u]}\n",
    "    $$\n",
    "    $$\n",
    "    C_v = \\frac{\\sum_{y}\\mathbb{I}[y_{1} = v]}{\\sum_{y}1}\n",
    "    $$\n",
    "    \n",
    "2. Реализовать процедуру получения оптимальной выходной последовательности, используя алгоритм Витерби\n",
    "\n",
    "3. Подобрать на валидационной выборке веса модели $w_1$ и $w_2$\n",
    "\n",
    "Для исходной модели, дающей на валидационной и тестовой выборке F1 меру 0.408 и 0.46 соответственно, качество после такого пост-процессинга выросло до 0.461 и 0.493. Заметим, что для тестирования модели не нужно переобучать исходную модель. Для более устойчивого поведения модели, используйте сглаживание матрицы $A$ (добавьте перед нормировкой ко всем значениям одинаковое небольшое число).\n",
    "\n",
    "**<font color='red'>Задание. Реализуйте требуемую модель, добейтесь улучшения качества на валидации и тесте, сделайте выводы.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "id": "kCu7KUyKsiPU"
   },
   "outputs": [],
   "source": [
    "class ViterbiPostprocesser:\n",
    "    def __init__(self, model, smoothing=1.0, w=1.0):\n",
    "        \"\"\"\n",
    "        model : torch.nn.Module\n",
    "            Tagging model\n",
    "        smoothing : float, constant in add-k-smoothing\n",
    "        w : feature weight\n",
    "             Use w for first feature weight and (1 - w) for second feature.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.smoothing = smoothing\n",
    "        self.w = w\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"\n",
    "        Fit the model using maximum likelihood method.\n",
    "\n",
    "        dataset: torch.dataset\n",
    "            One element if pair (sentence, tags)\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        tag_cnt, tag_prev_next = {}, {}\n",
    "        for sentence, tags in dataset:\n",
    "            prev_tag = None\n",
    "            for tag in tags:\n",
    "                tag = tag.item()\n",
    "                if tag not in tag_cnt:\n",
    "                    tag_cnt[tag] = 0\n",
    "                tag_cnt[tag] += 1\n",
    "                \n",
    "                if (prev_tag, tag) not in tag_prev_next:\n",
    "                    tag_prev_next[(prev_tag, tag)] = 0\n",
    "                tag_prev_next[(prev_tag, tag)] += 1\n",
    "                prev_tag = tag\n",
    "\n",
    "        self.A = torch.zeros((len(tag_cnt), len(tag_cnt)))\n",
    "        self.C = torch.zeros(len(tag_cnt))\n",
    "        tag_to_idx = {tag: i for i, tag in enumerate(tag_cnt.keys())}\n",
    "        \n",
    "        for (prev_tag, tag), count in tag_prev_next.items():\n",
    "            if prev_tag:\n",
    "                self.A[tag_to_idx[prev_tag], tag_to_idx[tag]] = count\n",
    "            else:\n",
    "                self.C[tag_to_idx[tag]] = count\n",
    "        \n",
    "        self.A = torch.log((self.A + 1e-7)) - torch.log((self.A.sum(axis=1, keepdims=True) + 1e-7))\n",
    "        self.C = torch.log((self.C + 1e-7)) - torch.log((self.C.sum() + 1e-7))\n",
    "\n",
    "    def decode(self, model_logprobs):\n",
    "        \"\"\"\n",
    "        Viterbi decoding for input model output\n",
    "\n",
    "        model_logprobs : torch.tensor\n",
    "            Shape is (sequence_length, tag_space_size)\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        sequence_length, tag_space_size = model_logprobs.shape\n",
    "        \n",
    "        prob_max = torch.zeros((sequence_length, tag_space_size))\n",
    "        prev_max = torch.zeros((sequence_length, tag_space_size), dtype=int)\n",
    "        \n",
    "        prob_max[0, :] = self.w*model_logprobs[0, :] + (1 - self.w)*self.C\n",
    "        for i in range(1, sequence_length):\n",
    "            for j in range(tag_space_size):\n",
    "                scores = self.w*model_logprobs[i, j] + (1 - self.w)*(prob_max[i-1] + self.A[:, j])\n",
    "                prob_max[i, j] = torch.max(scores)\n",
    "                prev_max[i, j] = torch.argmax(scores)\n",
    "        \n",
    "        y_pred = torch.zeros(sequence_length, dtype=int)\n",
    "        y_pred[-1] = torch.argmax(prob_max[-1, :])\n",
    "        \n",
    "        for i in reversed(range(sequence_length-1)):\n",
    "            y_pred[i] = prev_max[i+1, y_pred[i+1]]\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def predict_for_batch(self, x_batch, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Returns predictions for x_batch. Use viterbi decoding.\n",
    "\n",
    "        return type: torch.LongTensor\n",
    "        return shape: (number of samples in batch, number of words in sentence).\n",
    "\n",
    "        ----------\n",
    "        Parameters\n",
    "        x_batch: torch.LongTensor with shape (number of samples in batch, number of words in sentence).\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        ######### YOUR CODE HERE #############\n",
    "        ######################################\n",
    "        outputs = self.model(x_batch.to(device)).cpu()\n",
    "        y_pred = []\n",
    "        for output in outputs:\n",
    "            y_pred.append(self.decode(output))\n",
    "        return torch.stack(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVtN-0SpsiPU"
   },
   "source": [
    "Место для ваших экспериментов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "id": "Lxkdn8qOsiPU"
   },
   "outputs": [],
   "source": [
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 55.83596214511041, 'recall': 32.960893854748605, 'f1': 41.4519906323185, 'n_predicted_entities': 317, 'n_true_entities': 537}\n",
      "{'precision': 57.59162303664922, 'recall': 36.423841059602644, 'f1': 44.62474645030425, 'n_predicted_entities': 382, 'n_true_entities': 604}\n"
     ]
    }
   ],
   "source": [
    "viterbi = ViterbiPostprocesser(model_trained, w=0.95)\n",
    "viterbi.fit(train_dataset)\n",
    "\n",
    "print(evaluator.eval_conll(viterbi, val_loader))\n",
    "print(evaluator.eval_conll(viterbi, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 57.87781350482315, 'recall': 33.5195530726257, 'f1': 42.45283018867925, 'n_predicted_entities': 311, 'n_true_entities': 537}\n",
      "{'precision': 58.93333333333333, 'recall': 36.58940397350993, 'f1': 45.14811031664965, 'n_predicted_entities': 375, 'n_true_entities': 604}\n"
     ]
    }
   ],
   "source": [
    "viterbi = ViterbiPostprocesser(model_trained, w=0.85)\n",
    "viterbi.fit(train_dataset)\n",
    "\n",
    "print(evaluator.eval_conll(viterbi, val_loader))\n",
    "print(evaluator.eval_conll(viterbi, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 52.280701754385966, 'recall': 27.746741154562383, 'f1': 36.25304136253042, 'n_predicted_entities': 285, 'n_true_entities': 537}\n",
      "{'precision': 56.49717514124294, 'recall': 33.11258278145695, 'f1': 41.75365344467641, 'n_predicted_entities': 354, 'n_true_entities': 604}\n"
     ]
    }
   ],
   "source": [
    "viterbi = ViterbiPostprocesser(model_trained, w=0.5)\n",
    "viterbi.fit(train_dataset)\n",
    "\n",
    "print(evaluator.eval_conll(viterbi, val_loader))\n",
    "print(evaluator.eval_conll(viterbi, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 48.63813229571984, 'recall': 23.277467411545622, 'f1': 31.48614609571788, 'n_predicted_entities': 257, 'n_true_entities': 537}\n",
      "{'precision': 56.32911392405063, 'recall': 29.47019867549669, 'f1': 38.69565217391305, 'n_predicted_entities': 316, 'n_true_entities': 604}\n"
     ]
    }
   ],
   "source": [
    "viterbi = ViterbiPostprocesser(model_trained, w=0.3)\n",
    "viterbi.fit(train_dataset)\n",
    "\n",
    "print(evaluator.eval_conll(viterbi, val_loader))\n",
    "print(evaluator.eval_conll(viterbi, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить из экспериментов, данный метод генерации позволяет ещё сильнее улучшить качество работы модели. Однако необходимо корректно подбирать гиперпараметры. Сильно высокий показатель w или наоборот сильно низкий может даже ухудшить базовый алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48hGh0CPsiPU"
   },
   "source": [
    "## Бонусная часть. Обучаемый постпроцессинг через CRF (2 балла).\n",
    "\n",
    "Реализуйте сами / модифицируйте открытую реализацию CRF, соответствующую реализации обучаемого пост-процессинга в лекции. Например, можно использовать эту реализацию:\n",
    "https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "\n",
    "В такой модели должно использоваться два типа признаков:\n",
    "\n",
    "* $|Y|\\times|Y|$ признаков, учитывающих выход модели:\n",
    "$$\t\\phi_{uv}(y_{i-1}, y_i, x_i) = \\mathbb{I}[y_{i-1} = u]\\mathbb{I}[y_i = v] S_{i,y_i}$$\n",
    "* $|Y|\\times|Y|$ признаков, учитывающих связь меток:\n",
    "$$ \\psi_{uv}(y_{i-1}, y_i) = \\mathbb{I}[y_{i-1} = u]\\mathbb{I}[y_i = v] $$\n",
    "\n",
    "Итоговое выражение для $G_{x, i}[y_{i-1}, y_i]$ выглядит так:\n",
    "$$\n",
    "G_{x, i}[y_{i-1}, y_i] = w(y_{i-1}, y_i) S_{i, y_i} + a(y_{i-1}, y_i)\n",
    "$$\n",
    "\n",
    "\n",
    "**<font color='red'>Задание. Обучите модель BiLSTM-CRF в едином пайплайне, добейтесь улучшения качества модели, сделайте выводы по проделанным экспериментам.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqkhjWuFsiPV"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llH9f8KXsiPV"
   },
   "source": [
    "## Бонусная часть. Дополнительный char-LSTM слой (1 балл).\n",
    "\n",
    "Добавьте к слою представлений слов обучаемые char-based представления. Каждое слово разделяется на символы и прогоняется через char-based сеть (например, через LSTM). Финальное состояние сети (или конкатенация двух финальных состояний в случае bidirectional LSTM) подаётся как дополнительное представление.\n",
    "\n",
    "**<font color='red'> Задание. Обучите модель с дополнительным представлением и сравните качество с исходной моделью. Сделайте выводы по проделанным экспериментам.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQH_TLlCsiPV"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-kurtsev_sft_pipeline]",
   "language": "python",
   "name": "conda-env-.mlspace-kurtsev_sft_pipeline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
